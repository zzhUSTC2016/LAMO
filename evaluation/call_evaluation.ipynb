{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1_overall_evluation_vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA_VISIBLE_DEVICES=5 python evaluation_vllm.py --model_name /ratio_5_3e-5/checkpoint-2016  --gpu_memory_utilization 0.4 --data_len 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-03 16:11:19 llm_engine.py:72] Initializing an LLM engine with config: model='../Models/overall_history_concise_1/checkpoint-1152', tokenizer='../Models/overall_history_concise_1/checkpoint-1152', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, seed=0)\n",
      "INFO 02-03 16:11:56 llm_engine.py:207] # GPU blocks: 4727, # CPU blocks: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 30200/30200 [1:03:31<00:00,  7.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 jaccard: 0.2632, recall: 0.2941, precision: 0.7143, f1: 0.4167, ddi_rate: 0.0476, drug_num: 7.00/151, drug_num_gt: 17.00/151,refuse_rate: 0.0000\n",
      "10 jaccard: 0.3203, recall: 0.3769, precision: 0.7609, f1: 0.4751, ddi_rate: 0.1168, drug_num: 13.45/151, drug_num_gt: 26.64/151,refuse_rate: 0.0000\n",
      "20 jaccard: 0.3061, recall: 0.3540, precision: 0.7518, f1: 0.4588, ddi_rate: 0.1120, drug_num: 11.43/151, drug_num_gt: 23.95/151,refuse_rate: 0.0000\n",
      "30 jaccard: 0.2900, recall: 0.3382, precision: 0.7289, f1: 0.4413, ddi_rate: 0.1136, drug_num: 11.35/151, drug_num_gt: 24.42/151,refuse_rate: 0.0000\n",
      "40 jaccard: 0.2949, recall: 0.3463, precision: 0.6994, f1: 0.4400, ddi_rate: 0.1116, drug_num: 11.68/151, drug_num_gt: 23.95/151,refuse_rate: 0.0000\n",
      "50 jaccard: 0.2940, recall: 0.3444, precision: 0.7084, f1: 0.4405, ddi_rate: 0.1068, drug_num: 11.41/151, drug_num_gt: 23.76/151,refuse_rate: 0.0000\n",
      "60 jaccard: 0.2925, recall: 0.3480, precision: 0.6961, f1: 0.4400, ddi_rate: 0.1036, drug_num: 11.61/151, drug_num_gt: 23.39/151,refuse_rate: 0.0000\n",
      "70 jaccard: 0.3002, recall: 0.3561, precision: 0.7034, f1: 0.4489, ddi_rate: 0.0990, drug_num: 11.37/151, drug_num_gt: 22.82/151,refuse_rate: 0.0000\n",
      "80 jaccard: 0.2851, recall: 0.3415, precision: 0.6897, f1: 0.4291, ddi_rate: 0.0934, drug_num: 10.95/151, drug_num_gt: 22.80/151,refuse_rate: 0.0000\n",
      "90 jaccard: 0.2898, recall: 0.3462, precision: 0.7010, f1: 0.4355, ddi_rate: 0.0965, drug_num: 11.51/151, drug_num_gt: 23.88/151,refuse_rate: 0.0000\n",
      "100 jaccard: 0.2866, recall: 0.3432, precision: 0.6968, f1: 0.4314, ddi_rate: 0.0943, drug_num: 11.49/151, drug_num_gt: 23.94/151,refuse_rate: 0.0000\n",
      "110 jaccard: 0.2916, recall: 0.3508, precision: 0.6955, f1: 0.4372, ddi_rate: 0.0934, drug_num: 11.95/151, drug_num_gt: 24.11/151,refuse_rate: 0.0000\n",
      "120 jaccard: 0.2971, recall: 0.3593, precision: 0.6936, f1: 0.4433, ddi_rate: 0.0918, drug_num: 12.38/151, drug_num_gt: 24.44/151,refuse_rate: 0.0000\n",
      "130 jaccard: 0.3020, recall: 0.3648, precision: 0.6923, f1: 0.4479, ddi_rate: 0.0929, drug_num: 12.60/151, drug_num_gt: 24.58/151,refuse_rate: 0.0000\n",
      "140 jaccard: 0.2960, recall: 0.3568, precision: 0.6937, f1: 0.4410, ddi_rate: 0.0919, drug_num: 12.32/151, drug_num_gt: 24.51/151,refuse_rate: 0.0000\n",
      "150 jaccard: 0.2922, recall: 0.3508, precision: 0.7010, f1: 0.4365, ddi_rate: 0.0901, drug_num: 12.07/151, drug_num_gt: 24.56/151,refuse_rate: 0.0000\n",
      "160 jaccard: 0.2969, recall: 0.3593, precision: 0.6936, f1: 0.4411, ddi_rate: 0.0910, drug_num: 12.19/151, drug_num_gt: 24.11/151,refuse_rate: 0.0000\n",
      "170 jaccard: 0.2965, recall: 0.3586, precision: 0.6909, f1: 0.4404, ddi_rate: 0.0912, drug_num: 12.33/151, drug_num_gt: 24.31/151,refuse_rate: 0.0000\n",
      "180 jaccard: 0.2982, recall: 0.3587, precision: 0.6967, f1: 0.4426, ddi_rate: 0.0919, drug_num: 12.44/151, drug_num_gt: 24.73/151,refuse_rate: 0.0000\n",
      "190 jaccard: 0.3008, recall: 0.3629, precision: 0.6969, f1: 0.4454, ddi_rate: 0.0937, drug_num: 12.58/151, drug_num_gt: 24.59/151,refuse_rate: 0.0000\n",
      "jaccard: 0.3014, recall: 0.3646, precision: 0.6922, f1: 0.4457, ddi_rate: 0.0933, drug_num: 12.47/151, drug_num_gt: 24.20/151,refuse_rate: 0.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '5'\n",
    "from _1_overall_evaluation_vllm import overall_evaluation_vllm\n",
    "\n",
    "for lora_path in [\"../output/lora_0_151/checkpoint-704\"]:\n",
    "    overall_evaluation_vllm(lora_path, gpu_memory_utilization=0.63, data_len=200, base_model='llama-2-7b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2_each_drug_evluation_vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=5 nohup python _2_each_drug_evaluation_vllm.py --lora_path ../output/lora_0_151_ratio_4/checkpoint-1408 --test_med_list None --gpu_memory_utilization 0.8 --start_idx 120  --end_idx 151 --data_len -1 --base_model llama-2-7b --file_name data4LLM_CONCISE_NOTE.csv > nohup3.out 2> nohup3.err &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Single drug evaluation: Merging lora weights and saving hf model ------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ca9696845494c09949dc4dc18c3ad12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sds/cxfan/anaconda3/envs/qwen/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data size: 2368\n",
      "\n",
      "INFO 02-04 18:46:47 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/select_drug_Mirtazapine/finetune_results/141_checkpoint_Mirtazapine_0.1161-544', tokenizer='../Models/select_drug_Mirtazapine/finetune_results/141_checkpoint_Mirtazapine_0.1161-544', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 18:47:11 llm_engine.py:275] # GPU blocks: 7296, # CPU blocks: 512\n",
      "INFO 02-04 18:47:13 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 18:47:13 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 18:47:18 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:41<00:00, 23.41it/s]\n",
      "2024-02-04 18:49:01,858 - INFO - 141-Mirtazapine             , recall:0.3187  precision:0.6905  f1:0.4361  jaccard:0.2788  drug_pred:42/2368, drug_gt:91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jaccard: 0.2788\n",
      "len(jaccard_list): 1\n",
      "jaccard_list: [0.27884615384615385]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '3'\n",
    "from _2_each_drug_evaluation_vllm import each_drug_evaluation_vllm\n",
    "\n",
    "test_med_list = ['Mirtazapine']\n",
    "\n",
    "for model_name in [\"../output/select_drug_Mirtazapine/finetune_results/141_checkpoint_Mirtazapine_0.1161-544\"]:\n",
    "    each_drug_evaluation_vllm(model_name, test_med_list, gpu_memory_utilization=0.9, \n",
    "                              data_len=-1, vllm=True, remove_model=True, base_model='llama-2-7b', use_note=False, \n",
    "                              file_name='data4LLM_CONCISE_NOTE.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-11 22:35:45 llm_engine.py:72] Initializing an LLM engine with config: model='../Models/each_drug_55_100/finetune_results/55_checkpoint_Ceftriaxone_0.2345-1152', tokenizer='../Models/each_drug_55_100/finetune_results/55_checkpoint_Ceftriaxone_0.2345-1152', tokenizer_mode=auto, revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, seed=0)\n",
      "INFO 12-11 22:36:09 llm_engine.py:205] # GPU blocks: 3407, # CPU blocks: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2421/2421 [01:32<00:00, 26.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'Yes.', 'Yes.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'Yes.', 'Yes.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'Yes.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'Yes.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'Yes.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'Yes.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'Yes.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'Yes.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'Yes.', 'Yes.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'Yes.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'Yes.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'Yes.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'Yes.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'Yes.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'Yes.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'Yes.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'Yes.', 'No.', 'Yes.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'Yes.', 'No.', 'Yes.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'Yes.', 'Yes.', 'Yes.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'Yes.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'Yes.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'Yes.', 'No.', 'Yes.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'Yes.', 'Yes.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'Yes.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'Yes.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'Yes.', 'Yes.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'Yes.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'Yes.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'Yes.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'Yes.', 'No.', 'No.', 'No.', 'No.', 'No.', 'Yes.', 'No.']\n",
      "med:Ceftriaxone             , recall:0.3434  precision:0.4252  f1:0.3799  jaccard:0.2345  drug_pred:294/2421, drug_gt:364\n",
      "------ Single drug evaluation: Merging lora weights and saving hf model ------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d54cea39c38b4e6e91dd2b00049dceca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-11 22:38:55 llm_engine.py:72] Initializing an LLM engine with config: model='../Models/each_drug_55_100/finetune_results/56_checkpoint_Trazodone_0.1310-1', tokenizer='../Models/each_drug_55_100/finetune_results/56_checkpoint_Trazodone_0.1310-1', tokenizer_mode=auto, revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, seed=0)\n",
      "INFO 12-11 22:39:15 llm_engine.py:205] # GPU blocks: 3407, # CPU blocks: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2421/2421 [01:42<00:00, 23.60it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['###Answer:', \"###Current Clinical Condition: '''\\n\\nAge: \", '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', 'Yes.', '<Yes.> or <No.>', '###Explanation:', '###Current Clinical Condition:\\n\\nAge: 3', \"Yes.\\n\\n'''\\n###Explanation:\\n\\nTra\", '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.>', '<Yes.> or <No.>', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:\\n\\n###Answer:\\n\\n###Answer', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', '###Current Clinical Condition:\\n\\nAge: 8', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.>\\n\\n###Explanation:\\n\\nTra', '###Current Clinical Condition:\\n\\nAge: 5', '<Yes.> or <No.> without providing an explanation.', 'Yes.', 'Yes.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', '###Explanation:', \"<Yes.>\\n\\n'''\\n###Explanation:\", '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', \"<Yes.> or <No.>\\n\\n'''\\n###\", '###Current Clinical Condition:\\n\\nAge: 7', 'Yes.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', 'Yes.', '###Answer:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', '<Yes.> or <No.>', '###Explanation:', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 7', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', '<Yes.>\\n\\n###Explanation:\\n\\nTra', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:', '###Answer:\\n\\n###Answer:\\n\\n###Answer', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.>', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', 'Yes.', '<Yes.> or <No.> without providing an explanation.', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '###Answer:\\n\\n###Answer:\\n\\n###Answer', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.>', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.>', '###Explanation:', \"<Yes.>\\n\\n'''\\n###Explanation:\", '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 8', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Answer:\\n\\n###Answer:\\n\\n###Answer', '<Yes.> or <No.>', '###Current Clinical Condition:\\n\\nAge: 9', \"Yes.\\n\\n'''\\n###Explanation:\\n\\nTra\", '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.>', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 3', 'Yes.', '###Explanation:', '<Yes.> or <No.>', '###Explanation:', '<Yes.> or <No.>', '<Yes.> or <No.>', 'Yes.', 'Yes.', 'Yes.', 'Yes.', 'Yes.', '###Explanation:', '###Explanation:', '<Yes.>', '###Current Clinical Condition:\\n\\nAge: 3', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:\\n\\n###Answer:\\n\\n###Answer', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 6', '###Explanation:', 'Yes.', '###Explanation:', 'Yes.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.>', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', \"<Yes.> or <No.>\\n\\n'''\\n###\", '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 5', '<Yes.> or <No.> without providing an explanation.', 'Yes.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.>', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.>', 'Yes.', 'Yes.', '###Explanation:', \"<Yes.>\\n\\n'''\\n###Explanation:\", '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 8', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:\\n\\n###Answer:\\n\\n###Answer', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', \"<Yes.>\\n\\n'''\\n###Explanation:\", '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', 'Yes.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.>', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', \"Yes.\\n\\n'''\\n###Explanation:\\n\\nTra\", '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:\\n\\n###Answer:\\n\\n###Answer', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.>', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 9', '###Explanation:', '<Yes.>', 'Yes.', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', \"<Yes.>\\n\\n'''\\n###Explanation:\", '###Explanation:', '<Yes.> or <No.> without providing an explanation.', \"<Yes.> or <No.>\\n\\n'''\\n###\", '###Explanation:', '###Explanation:', '###Explanation:', '<Yes.> or <No.>', '<Yes.> or <No.>', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 5', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.>', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', '###Explanation:', '<Yes.> or <No.>', 'Yes.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', '###Explanation:', \"<Yes.> or <No.>\\n\\n'''\\n###\", '###Explanation:', '<Yes.> or <No.> without providing an explanation.', \"###Current Clinical Condition: '''\\n\\nAge: \", '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Current Clinical Condition:\\n\\nAge: 7', '###Explanation:', '###Explanation:', '###Explanation:', '###Current Clinical Condition:\\n\\nAge: 2', 'Yes.', '###Explanation:', '<Yes.> or <No.>', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', \"<Yes.> or <No.>\\n\\n'''\\n###\", '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 6', 'Yes.', '###Answer:\\n\\n###Answer:\\n\\n###Answer', '<Yes.> or <No.> without providing an explanation.', '###Answer:\\n\\n###Answer:\\n\\n###Answer', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', \"<Yes.>\\n\\n'''\\n###Explanation:\", '###Explanation:', '<Yes.> or <No.> without providing an explanation.', \"###Current Clinical Condition: '''\\n\\nAge: \", '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', 'Yes.', '###Answer:', '<Yes.> or <No.> without providing an explanation.', \"###Current Clinical Condition: '''\\n\\nAge: \", '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.>', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', \"###Current Clinical Condition: '''\\n\\nAge: \", \"<Yes.> or <No.>\\n\\n'''\\n###\", '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 4', '<Yes.> or <No.> without providing an explanation.', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.>', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.>', '###Explanation:', '###Current Clinical Condition:\\n\\nAge: 7', '<Yes.>\\n\\n###Explanation:\\n\\nTra', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', \"###Current Clinical Condition: '''\\n\\nAge: \", '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.>', '<Yes.> or <No.>', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', '<Yes.> or <No.> without providing an explanation.', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Answer:', '###Explanation:', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', '###Explanation:', '<Yes.> or <No.>', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.>', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', '###Explanation:', 'Yes.', '###Answer:\\n\\n###Answer:\\n\\n###Answer', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.>', '###Current Clinical Condition:\\n\\nAge: 9', '<Yes.> or <No.> without providing an explanation.', '###Answer:\\n\\n###Answer:\\n\\n###Answer', '<Yes.> or <No.>', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:\\n\\n###Answer:\\n\\n###Answer', '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 3', '###Explanation:', '<Yes.> or <No.>', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.>', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', '###Explanation:', \"<Yes.>\\n\\n'''\\n###Explanation:\", '###Answer:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.>', '###Explanation:', 'Yes.', '###Explanation:', '<Yes.> or <No.>', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 4', 'Yes.', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.>', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', 'Yes.', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Answer:', '###Explanation:', \"Yes.\\n\\n'''\\n###Explanation:\\n\\nTra\", '###Explanation:', '###Current Clinical Condition:\\n\\nAge: 8', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', \"<Yes.>\\n\\n'''\\n###Explanation:\", '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.>', '###Explanation:', '###Explanation:', 'Yes.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', '<Yes.> or <No.>', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.>', 'Yes.', '###Current Clinical Condition:\\n\\nAge: 3', 'Yes.', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.>', '<Yes.> or <No.>', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 4', 'Yes.', '###Answer:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.>', \"<Yes.> or <No.>\\n\\n'''\\n###\", '###Explanation:', 'Yes.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.>', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Answer:', 'Yes.', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:\\n\\n###Answer:\\n\\n###Answer', '<Yes.> or <No.>', \"Yes.\\n\\n'''\\n###Explanation:\\n\\nTra\", '###Current Clinical Condition:\\n\\nAge: 6', 'Yes.', '###Explanation:', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:\\n\\n###Answer:\\n\\n###Answer', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.>', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', \"<Yes.>\\n\\n'''\\n###Explanation:\", '###Explanation:', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', \"###Current Clinical Condition: '''\\n\\nAge: \", \"<Yes.> or <No.>\\n\\n'''\\n###\", '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', '###Explanation:', 'Yes.', 'Yes.', '###Explanation:', '<Yes.> or <No.>', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:\\n\\n###Answer:\\n\\n###Answer', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.>', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 5', 'Yes.', '<Yes.>', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', \"<Yes.>\\n\\n'''\\n###Explanation:\", 'Yes.', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.>', '###Explanation:', 'Yes.', '###Current Clinical Condition:\\n\\nAge: 9', 'Yes.', \"Yes.\\n\\n'''\\n###Explanation:\\n\\nTra\", '###Explanation:', 'Yes.', '###Explanation:', \"Yes.\\n\\n'''\\n###Explanation:\\n\\nTra\", '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.>', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:', '<Yes.> or <No.> without providing an explanation.', \"###Current Clinical Condition: '''\\n\\nAge: \", '<Yes.> or <No.> without providing an explanation.', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', \"Yes.\\n\\n'''\\n###Explanation:\\n\\nTra\", '###Explanation:', '<Yes.> or <No.>', '###Answer:\\n\\n###Answer:\\n\\n###Answer', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', \"###Current Clinical Condition: '''\\n\\nAge: \", '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', '###Answer:', '###Current Clinical Condition:\\n\\nAge: 7', 'Yes.', '###Explanation:', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', \"<Yes.>\\n\\n'''\\n###Explanation:\", '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.>', '###Answer:\\n\\n###Answer:\\n\\n###Answer', '<Yes.> or <No.>', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', 'Yes.', '###Explanation:', 'Yes.', '###Current Clinical Condition:\\n\\nAge: 6', '<Yes.> or <No.>', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', '<Yes.> or <No.>', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 6', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 6', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.>\\n\\n###Explanation:\\n\\nTra', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.>', '###Current Clinical Condition:\\n\\nAge: 3', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.>', '###Answer:\\n\\n###Answer:\\n\\n###Answer', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.>', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', 'Yes.', '###Explanation:', '###Explanation:', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', '###Current Clinical Condition:\\n\\nAge: 2', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.>', '###Explanation:', '<Yes.> or <No.>', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Current Clinical Condition:\\n\\nAge: 6', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.>', \"Yes.\\n\\n'''\\n###Explanation:\\n\\nTra\", '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.>', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.>', '###Current Clinical Condition:\\n\\nAge: 6', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:\\n\\n###Answer:\\n\\n###Answer', '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 5', '<Yes.> or <No.> without providing an explanation.', \"###Current Clinical Condition: '''\\n\\nAge: \", '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', \"<Yes.> or <No.>\\n\\n'''\\n###\", '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:\\n\\n###Answer:\\n\\n###Answer', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:\\n\\n###Answer:\\n\\n###Answer', '<Yes.> or <No.>', '###Explanation:', '<Yes.> or <No.>', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.>', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Answer:\\n\\n###Answer:\\n\\n###Answer', '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 6', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.>', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 5', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.>', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.>', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.>', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', '###Explanation:', '###Explanation:', 'Yes.', '<Yes.> or <No.> without providing an explanation.', '<Yes.>\\n\\n###Explanation:\\n\\nTra', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 6', '<Yes.> or <No.>', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', 'Yes.', 'Yes.', '<Yes.> or <No.>', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', '###Answer:\\n\\n###Answer:\\n\\n###Answer', '<Yes.> or <No.>', '###Explanation:', '<Yes.> or <No.>', '###Explanation:', \"<Yes.>\\n\\n'''\\n###Explanation:\", '###Current Clinical Condition:\\n\\nAge: 6', '<Yes.> or <No.> without providing an explanation.', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.>', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.>', '<Yes.> or <No.> without providing an explanation.', \"###Current Clinical Condition: '''\\n\\nAge: \", '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.>', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', 'Yes.', '###Explanation:', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 5', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:\\n\\n###Answer:\\n\\n###Answer', '###Explanation:', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 4', '###Explanation:', '###Explanation:', '<Yes.> or <No.>', \"###Current Clinical Condition: '''\\n\\nAge: \", '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.>', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Answer:', '<Yes.> or <No.>', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.>', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', \"###Current Clinical Condition: '''\\n\\nAge: \", '<Yes.> or <No.>', '<Yes.> or <No.>', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:\\n\\n###Answer:\\n\\n###Answer', '<Yes.> or <No.> without providing an explanation.', \"<Yes.>\\n\\n'''\\n###Explanation:\", '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 5', 'Yes.', '<Yes.>', '###Explanation:', 'Yes.', '###Explanation:', '<Yes.> or <No.>', '###Answer:', '###Explanation:', '###Explanation:', '<Yes.> or <No.>', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 4', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.>', '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 6', \"Yes.\\n\\n'''\\n###Explanation:\\n\\nTra\", '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 4', 'Yes.', 'Yes.', '###Explanation:', \"Yes.\\n\\n'''\\n###Explanation:\\n\\nTra\", '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.>', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:\\n\\n###Answer:\\n\\n###Answer', '<Yes.> or <No.> without providing an explanation.', '<Yes.>', '###Explanation:', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.>', '###Explanation:', '###Answer:\\n\\n###Answer:\\n\\n###Answer', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', '<Yes.> or <No.>', '###Current Clinical Condition:\\n\\nAge: 7', 'Yes.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', '###Explanation:', 'Yes.', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.>', '<Yes.> or <No.>', '###Explanation:', \"<Yes.> or <No.>\\n\\n'''\\n###\", '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.>', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Answer:\\n\\n###Answer:\\n\\n###Answer', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.>', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 6', 'Yes.', '###Explanation:', '<Yes.> or <No.>', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', '###Explanation:', '###Answer:\\n\\n###Answer:\\n\\n###Answer', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.>', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', '###Explanation:', 'Yes.', '###Explanation:', '<Yes.> or <No.>', '<Yes.> or <No.> without providing an explanation.', \"<Yes.> or <No.>\\n\\n'''\\n###\", '<Yes.> or <No.> without providing an explanation.', '###Explanation:', \"<Yes.>\\n\\n'''\\n###Explanation:\", '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Answer:', 'Yes.', '###Explanation:', 'Yes.', '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 4', 'Yes.', '###Explanation:', '<Yes.>\\n\\n###Explanation:\\n\\nTra', '###Answer:\\n\\n###Answer:\\n\\n###Answer', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.>', '###Explanation:', 'Yes.', '###Answer:\\n\\n###Answer:\\n\\n###Answer', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Answer:\\n\\n###Answer:\\n\\n###Answer', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', '###Current Clinical Condition:\\n\\nAge: 5', 'Yes.', '###Explanation:', '###Explanation:', 'Yes.', '###Explanation:', '###Explanation:', \"<Yes.> or <No.>\\n\\n'''\\n###\", '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', '###Explanation:', '<Yes.> or <No.>', '###Answer:\\n\\n###Answer:\\n\\n###Answer', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', '<Yes.>\\n\\n###Explanation:\\n\\nTra', 'Yes.', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:\\n\\n###Answer:\\n\\n###Answer', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', \"###Current Clinical Condition: '''\\n\\nAge: \", '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.>\\n\\n###Explanation:\\n\\nTra', '###Explanation:', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 8', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', '###Explanation:', \"<Yes.> or <No.>\\n\\n'''\\n###\", '###Answer:', '<Yes.> or <No.> without providing an explanation.', '<Yes.>', \"Yes.\\n\\n'''\\n###Explanation:\\n\\nTra\", '###Explanation:', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', \"###Current Clinical Condition: '''\\n\\nAge: \", \"<Yes.> or <No.>\\n\\n'''\\n###\", '<Yes.> or <No.> without providing an explanation.', '###Explanation:', \"###Current Clinical Condition: '''\\n\\nAge: \", '<Yes.> or <No.> without providing an explanation.', 'Yes.', 'Yes.', '###Explanation:', '###Explanation:', 'Yes.', '###Current Clinical Condition:\\n\\nAge: 7', 'Yes.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 8', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.>', '###Current Clinical Condition:\\n\\nAge: 5', 'Yes.', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', '<Yes.>\\n\\n###Explanation:\\n\\nTra', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.>', \"###Current Clinical Condition: '''\\n\\nAge: \", '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.>', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', '###Current Clinical Condition:\\n\\nAge: 6', 'Yes.', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', 'Yes.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.>', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 4', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.>\\n\\n###Explan', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.>', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.>', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.>', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', \"<Yes.> or <No.>\\n\\n'''\\n###\", '<Yes.> or <No.>', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', 'Yes.', '###Explanation:', '<Yes.> or <No.>', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Answer:', '###Explanation:', '<Yes.> or <No.>', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', '###Explanation:', '<Yes.>\\n\\n###Explanation:\\n\\nTra', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:', '<Yes.> or <No.>', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', \"<Yes.>\\n\\n'''\\n###Explanation:\", '###Current Clinical Condition:\\n\\nAge: 3', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 7', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.>', '###Explanation:', '###Explanation:', '###Explanation:', \"<Yes.> or <No.>\\n\\n'''\\n###\", '###Explanation:', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', '###Explanation:', 'Yes.', \"###Current Clinical Condition: '''\\n\\nAge: \", '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Current Clinical Condition:\\n\\nAge: 3', '###Current Clinical Condition:\\n\\nAge: 8', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', '<Yes.> or <No.>', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', \"###Current Clinical Condition: '''\\n\\nAge: \", '<Yes.> or <No.> without providing an explanation.', '###Explanation:', \"<Yes.> or <No.>\\n\\n'''\\n###\", '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', '<Yes.> or <No.>', '###Explanation:', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.>', '<Yes.> or <No.> without providing an explanation.', 'Yes.', '<Yes.> or <No.> without providing an explanation.', \"###Current Clinical Condition: '''\\n\\nAge: \", '<Yes.> or <No.>', '<Yes.> or <No.>', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:\\n\\n###Answer:\\n\\n###Answer', '###Explanation:', '<Yes.>', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.>', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 7', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 6', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.>', '###Current Clinical Condition:\\n\\nAge: 7', '###Explanation:', '###Answer:', '<Yes.> or <No.> without providing an explanation.', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:\\n\\n###Answer:\\n\\n###Answer', '<Yes.> or <No.> without providing an explanation.', \"###Current Clinical Condition: '''\\n\\nAge: \", '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 3', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.>', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 5', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.>', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.>', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:\\n\\n###Answer:\\n\\n###Answer', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:\\n\\n###Answer:\\n\\n###Answer', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', '###Current Clinical Condition:\\n\\nAge: 6', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', \"<Yes.> or <No.>\\n\\n'''\\n###\", '###Explanation:', '###Current Clinical Condition:\\n\\nAge: 6', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', '###Explanation:', '<Yes.> or <No.>', '###Explanation:', \"Yes.\\n\\n'''\\n###Explanation:\\n\\nTra\", '###Explanation:', '###Explanation:', 'Yes.', '###Explanation:', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', \"<Yes.>\\n\\n'''\\n###Explanation:\", '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.>', \"<Yes.>\\n\\n'''\\n###Explanation:\", '###Explanation:', '###Explanation:', \"###Current Clinical Condition: '''\\n\\nAge: \", '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:', '<Yes.> or <No.> without providing an explanation.', \"###Current Clinical Condition: '''\\n\\nAge: \", '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Answer:\\n\\n###Answer:\\n\\n###Answer', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', \"<Yes.>\\n\\n'''\\n###Explanation:\", 'Yes.', '###Explanation:', 'Yes.', 'Yes.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', 'Yes.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.>', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 6', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 5', '###Explanation:', \"<Yes.>\\n\\n'''\\n###Explanation:\", '###Explanation:', '<Yes.> or <No.>', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 8', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', '###Current Clinical Condition:\\n\\nAge: 7', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.>', '<Yes.> or <No.>', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.>\\n\\n###Explanation:\\n\\nTra', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 2', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.>', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 3', '###Answer:\\n\\n###Answer:\\n\\n###Answer', '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 8', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 7', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Answer:\\n\\n###Answer:\\n\\n###Answer', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', '###Answer:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', \"<Yes.>\\n\\n'''\\n###Explanation:\", 'Yes.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.>', '###Explanation:', '<Yes.> or <No.>', '###Explanation:', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 3', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 6', '<Yes.> or <No.>', '###Explanation:', '###Explanation:', '###Explanation:', '###Explanation:', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 7', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.>', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.>', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.>', \"<Yes.>\\n\\n'''\\n###Explanation:\", '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.>', '###Explanation:', '###Explanation:', '###Explanation:', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Answer:', '###Answer:\\n\\n###Answer:\\n\\n###Answer', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:', '<Yes.> or <No.>', '###Explanation:', '###Current Clinical Condition:\\n\\nAge: 5', '<Yes.> or <No.>', '<Yes.>', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.>', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.>', 'Yes.', '###Explanation:', \"<Yes.>\\n\\n'''\\n###Explanation:\", '###Answer:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.>', '###Explanation:', '###Explanation:', '###Explanation:', \"Yes.\\n\\n'''\\n###Explanation:\\n\\nTra\", '###Explanation:', \"<Yes.> or <No.>\\n\\n'''\\n###\", '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.>', 'Yes.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', \"<Yes.>\\n\\n'''\\n###Explanation:\", '###Answer:', '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 7', '<Yes.> or <No.>', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:\\n\\n###Answer:\\n\\n###Answer', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', \"<Yes.>\\n\\n'''\\n###Explanation:\", '###Current Clinical Condition:\\n\\nAge: 8', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', \"<Yes.>\\n\\n'''\\n###Explanation:\", '###Explanation:', '<Yes.> or <No.>', '###Current Clinical Condition:\\n\\nAge: 7', 'Yes.', '###Current Clinical Condition:\\n\\nAge: 5', \"Yes.\\n\\n'''\\n###Explanation:\\n\\nTra\", '<Yes.>', 'Yes.', 'Yes.', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', \"###Current Clinical Condition: '''\\n\\nAge: \", '<Yes.> or <No.>', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', 'Yes.', '###Current Clinical Condition:\\n\\nAge: 5', '<Yes.> or <No.>', '###Explanation:', '<Yes.> or <No.>', '###Answer:', '<Yes.>', '<Yes.> or <No.>', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:\\n\\n###Answer:\\n\\n###Answer', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.>', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', '<Yes.> or <No.>', \"<Yes.>\\n\\n'''\\n###Explanation:\", '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:', '###Explanation:', '<Yes.> or <No.>', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', 'Yes.', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 5', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Current Clinical Condition:\\n\\nAge: 6', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:\\n\\n###Answer:\\n\\n###Answer', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:\\n\\n###Answer:\\n\\n###Answer', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Answer:', \"<Yes.>\\n\\n'''\\n###Explanation:\", '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 5', 'Yes.', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', \"<Yes.> or <No.>\\n\\n'''\\n###\", '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', \"<Yes.> or <No.>\\n\\n'''\\n###\", '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 8', 'Yes.', '###Explanation:', '<Yes.> or <No.>', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', '###Current Clinical Condition:\\n\\nAge: 4', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', \"###Current Clinical Condition: '''\\n\\nAge: \", '<Yes.> or <No.> without providing an explanation.', \"<Yes.> or <No.>\\n\\n'''\\n###\", '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Current Clinical Condition:\\n\\nAge: 4', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.>', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', \"###Current Clinical Condition: '''\\n\\nAge: \", '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.>', '<Yes.> or <No.>', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Current Clinical Condition:\\n\\nAge: 6', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.>', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 4', 'Yes.', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '###Answer:\\n\\n###Answer:\\n\\n###Answer', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.>', '###Current Clinical Condition:\\n\\nAge: 3', 'Yes.', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.>', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', \"###Current Clinical Condition: '''\\n\\nAge: \", '<Yes.> or <No.> without providing an explanation.', '<Yes.>', 'Yes.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.>', '<Yes.> or <No.>', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', '<Yes.> or <No.>', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Current Clinical Condition:\\n\\nAge: 6', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', 'Yes.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', '###Explanation:', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.>', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', \"<Yes.>\\n\\n'''\\n###Explanation:\", '###Answer:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 3', \"<Yes.>\\n\\n'''\\n###Explanation:\", '###Explanation:', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:\\n\\n###Answer:\\n\\n###Answer', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Answer:\\n\\n###Answer:\\n\\n###Answer', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Explanation:', '<Yes.> or <No.>', \"<Yes.> or <No.>\\n\\n'''\\n###\", '###Explanation:', 'Yes.', '###Explanation:', 'Yes.', '###Answer:\\n\\n###Answer:\\n\\n###Answer', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 6', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', '###Explanation:', '###Explanation:', '###Explanation:', '###Current Clinical Condition:\\n\\nAge: 7', 'Yes.', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.>', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', '###Explanation:', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', 'Yes.', \"###Current Clinical Condition: '''\\n\\nAge: \", '<Yes.> or <No.> without providing an explanation.', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', \"###Current Clinical Condition: '''\\n\\nAge: \", '<Yes.> or <No.> without providing an explanation.', '<Yes.> or <No.> without providing an explanation.', '###Explanation:', '###Answer:', '<Yes.> or <No.> without providing an explanation.', '###Current Clinical Condition:\\n\\nAge: 5']\n",
      "med:Trazodone               , recall:0.5215  precision:0.1489  f1:0.2316  jaccard:0.1310  drug_pred:1303/2421, drug_gt:372\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '2'\n",
    "from _2_each_drug_evaluation_vllm import each_drug_evaluation_vllm\n",
    "\n",
    "# list dir path\n",
    "result_path = '../output/each_drug_55_100/finetune_results/'\n",
    "dir_paths = os.listdir(result_path)\n",
    "dir_paths = sorted(dir_paths)\n",
    "for dir_path in dir_paths:\n",
    "    test_med_list = [dir_path.split('_')[2]]\n",
    "    model_name = result_path + dir_path\n",
    "    each_drug_evaluation_vllm(model_name, test_med_list, gpu_memory_utilization=0.5, \n",
    "                              data_len=-1, vllm=True, remove_model=False, base_model='llama-2-7b', \n",
    "                              file_name='data4LLM_CONCISE_TITLE.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3_each_lora_evluation_vllm\n",
    "给151个lora的结果进行评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=6 nohup python _3_each_lora_evaluation_vllm.py --run_name each_drug_short_linear_v1 --gpu_memory_utilization 0.9 --file_name data4LLM_CONCISE_TITLE.csv --template_name llama-2-7b_copy > each_drug_short_linear_v1.out 2>&1  &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data size: 2368\n",
      "\n",
      "{'Phenylephrine': '32_checkpoint_Phenylephrine_0.4924-640', 'Diltiazem': '69_checkpoint_Diltiazem_0.3906-1056', 'Azithromycin': '110_checkpoint_Azithromycin_0.2105-1056', 'Captopril': '83_checkpoint_Captopril_0.1985-448', 'Metoprolol': '3_checkpoint_Metoprolol_0.7179-512', 'Octreotide': '139_checkpoint_Octreotide_0.5923-800', 'Phylloquinone': '57_checkpoint_Phylloquinone_0.2936-768', 'Tetracosactide': '97_checkpoint_Tetracosactide_0.1329-416', 'Calcium gluconate': '17_checkpoint_Calcium gluconate_0.2949-352', 'Prednisone': '37_checkpoint_Prednisone_0.6485-704', 'Hydrochlorothiazide': '124_checkpoint_Hydrochlorothiazide_0.4821-416', 'Oxycodone': '8_checkpoint_Oxycodone_0.5905-320', 'Omeprazole': '42_checkpoint_Omeprazole_0.3982-160', 'Dolasetron': '96_checkpoint_Dolasetron_0.1111-768', 'Sodium bicarbonate': '38_checkpoint_Sodium bicarbonate_0.3427-1472', 'Cefazolin': '58_checkpoint_Cefazolin_0.3692-544', 'Metoclopramide': '33_checkpoint_Metoclopramide_0.5540-832', 'Magnesium': '11_checkpoint_Magnesium_0.6056-1120', 'Enalaprilat': '63_checkpoint_Enalaprilat_0.1111-512', 'Tiotropium': '140_checkpoint_Tiotropium_0.5912-416', 'Miconazole': '59_checkpoint_Miconazole_0.2383-448', 'Ascorbic acid': '94_checkpoint_Ascorbic acid_0.5155-416', 'Cefepime': '61_checkpoint_Cefepime_0.3687-608', 'Pantoprazole': '2_checkpoint_Pantoprazole_0.6063-512', 'Ipratropium': '12_checkpoint_Ipratropium_0.5416-416', 'Sodium phosphate, monobasic': '111_checkpoint_Sodium phosphate, monobasic_0.1505-768', 'Levothyroxine': '46_checkpoint_Levothyroxine_0.9068-224', 'Isosorbide': '112_checkpoint_Isosorbide_0.3376-224', 'Acetaminophen': '0_checkpoint_Acetaminophen_0.8139-448', 'Morphine': '14_checkpoint_Morphine_0.4551-1056', 'Dexamethasone': '105_checkpoint_Dexamethasone_0.4313-608', 'Haloperidol': '54_checkpoint_Haloperidol_0.2635-896', 'Ergocalciferol': '77_checkpoint_Ergocalciferol_0.3516-544', 'Folic acid': '65_checkpoint_Folic acid_0.4737-352', 'Lidocaine': '28_checkpoint_Lidocaine_0.3150-1280', 'Sodium sulfate': '134_checkpoint_Sodium sulfate_0.5948-480', 'Tramadol': '106_checkpoint_Tramadol_0.2312-768', 'Norepinephrine': '47_checkpoint_Norepinephrine_0.3356-704', 'Ampicillin': '91_checkpoint_Ampicillin_0.1781-992', 'Trazodone': '56_checkpoint_Trazodone_0.2897-352', 'Tamsulosin': '130_checkpoint_Tamsulosin_0.5481-672', 'Meropenem': '76_checkpoint_Meropenem_0.3785-672', 'Isosorbide mononitrate': '120_checkpoint_Isosorbide mononitrate_0.5078-1184', 'Calcium acetate': '121_checkpoint_Calcium acetate_0.3036-256', 'Clopidogrel': '43_checkpoint_Clopidogrel_0.6504-896', 'Ondansetron': '16_checkpoint_Ondansetron_0.4133-1504', 'Citalopram': '92_checkpoint_Citalopram_0.7547-864', 'Diazepam': '113_checkpoint_Diazepam_0.4279-448', 'Bisacodyl': '9_checkpoint_Bisacodyl_0.5124-736', 'Epinephrine': '131_checkpoint_Epinephrine_0.2447-224', 'Atorvastatin': '25_checkpoint_Atorvastatin_0.7058-576', 'Promethazine': '128_checkpoint_Promethazine_0.0448-1280', 'Fentanyl': '15_checkpoint_Fentanyl_0.5213-544', 'Acetylcysteine': '60_checkpoint_Acetylcysteine_0.1859-704', 'Gabapentin': '62_checkpoint_Gabapentin_0.6127-320', 'Atropine': '98_checkpoint_Atropine_0.4493-352', 'Mirtazapine': '141_checkpoint_Mirtazapine_0.5620-544', 'Linezolid': '117_checkpoint_Linezolid_0.2674-864', 'Ranitidine': '49_checkpoint_Ranitidine_0.5212-544', 'Clonidine': '129_checkpoint_Clonidine_0.4314-832', 'Tacrolimus': '142_checkpoint_Tacrolimus_0.8803-416', 'Furosemide': '5_checkpoint_Furosemide_0.7069-448', 'Nystatin': '66_checkpoint_Nystatin_0.2538-416', 'Niacin': '95_checkpoint_Niacin_0.5227-192', 'Sulfamethoxazole': '70_checkpoint_Sulfamethoxazole_0.4401-672', 'Allopurinol': '116_checkpoint_Allopurinol_0.6667-864', 'Sevelamer': '99_checkpoint_Sevelamer_0.5568-1120', 'Codeine': '148_checkpoint_Codeine_0.0407-1760', 'Zolpidem': '44_checkpoint_Zolpidem_0.2159-480', 'Acetazolamide': '123_checkpoint_Acetazolamide_0.1687-960', 'Sucralfate': '78_checkpoint_Sucralfate_0.2722-800', 'Metronidazole': '23_checkpoint_Metronidazole_0.4546-480', 'Glycopyrronium': '75_checkpoint_Glycopyrronium_0.7642-224', 'Valsartan': '147_checkpoint_Valsartan_0.4362-416', 'Diphenhydramine': '40_checkpoint_Diphenhydramine_0.2859-1280', 'Chlorhexidine': '30_checkpoint_Chlorhexidine_0.5358-1408', 'Ibuprofen': '89_checkpoint_Ibuprofen_0.1268-1216', 'Pantothenic acid': '102_checkpoint_Pantothenic acid_0.5720-416', 'Magnesium hydroxide': '87_checkpoint_Magnesium hydroxide_0.1443-1216', 'Ketorolac': '107_checkpoint_Ketorolac_0.3092-672', 'Salbutamol': '7_checkpoint_Salbutamol_0.6243-640', 'Monopotassium phosphate': '50_checkpoint_Monopotassium phosphate_0.2511-640', 'Hydromorphone': '24_checkpoint_Hydromorphone_0.4573-448', 'Quetiapine': '114_checkpoint_Quetiapine_0.4542-800', 'Loperamide': '138_checkpoint_Loperamide_0.1835-800', 'Nicotine': '135_checkpoint_Nicotine_0.2978-768', 'Clindamycin': '143_checkpoint_Clindamycin_0.1207-1312', 'D-glucose': '20_checkpoint_D-glucose_0.5026-704', 'Aluminum hydroxide': '126_checkpoint_Aluminum hydroxide_0.0588-1920', 'Fluticasone propionate': '45_checkpoint_Fluticasone propionate_0.6170-256', 'Amlodipine': '64_checkpoint_Amlodipine_0.5920-640', 'Dopamine': '73_checkpoint_Dopamine_0.0914-1376', 'Simvastatin': '48_checkpoint_Simvastatin_0.7268-576', 'Potassium chloride': '1_checkpoint_Potassium chloride_0.7367-352', 'Lorazepam': '10_checkpoint_Lorazepam_0.4973-864', 'Propofol': '18_checkpoint_Propofol_0.6354-672', 'Midazolam': '22_checkpoint_Midazolam_0.5126-608', 'Salmeterol': '84_checkpoint_Salmeterol_0.6184-320', 'Cyanocobalamin': '67_checkpoint_Cyanocobalamin_0.5517-1024', 'Warfarin': '27_checkpoint_Warfarin_0.6093-704', 'Olanzapine': '86_checkpoint_Olanzapine_0.2577-544', 'Mycophenolate mofetil': '132_checkpoint_Mycophenolate mofetil_0.8034-704', 'Guaifenesin': '80_checkpoint_Guaifenesin_0.2192-800', 'Levetiracetam': '108_checkpoint_Levetiracetam_0.5441-640', 'Lisinopril': '26_checkpoint_Lisinopril_0.5400-672', 'Lactulose': '41_checkpoint_Lactulose_0.3848-928', 'Biotin': '101_checkpoint_Biotin_0.5625-192', 'Atenolol': '115_checkpoint_Atenolol_0.5545-256', 'Acyclovir': '137_checkpoint_Acyclovir_0.4551-832', 'Nifedipine': '150_checkpoint_Nifedipine_0.4375-512', 'Fluoxetine': '149_checkpoint_Fluoxetine_0.7273-352', 'Sulbactam': '133_checkpoint_Sulbactam_0.1920-576', 'Fluconazole': '74_checkpoint_Fluconazole_0.4798-640', 'Sertraline': '127_checkpoint_Sertraline_0.7356-608', 'Thiamine': '51_checkpoint_Thiamine_0.5681-608', 'Acetylsalicylic acid': '13_checkpoint_Acetylsalicylic acid_0.5719-448', 'Lansoprazole': '19_checkpoint_Lansoprazole_0.3972-608', 'Ceftriaxone': '55_checkpoint_Ceftriaxone_0.2619-672', 'Hydralazine': '31_checkpoint_Hydralazine_0.3441-992', 'Hydrocortisone': '79_checkpoint_Hydrocortisone_0.3036-832', 'Hydrocortisone succinate': '104_checkpoint_Hydrocortisone succinate_0.2912-672', 'Gentamicin': '109_checkpoint_Gentamicin_0.1443-512', 'Levofloxacin': '21_checkpoint_Levofloxacin_0.2791-448', 'Nitroprusside': '122_checkpoint_Nitroprusside_0.0625-256', 'Piperacillin': '52_checkpoint_Piperacillin_0.3671-1216', 'Meperidine': '81_checkpoint_Meperidine_0.2415-160', 'Amiodarone': '53_checkpoint_Amiodarone_0.5815-480', 'Methylprednisolone hemisuccinate': '71_checkpoint_Methylprednisolone hemisuccinate_0.3956-608', 'Riboflavin': '100_checkpoint_Riboflavin_0.5667-416', 'Neostigmine': '85_checkpoint_Neostigmine_0.8795-1056', 'Digoxin': '90_checkpoint_Digoxin_0.5572-256', 'Ciprofloxacin': '34_checkpoint_Ciprofloxacin_0.3607-1376', 'Tazobactam': '39_checkpoint_Tazobactam_0.3814-960', 'Spironolactone': '35_checkpoint_Spironolactone_0.4736-512', 'Hydrocodone': '146_checkpoint_Hydrocodone_0.2202-1216', 'Ceftazidime': '136_checkpoint_Ceftazidime_0.0625-960', 'Pyridoxine': '93_checkpoint_Pyridoxine_0.5500-352', 'Magnesium sulfate': '4_checkpoint_Magnesium sulfate_0.6935-320', 'Labetalol': '88_checkpoint_Labetalol_0.2597-704', 'Metolazone': '145_checkpoint_Metolazone_0.2336-1120', 'Famotidine': '29_checkpoint_Famotidine_0.3370-800', 'Trimethoprim': '72_checkpoint_Trimethoprim_0.4498-800', 'Erythromycin': '144_checkpoint_Erythromycin_0.1000-1824', 'Sodium chloride': '103_checkpoint_Sodium chloride_0.4181-640', 'Prochlorperazine': '68_checkpoint_Prochlorperazine_0.2043-1152', 'Phenytoin': '118_checkpoint_Phenytoin_0.5266-448', 'Carvedilol': '119_checkpoint_Carvedilol_0.6171-448', 'Vancomycin': '6_checkpoint_Vancomycin_0.6966-576', 'Clonazepam': '125_checkpoint_Clonazepam_0.5692-448', 'Nitroglycerin': '36_checkpoint_Nitroglycerin_0.5611-576', 'Methylprednisolone': '82_checkpoint_Methylprednisolone_0.3686-512'}\n",
      "------ Evaluating 0-Acetaminophen, model_path: ../Models/each_drug_note_v3_without_disease/0_checkpoint_Acetaminophen_0.8139-448 ------\n",
      "INFO 02-04 19:46:27 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/0_checkpoint_Acetaminophen_0.8139-448', tokenizer='../Models/each_drug_note_v3_without_disease/0_checkpoint_Acetaminophen_0.8139-448', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 19:47:01 llm_engine.py:275] # GPU blocks: 7402, # CPU blocks: 512\n",
      "INFO 02-04 19:47:02 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 19:47:02 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 19:47:07 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 1-Potassium chloride, model_path: ../Models/each_drug_note_v3_without_disease/1_checkpoint_Potassium chloride_0.7367-352 ------\n",
      "INFO 02-04 19:48:57 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/1_checkpoint_Potassium chloride_0.7367-352', tokenizer='../Models/each_drug_note_v3_without_disease/1_checkpoint_Potassium chloride_0.7367-352', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 19:49:27 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 19:49:27 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 19:49:27 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 19:49:32 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:47<00:00, 22.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 2-Pantoprazole, model_path: ../Models/each_drug_note_v3_without_disease/2_checkpoint_Pantoprazole_0.6063-512 ------\n",
      "INFO 02-04 19:51:23 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/2_checkpoint_Pantoprazole_0.6063-512', tokenizer='../Models/each_drug_note_v3_without_disease/2_checkpoint_Pantoprazole_0.6063-512', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 19:51:51 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 19:51:51 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 19:51:51 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 19:51:56 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 3-Metoprolol, model_path: ../Models/each_drug_note_v3_without_disease/3_checkpoint_Metoprolol_0.7179-512 ------\n",
      "INFO 02-04 19:53:46 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/3_checkpoint_Metoprolol_0.7179-512', tokenizer='../Models/each_drug_note_v3_without_disease/3_checkpoint_Metoprolol_0.7179-512', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 19:54:17 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 19:54:17 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 19:54:17 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 19:54:22 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:47<00:00, 22.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 4-Magnesium sulfate, model_path: ../Models/each_drug_note_v3_without_disease/4_checkpoint_Magnesium sulfate_0.6935-320 ------\n",
      "INFO 02-04 19:56:12 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/4_checkpoint_Magnesium sulfate_0.6935-320', tokenizer='../Models/each_drug_note_v3_without_disease/4_checkpoint_Magnesium sulfate_0.6935-320', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 19:56:42 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 19:56:42 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 19:56:42 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 19:56:47 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:47<00:00, 22.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 5-Furosemide, model_path: ../Models/each_drug_note_v3_without_disease/5_checkpoint_Furosemide_0.7069-448 ------\n",
      "INFO 02-04 19:58:37 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/5_checkpoint_Furosemide_0.7069-448', tokenizer='../Models/each_drug_note_v3_without_disease/5_checkpoint_Furosemide_0.7069-448', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 19:59:07 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 19:59:07 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 19:59:07 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 19:59:12 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 6-Vancomycin, model_path: ../Models/each_drug_note_v3_without_disease/6_checkpoint_Vancomycin_0.6966-576 ------\n",
      "INFO 02-04 20:01:02 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/6_checkpoint_Vancomycin_0.6966-576', tokenizer='../Models/each_drug_note_v3_without_disease/6_checkpoint_Vancomycin_0.6966-576', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 20:01:31 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 20:01:31 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 20:01:31 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 20:01:36 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 7-Salbutamol, model_path: ../Models/each_drug_note_v3_without_disease/7_checkpoint_Salbutamol_0.6243-640 ------\n",
      "INFO 02-04 20:03:26 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/7_checkpoint_Salbutamol_0.6243-640', tokenizer='../Models/each_drug_note_v3_without_disease/7_checkpoint_Salbutamol_0.6243-640', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 20:03:55 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 20:03:55 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 20:03:55 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 20:04:00 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 8-Oxycodone, model_path: ../Models/each_drug_note_v3_without_disease/8_checkpoint_Oxycodone_0.5905-320 ------\n",
      "INFO 02-04 20:05:50 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/8_checkpoint_Oxycodone_0.5905-320', tokenizer='../Models/each_drug_note_v3_without_disease/8_checkpoint_Oxycodone_0.5905-320', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 20:06:19 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 20:06:19 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 20:06:19 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 20:06:24 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 9-Bisacodyl, model_path: ../Models/each_drug_note_v3_without_disease/9_checkpoint_Bisacodyl_0.5124-736 ------\n",
      "INFO 02-04 20:08:14 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/9_checkpoint_Bisacodyl_0.5124-736', tokenizer='../Models/each_drug_note_v3_without_disease/9_checkpoint_Bisacodyl_0.5124-736', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 20:08:43 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 20:08:44 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 20:08:44 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 20:08:48 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 10-Lorazepam, model_path: ../Models/each_drug_note_v3_without_disease/10_checkpoint_Lorazepam_0.4973-864 ------\n",
      "INFO 02-04 20:10:38 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/10_checkpoint_Lorazepam_0.4973-864', tokenizer='../Models/each_drug_note_v3_without_disease/10_checkpoint_Lorazepam_0.4973-864', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 20:11:09 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 20:11:09 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 20:11:09 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 20:11:14 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 11-Magnesium, model_path: ../Models/each_drug_note_v3_without_disease/11_checkpoint_Magnesium_0.6056-1120 ------\n",
      "INFO 02-04 20:13:03 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/11_checkpoint_Magnesium_0.6056-1120', tokenizer='../Models/each_drug_note_v3_without_disease/11_checkpoint_Magnesium_0.6056-1120', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 20:13:33 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 20:13:33 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 20:13:33 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 20:13:38 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 12-Ipratropium, model_path: ../Models/each_drug_note_v3_without_disease/12_checkpoint_Ipratropium_0.5416-416 ------\n",
      "INFO 02-04 20:15:28 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/12_checkpoint_Ipratropium_0.5416-416', tokenizer='../Models/each_drug_note_v3_without_disease/12_checkpoint_Ipratropium_0.5416-416', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 20:15:58 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 20:15:58 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 20:15:58 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 20:16:03 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 13-Acetylsalicylic acid, model_path: ../Models/each_drug_note_v3_without_disease/13_checkpoint_Acetylsalicylic acid_0.5719-448 ------\n",
      "INFO 02-04 20:17:52 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/13_checkpoint_Acetylsalicylic acid_0.5719-448', tokenizer='../Models/each_drug_note_v3_without_disease/13_checkpoint_Acetylsalicylic acid_0.5719-448', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 20:18:23 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 20:18:23 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 20:18:23 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 20:18:28 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:47<00:00, 22.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 14-Morphine, model_path: ../Models/each_drug_note_v3_without_disease/14_checkpoint_Morphine_0.4551-1056 ------\n",
      "INFO 02-04 20:20:19 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/14_checkpoint_Morphine_0.4551-1056', tokenizer='../Models/each_drug_note_v3_without_disease/14_checkpoint_Morphine_0.4551-1056', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 20:20:49 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 20:20:49 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 20:20:49 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 20:20:54 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 15-Fentanyl, model_path: ../Models/each_drug_note_v3_without_disease/15_checkpoint_Fentanyl_0.5213-544 ------\n",
      "INFO 02-04 20:22:43 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/15_checkpoint_Fentanyl_0.5213-544', tokenizer='../Models/each_drug_note_v3_without_disease/15_checkpoint_Fentanyl_0.5213-544', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 20:23:14 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 20:23:14 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 20:23:14 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 20:23:19 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 16-Ondansetron, model_path: ../Models/each_drug_note_v3_without_disease/16_checkpoint_Ondansetron_0.4133-1504 ------\n",
      "INFO 02-04 20:25:08 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/16_checkpoint_Ondansetron_0.4133-1504', tokenizer='../Models/each_drug_note_v3_without_disease/16_checkpoint_Ondansetron_0.4133-1504', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 20:25:39 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 20:25:39 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 20:25:39 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 20:25:44 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 17-Calcium gluconate, model_path: ../Models/each_drug_note_v3_without_disease/17_checkpoint_Calcium gluconate_0.2949-352 ------\n",
      "INFO 02-04 20:27:33 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/17_checkpoint_Calcium gluconate_0.2949-352', tokenizer='../Models/each_drug_note_v3_without_disease/17_checkpoint_Calcium gluconate_0.2949-352', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 20:28:04 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 20:28:04 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 20:28:04 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 20:28:09 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:47<00:00, 22.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 18-Propofol, model_path: ../Models/each_drug_note_v3_without_disease/18_checkpoint_Propofol_0.6354-672 ------\n",
      "INFO 02-04 20:29:59 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/18_checkpoint_Propofol_0.6354-672', tokenizer='../Models/each_drug_note_v3_without_disease/18_checkpoint_Propofol_0.6354-672', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 20:30:30 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 20:30:30 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 20:30:30 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 20:30:35 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 19-Lansoprazole, model_path: ../Models/each_drug_note_v3_without_disease/19_checkpoint_Lansoprazole_0.3972-608 ------\n",
      "INFO 02-04 20:32:24 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/19_checkpoint_Lansoprazole_0.3972-608', tokenizer='../Models/each_drug_note_v3_without_disease/19_checkpoint_Lansoprazole_0.3972-608', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 20:32:55 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 20:32:55 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 20:32:55 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 20:33:00 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 20-D-glucose, model_path: ../Models/each_drug_note_v3_without_disease/20_checkpoint_D-glucose_0.5026-704 ------\n",
      "INFO 02-04 20:34:49 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/20_checkpoint_D-glucose_0.5026-704', tokenizer='../Models/each_drug_note_v3_without_disease/20_checkpoint_D-glucose_0.5026-704', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 20:35:19 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 20:35:19 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 20:35:19 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 20:35:24 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 21-Levofloxacin, model_path: ../Models/each_drug_note_v3_without_disease/21_checkpoint_Levofloxacin_0.2791-448 ------\n",
      "INFO 02-04 20:37:14 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/21_checkpoint_Levofloxacin_0.2791-448', tokenizer='../Models/each_drug_note_v3_without_disease/21_checkpoint_Levofloxacin_0.2791-448', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 20:37:43 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 20:37:43 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 20:37:43 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 20:37:48 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 22-Midazolam, model_path: ../Models/each_drug_note_v3_without_disease/22_checkpoint_Midazolam_0.5126-608 ------\n",
      "INFO 02-04 20:39:38 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/22_checkpoint_Midazolam_0.5126-608', tokenizer='../Models/each_drug_note_v3_without_disease/22_checkpoint_Midazolam_0.5126-608', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 20:40:07 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 20:40:07 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 20:40:07 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 20:40:12 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 23-Metronidazole, model_path: ../Models/each_drug_note_v3_without_disease/23_checkpoint_Metronidazole_0.4546-480 ------\n",
      "INFO 02-04 20:42:02 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/23_checkpoint_Metronidazole_0.4546-480', tokenizer='../Models/each_drug_note_v3_without_disease/23_checkpoint_Metronidazole_0.4546-480', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 20:42:32 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 20:42:32 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 20:42:32 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 20:42:37 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 24-Hydromorphone, model_path: ../Models/each_drug_note_v3_without_disease/24_checkpoint_Hydromorphone_0.4573-448 ------\n",
      "INFO 02-04 20:44:26 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/24_checkpoint_Hydromorphone_0.4573-448', tokenizer='../Models/each_drug_note_v3_without_disease/24_checkpoint_Hydromorphone_0.4573-448', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 20:44:56 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 20:44:56 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 20:44:56 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 20:45:01 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 25-Atorvastatin, model_path: ../Models/each_drug_note_v3_without_disease/25_checkpoint_Atorvastatin_0.7058-576 ------\n",
      "INFO 02-04 20:46:50 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/25_checkpoint_Atorvastatin_0.7058-576', tokenizer='../Models/each_drug_note_v3_without_disease/25_checkpoint_Atorvastatin_0.7058-576', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 20:47:19 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 20:47:19 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 20:47:19 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 20:47:23 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 26-Lisinopril, model_path: ../Models/each_drug_note_v3_without_disease/26_checkpoint_Lisinopril_0.5400-672 ------\n",
      "INFO 02-04 20:49:13 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/26_checkpoint_Lisinopril_0.5400-672', tokenizer='../Models/each_drug_note_v3_without_disease/26_checkpoint_Lisinopril_0.5400-672', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 20:49:42 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 20:49:43 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 20:49:43 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 20:49:47 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 27-Warfarin, model_path: ../Models/each_drug_note_v3_without_disease/27_checkpoint_Warfarin_0.6093-704 ------\n",
      "INFO 02-04 20:51:37 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/27_checkpoint_Warfarin_0.6093-704', tokenizer='../Models/each_drug_note_v3_without_disease/27_checkpoint_Warfarin_0.6093-704', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 20:52:06 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 20:52:06 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 20:52:06 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 20:52:11 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 28-Lidocaine, model_path: ../Models/each_drug_note_v3_without_disease/28_checkpoint_Lidocaine_0.3150-1280 ------\n",
      "INFO 02-04 20:54:00 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/28_checkpoint_Lidocaine_0.3150-1280', tokenizer='../Models/each_drug_note_v3_without_disease/28_checkpoint_Lidocaine_0.3150-1280', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 20:54:30 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 20:54:30 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 20:54:30 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 20:54:35 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 29-Famotidine, model_path: ../Models/each_drug_note_v3_without_disease/29_checkpoint_Famotidine_0.3370-800 ------\n",
      "INFO 02-04 20:56:25 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/29_checkpoint_Famotidine_0.3370-800', tokenizer='../Models/each_drug_note_v3_without_disease/29_checkpoint_Famotidine_0.3370-800', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 20:56:55 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 20:56:55 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 20:56:55 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 20:57:00 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 30-Chlorhexidine, model_path: ../Models/each_drug_note_v3_without_disease/30_checkpoint_Chlorhexidine_0.5358-1408 ------\n",
      "INFO 02-04 20:58:49 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/30_checkpoint_Chlorhexidine_0.5358-1408', tokenizer='../Models/each_drug_note_v3_without_disease/30_checkpoint_Chlorhexidine_0.5358-1408', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 20:59:19 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 20:59:19 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 20:59:19 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 20:59:23 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 31-Hydralazine, model_path: ../Models/each_drug_note_v3_without_disease/31_checkpoint_Hydralazine_0.3441-992 ------\n",
      "INFO 02-04 21:01:13 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/31_checkpoint_Hydralazine_0.3441-992', tokenizer='../Models/each_drug_note_v3_without_disease/31_checkpoint_Hydralazine_0.3441-992', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 21:01:42 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 21:01:42 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 21:01:42 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 21:01:47 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 32-Phenylephrine, model_path: ../Models/each_drug_note_v3_without_disease/32_checkpoint_Phenylephrine_0.4924-640 ------\n",
      "INFO 02-04 21:03:37 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/32_checkpoint_Phenylephrine_0.4924-640', tokenizer='../Models/each_drug_note_v3_without_disease/32_checkpoint_Phenylephrine_0.4924-640', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 21:04:06 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 21:04:06 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 21:04:06 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 21:04:11 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 33-Metoclopramide, model_path: ../Models/each_drug_note_v3_without_disease/33_checkpoint_Metoclopramide_0.5540-832 ------\n",
      "INFO 02-04 21:06:00 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/33_checkpoint_Metoclopramide_0.5540-832', tokenizer='../Models/each_drug_note_v3_without_disease/33_checkpoint_Metoclopramide_0.5540-832', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 21:06:29 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 21:06:29 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 21:06:29 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 21:06:34 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:47<00:00, 22.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 34-Ciprofloxacin, model_path: ../Models/each_drug_note_v3_without_disease/34_checkpoint_Ciprofloxacin_0.3607-1376 ------\n",
      "INFO 02-04 21:08:24 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/34_checkpoint_Ciprofloxacin_0.3607-1376', tokenizer='../Models/each_drug_note_v3_without_disease/34_checkpoint_Ciprofloxacin_0.3607-1376', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 21:08:52 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 21:08:52 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 21:08:52 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 21:08:58 model_runner.py:547] Graph capturing finished in 6 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:47<00:00, 21.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 35-Spironolactone, model_path: ../Models/each_drug_note_v3_without_disease/35_checkpoint_Spironolactone_0.4736-512 ------\n",
      "INFO 02-04 21:10:49 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/35_checkpoint_Spironolactone_0.4736-512', tokenizer='../Models/each_drug_note_v3_without_disease/35_checkpoint_Spironolactone_0.4736-512', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 21:11:18 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 21:11:18 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 21:11:18 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 21:11:23 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 36-Nitroglycerin, model_path: ../Models/each_drug_note_v3_without_disease/36_checkpoint_Nitroglycerin_0.5611-576 ------\n",
      "INFO 02-04 21:13:13 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/36_checkpoint_Nitroglycerin_0.5611-576', tokenizer='../Models/each_drug_note_v3_without_disease/36_checkpoint_Nitroglycerin_0.5611-576', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 21:13:42 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 21:13:42 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 21:13:42 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 21:13:47 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:47<00:00, 22.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 37-Prednisone, model_path: ../Models/each_drug_note_v3_without_disease/37_checkpoint_Prednisone_0.6485-704 ------\n",
      "INFO 02-04 21:15:37 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/37_checkpoint_Prednisone_0.6485-704', tokenizer='../Models/each_drug_note_v3_without_disease/37_checkpoint_Prednisone_0.6485-704', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 21:16:07 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 21:16:07 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 21:16:07 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 21:16:12 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 38-Sodium bicarbonate, model_path: ../Models/each_drug_note_v3_without_disease/38_checkpoint_Sodium bicarbonate_0.3427-1472 ------\n",
      "INFO 02-04 21:18:01 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/38_checkpoint_Sodium bicarbonate_0.3427-1472', tokenizer='../Models/each_drug_note_v3_without_disease/38_checkpoint_Sodium bicarbonate_0.3427-1472', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 21:18:31 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 21:18:31 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 21:18:31 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 21:18:37 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:47<00:00, 21.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 39-Tazobactam, model_path: ../Models/each_drug_note_v3_without_disease/39_checkpoint_Tazobactam_0.3814-960 ------\n",
      "INFO 02-04 21:20:28 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/39_checkpoint_Tazobactam_0.3814-960', tokenizer='../Models/each_drug_note_v3_without_disease/39_checkpoint_Tazobactam_0.3814-960', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 21:20:58 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 21:20:58 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 21:20:58 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 21:21:03 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 40-Diphenhydramine, model_path: ../Models/each_drug_note_v3_without_disease/40_checkpoint_Diphenhydramine_0.2859-1280 ------\n",
      "INFO 02-04 21:22:53 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/40_checkpoint_Diphenhydramine_0.2859-1280', tokenizer='../Models/each_drug_note_v3_without_disease/40_checkpoint_Diphenhydramine_0.2859-1280', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 21:23:23 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 21:23:24 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 21:23:24 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 21:23:29 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 41-Lactulose, model_path: ../Models/each_drug_note_v3_without_disease/41_checkpoint_Lactulose_0.3848-928 ------\n",
      "INFO 02-04 21:25:19 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/41_checkpoint_Lactulose_0.3848-928', tokenizer='../Models/each_drug_note_v3_without_disease/41_checkpoint_Lactulose_0.3848-928', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 21:25:49 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 21:25:49 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 21:25:49 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 21:25:54 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 42-Omeprazole, model_path: ../Models/each_drug_note_v3_without_disease/42_checkpoint_Omeprazole_0.3982-160 ------\n",
      "INFO 02-04 21:27:43 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/42_checkpoint_Omeprazole_0.3982-160', tokenizer='../Models/each_drug_note_v3_without_disease/42_checkpoint_Omeprazole_0.3982-160', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 21:28:15 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 21:28:15 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 21:28:15 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 21:28:20 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 43-Clopidogrel, model_path: ../Models/each_drug_note_v3_without_disease/43_checkpoint_Clopidogrel_0.6504-896 ------\n",
      "INFO 02-04 21:30:10 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/43_checkpoint_Clopidogrel_0.6504-896', tokenizer='../Models/each_drug_note_v3_without_disease/43_checkpoint_Clopidogrel_0.6504-896', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 21:30:40 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 21:30:40 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 21:30:40 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 21:30:45 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 44-Zolpidem, model_path: ../Models/each_drug_note_v3_without_disease/44_checkpoint_Zolpidem_0.2159-480 ------\n",
      "INFO 02-04 21:32:35 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/44_checkpoint_Zolpidem_0.2159-480', tokenizer='../Models/each_drug_note_v3_without_disease/44_checkpoint_Zolpidem_0.2159-480', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 21:33:06 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 21:33:07 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 21:33:07 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 21:33:12 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 45-Fluticasone propionate, model_path: ../Models/each_drug_note_v3_without_disease/45_checkpoint_Fluticasone propionate_0.6170-256 ------\n",
      "INFO 02-04 21:35:02 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/45_checkpoint_Fluticasone propionate_0.6170-256', tokenizer='../Models/each_drug_note_v3_without_disease/45_checkpoint_Fluticasone propionate_0.6170-256', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 21:35:32 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 21:35:32 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 21:35:32 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 21:35:37 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:47<00:00, 22.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 46-Levothyroxine, model_path: ../Models/each_drug_note_v3_without_disease/46_checkpoint_Levothyroxine_0.9068-224 ------\n",
      "INFO 02-04 21:37:28 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/46_checkpoint_Levothyroxine_0.9068-224', tokenizer='../Models/each_drug_note_v3_without_disease/46_checkpoint_Levothyroxine_0.9068-224', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 21:37:58 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 21:37:58 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 21:37:58 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 21:38:03 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 47-Norepinephrine, model_path: ../Models/each_drug_note_v3_without_disease/47_checkpoint_Norepinephrine_0.3356-704 ------\n",
      "INFO 02-04 21:39:53 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/47_checkpoint_Norepinephrine_0.3356-704', tokenizer='../Models/each_drug_note_v3_without_disease/47_checkpoint_Norepinephrine_0.3356-704', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 21:40:22 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 21:40:23 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 21:40:23 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 21:40:28 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 48-Simvastatin, model_path: ../Models/each_drug_note_v3_without_disease/48_checkpoint_Simvastatin_0.7268-576 ------\n",
      "INFO 02-04 21:42:18 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/48_checkpoint_Simvastatin_0.7268-576', tokenizer='../Models/each_drug_note_v3_without_disease/48_checkpoint_Simvastatin_0.7268-576', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 21:42:46 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 21:42:46 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 21:42:46 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 21:42:51 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 49-Ranitidine, model_path: ../Models/each_drug_note_v3_without_disease/49_checkpoint_Ranitidine_0.5212-544 ------\n",
      "INFO 02-04 21:44:41 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/49_checkpoint_Ranitidine_0.5212-544', tokenizer='../Models/each_drug_note_v3_without_disease/49_checkpoint_Ranitidine_0.5212-544', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 21:45:11 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 21:45:11 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 21:45:11 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 21:45:16 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 50-Monopotassium phosphate, model_path: ../Models/each_drug_note_v3_without_disease/50_checkpoint_Monopotassium phosphate_0.2511-640 ------\n",
      "INFO 02-04 21:47:06 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/50_checkpoint_Monopotassium phosphate_0.2511-640', tokenizer='../Models/each_drug_note_v3_without_disease/50_checkpoint_Monopotassium phosphate_0.2511-640', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 21:47:36 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 21:47:36 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 21:47:36 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 21:47:41 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:47<00:00, 22.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 51-Thiamine, model_path: ../Models/each_drug_note_v3_without_disease/51_checkpoint_Thiamine_0.5681-608 ------\n",
      "INFO 02-04 21:49:31 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/51_checkpoint_Thiamine_0.5681-608', tokenizer='../Models/each_drug_note_v3_without_disease/51_checkpoint_Thiamine_0.5681-608', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 21:50:01 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 21:50:01 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 21:50:01 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 21:50:06 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 52-Piperacillin, model_path: ../Models/each_drug_note_v3_without_disease/52_checkpoint_Piperacillin_0.3671-1216 ------\n",
      "INFO 02-04 21:51:56 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/52_checkpoint_Piperacillin_0.3671-1216', tokenizer='../Models/each_drug_note_v3_without_disease/52_checkpoint_Piperacillin_0.3671-1216', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 21:52:25 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 21:52:26 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 21:52:26 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 21:52:31 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 53-Amiodarone, model_path: ../Models/each_drug_note_v3_without_disease/53_checkpoint_Amiodarone_0.5815-480 ------\n",
      "INFO 02-04 21:54:21 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/53_checkpoint_Amiodarone_0.5815-480', tokenizer='../Models/each_drug_note_v3_without_disease/53_checkpoint_Amiodarone_0.5815-480', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 21:54:51 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 21:54:51 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 21:54:51 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 21:54:56 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 54-Haloperidol, model_path: ../Models/each_drug_note_v3_without_disease/54_checkpoint_Haloperidol_0.2635-896 ------\n",
      "INFO 02-04 21:56:45 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/54_checkpoint_Haloperidol_0.2635-896', tokenizer='../Models/each_drug_note_v3_without_disease/54_checkpoint_Haloperidol_0.2635-896', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 21:57:15 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 21:57:15 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 21:57:15 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 21:57:20 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 55-Ceftriaxone, model_path: ../Models/each_drug_note_v3_without_disease/55_checkpoint_Ceftriaxone_0.2619-672 ------\n",
      "INFO 02-04 21:59:10 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/55_checkpoint_Ceftriaxone_0.2619-672', tokenizer='../Models/each_drug_note_v3_without_disease/55_checkpoint_Ceftriaxone_0.2619-672', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 21:59:40 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 21:59:40 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 21:59:40 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 21:59:45 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 56-Trazodone, model_path: ../Models/each_drug_note_v3_without_disease/56_checkpoint_Trazodone_0.2897-352 ------\n",
      "INFO 02-04 22:01:35 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/56_checkpoint_Trazodone_0.2897-352', tokenizer='../Models/each_drug_note_v3_without_disease/56_checkpoint_Trazodone_0.2897-352', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 22:02:04 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 22:02:04 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 22:02:04 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 22:02:09 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 57-Phylloquinone, model_path: ../Models/each_drug_note_v3_without_disease/57_checkpoint_Phylloquinone_0.2936-768 ------\n",
      "INFO 02-04 22:03:59 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/57_checkpoint_Phylloquinone_0.2936-768', tokenizer='../Models/each_drug_note_v3_without_disease/57_checkpoint_Phylloquinone_0.2936-768', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 22:04:28 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 22:04:28 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 22:04:28 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 22:04:33 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 58-Cefazolin, model_path: ../Models/each_drug_note_v3_without_disease/58_checkpoint_Cefazolin_0.3692-544 ------\n",
      "INFO 02-04 22:06:23 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/58_checkpoint_Cefazolin_0.3692-544', tokenizer='../Models/each_drug_note_v3_without_disease/58_checkpoint_Cefazolin_0.3692-544', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 22:06:53 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 22:06:53 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 22:06:53 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 22:06:58 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 59-Miconazole, model_path: ../Models/each_drug_note_v3_without_disease/59_checkpoint_Miconazole_0.2383-448 ------\n",
      "INFO 02-04 22:08:48 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/59_checkpoint_Miconazole_0.2383-448', tokenizer='../Models/each_drug_note_v3_without_disease/59_checkpoint_Miconazole_0.2383-448', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 22:09:20 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 22:09:20 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 22:09:20 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 22:09:25 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 60-Acetylcysteine, model_path: ../Models/each_drug_note_v3_without_disease/60_checkpoint_Acetylcysteine_0.1859-704 ------\n",
      "INFO 02-04 22:11:15 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/60_checkpoint_Acetylcysteine_0.1859-704', tokenizer='../Models/each_drug_note_v3_without_disease/60_checkpoint_Acetylcysteine_0.1859-704', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 22:11:45 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 22:11:45 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 22:11:45 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 22:11:50 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:47<00:00, 22.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 61-Cefepime, model_path: ../Models/each_drug_note_v3_without_disease/61_checkpoint_Cefepime_0.3687-608 ------\n",
      "INFO 02-04 22:13:40 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/61_checkpoint_Cefepime_0.3687-608', tokenizer='../Models/each_drug_note_v3_without_disease/61_checkpoint_Cefepime_0.3687-608', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 22:14:11 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 22:14:11 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 22:14:11 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 22:14:16 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 62-Gabapentin, model_path: ../Models/each_drug_note_v3_without_disease/62_checkpoint_Gabapentin_0.6127-320 ------\n",
      "INFO 02-04 22:16:06 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/62_checkpoint_Gabapentin_0.6127-320', tokenizer='../Models/each_drug_note_v3_without_disease/62_checkpoint_Gabapentin_0.6127-320', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 22:16:35 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 22:16:36 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 22:16:36 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 22:16:41 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 63-Enalaprilat, model_path: ../Models/each_drug_note_v3_without_disease/63_checkpoint_Enalaprilat_0.1111-512 ------\n",
      "INFO 02-04 22:18:30 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/63_checkpoint_Enalaprilat_0.1111-512', tokenizer='../Models/each_drug_note_v3_without_disease/63_checkpoint_Enalaprilat_0.1111-512', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 22:19:00 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 22:19:00 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 22:19:00 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 22:19:05 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 64-Amlodipine, model_path: ../Models/each_drug_note_v3_without_disease/64_checkpoint_Amlodipine_0.5920-640 ------\n",
      "INFO 02-04 22:20:55 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/64_checkpoint_Amlodipine_0.5920-640', tokenizer='../Models/each_drug_note_v3_without_disease/64_checkpoint_Amlodipine_0.5920-640', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 22:21:24 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 22:21:25 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 22:21:25 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 22:21:30 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 65-Folic acid, model_path: ../Models/each_drug_note_v3_without_disease/65_checkpoint_Folic acid_0.4737-352 ------\n",
      "INFO 02-04 22:23:20 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/65_checkpoint_Folic acid_0.4737-352', tokenizer='../Models/each_drug_note_v3_without_disease/65_checkpoint_Folic acid_0.4737-352', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 22:23:49 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 22:23:49 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 22:23:49 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 22:23:54 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 66-Nystatin, model_path: ../Models/each_drug_note_v3_without_disease/66_checkpoint_Nystatin_0.2538-416 ------\n",
      "INFO 02-04 22:25:44 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/66_checkpoint_Nystatin_0.2538-416', tokenizer='../Models/each_drug_note_v3_without_disease/66_checkpoint_Nystatin_0.2538-416', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 22:26:14 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 22:26:14 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 22:26:14 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 22:26:19 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 67-Cyanocobalamin, model_path: ../Models/each_drug_note_v3_without_disease/67_checkpoint_Cyanocobalamin_0.5517-1024 ------\n",
      "INFO 02-04 22:28:09 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/67_checkpoint_Cyanocobalamin_0.5517-1024', tokenizer='../Models/each_drug_note_v3_without_disease/67_checkpoint_Cyanocobalamin_0.5517-1024', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 22:28:38 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 22:28:38 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 22:28:38 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 22:28:43 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:47<00:00, 22.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 68-Prochlorperazine, model_path: ../Models/each_drug_note_v3_without_disease/68_checkpoint_Prochlorperazine_0.2043-1152 ------\n",
      "INFO 02-04 22:30:33 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/68_checkpoint_Prochlorperazine_0.2043-1152', tokenizer='../Models/each_drug_note_v3_without_disease/68_checkpoint_Prochlorperazine_0.2043-1152', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 22:31:03 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 22:31:03 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 22:31:03 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 22:31:08 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 69-Diltiazem, model_path: ../Models/each_drug_note_v3_without_disease/69_checkpoint_Diltiazem_0.3906-1056 ------\n",
      "INFO 02-04 22:32:58 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/69_checkpoint_Diltiazem_0.3906-1056', tokenizer='../Models/each_drug_note_v3_without_disease/69_checkpoint_Diltiazem_0.3906-1056', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 22:33:28 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 22:33:28 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 22:33:28 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 22:33:33 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 70-Sulfamethoxazole, model_path: ../Models/each_drug_note_v3_without_disease/70_checkpoint_Sulfamethoxazole_0.4401-672 ------\n",
      "INFO 02-04 22:35:23 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/70_checkpoint_Sulfamethoxazole_0.4401-672', tokenizer='../Models/each_drug_note_v3_without_disease/70_checkpoint_Sulfamethoxazole_0.4401-672', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 22:35:52 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 22:35:53 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 22:35:53 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 22:35:58 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:47<00:00, 22.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 71-Methylprednisolone hemisuccinate, model_path: ../Models/each_drug_note_v3_without_disease/71_checkpoint_Methylprednisolone hemisuccinate_0.3956-608 ------\n",
      "INFO 02-04 22:37:48 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/71_checkpoint_Methylprednisolone hemisuccinate_0.3956-608', tokenizer='../Models/each_drug_note_v3_without_disease/71_checkpoint_Methylprednisolone hemisuccinate_0.3956-608', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 22:38:18 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 22:38:18 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 22:38:18 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 22:38:23 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:48<00:00, 21.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 72-Trimethoprim, model_path: ../Models/each_drug_note_v3_without_disease/72_checkpoint_Trimethoprim_0.4498-800 ------\n",
      "INFO 02-04 22:40:14 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/72_checkpoint_Trimethoprim_0.4498-800', tokenizer='../Models/each_drug_note_v3_without_disease/72_checkpoint_Trimethoprim_0.4498-800', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 22:40:44 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 22:40:44 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 22:40:44 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 22:40:49 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:47<00:00, 22.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 73-Dopamine, model_path: ../Models/each_drug_note_v3_without_disease/73_checkpoint_Dopamine_0.0914-1376 ------\n",
      "INFO 02-04 22:42:39 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/73_checkpoint_Dopamine_0.0914-1376', tokenizer='../Models/each_drug_note_v3_without_disease/73_checkpoint_Dopamine_0.0914-1376', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 22:43:09 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 22:43:09 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 22:43:09 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 22:43:14 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 74-Fluconazole, model_path: ../Models/each_drug_note_v3_without_disease/74_checkpoint_Fluconazole_0.4798-640 ------\n",
      "INFO 02-04 22:45:04 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/74_checkpoint_Fluconazole_0.4798-640', tokenizer='../Models/each_drug_note_v3_without_disease/74_checkpoint_Fluconazole_0.4798-640', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 22:45:34 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 22:45:34 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 22:45:34 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 22:45:39 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 75-Glycopyrronium, model_path: ../Models/each_drug_note_v3_without_disease/75_checkpoint_Glycopyrronium_0.7642-224 ------\n",
      "INFO 02-04 22:47:29 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/75_checkpoint_Glycopyrronium_0.7642-224', tokenizer='../Models/each_drug_note_v3_without_disease/75_checkpoint_Glycopyrronium_0.7642-224', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 22:47:59 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 22:47:59 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 22:47:59 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 22:48:04 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:47<00:00, 22.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 76-Meropenem, model_path: ../Models/each_drug_note_v3_without_disease/76_checkpoint_Meropenem_0.3785-672 ------\n",
      "INFO 02-04 22:49:54 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/76_checkpoint_Meropenem_0.3785-672', tokenizer='../Models/each_drug_note_v3_without_disease/76_checkpoint_Meropenem_0.3785-672', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 22:50:24 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 22:50:24 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 22:50:24 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 22:50:29 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 77-Ergocalciferol, model_path: ../Models/each_drug_note_v3_without_disease/77_checkpoint_Ergocalciferol_0.3516-544 ------\n",
      "INFO 02-04 22:52:19 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/77_checkpoint_Ergocalciferol_0.3516-544', tokenizer='../Models/each_drug_note_v3_without_disease/77_checkpoint_Ergocalciferol_0.3516-544', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 22:52:49 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 22:52:49 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 22:52:49 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 22:52:54 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:47<00:00, 22.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 78-Sucralfate, model_path: ../Models/each_drug_note_v3_without_disease/78_checkpoint_Sucralfate_0.2722-800 ------\n",
      "INFO 02-04 22:54:44 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/78_checkpoint_Sucralfate_0.2722-800', tokenizer='../Models/each_drug_note_v3_without_disease/78_checkpoint_Sucralfate_0.2722-800', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 22:55:14 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 22:55:14 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 22:55:14 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 22:55:19 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:47<00:00, 22.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 79-Hydrocortisone, model_path: ../Models/each_drug_note_v3_without_disease/79_checkpoint_Hydrocortisone_0.3036-832 ------\n",
      "INFO 02-04 22:57:09 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/79_checkpoint_Hydrocortisone_0.3036-832', tokenizer='../Models/each_drug_note_v3_without_disease/79_checkpoint_Hydrocortisone_0.3036-832', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 22:57:39 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 22:57:39 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 22:57:39 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 22:57:44 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:47<00:00, 22.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 80-Guaifenesin, model_path: ../Models/each_drug_note_v3_without_disease/80_checkpoint_Guaifenesin_0.2192-800 ------\n",
      "INFO 02-04 22:59:34 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/80_checkpoint_Guaifenesin_0.2192-800', tokenizer='../Models/each_drug_note_v3_without_disease/80_checkpoint_Guaifenesin_0.2192-800', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 23:00:04 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 23:00:04 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 23:00:04 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 23:00:09 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 81-Meperidine, model_path: ../Models/each_drug_note_v3_without_disease/81_checkpoint_Meperidine_0.2415-160 ------\n",
      "INFO 02-04 23:01:59 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/81_checkpoint_Meperidine_0.2415-160', tokenizer='../Models/each_drug_note_v3_without_disease/81_checkpoint_Meperidine_0.2415-160', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 23:02:31 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 23:02:31 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 23:02:31 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 23:02:36 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 82-Methylprednisolone, model_path: ../Models/each_drug_note_v3_without_disease/82_checkpoint_Methylprednisolone_0.3686-512 ------\n",
      "INFO 02-04 23:04:26 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/82_checkpoint_Methylprednisolone_0.3686-512', tokenizer='../Models/each_drug_note_v3_without_disease/82_checkpoint_Methylprednisolone_0.3686-512', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 23:04:57 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 23:04:57 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 23:04:57 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 23:05:02 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:47<00:00, 22.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 83-Captopril, model_path: ../Models/each_drug_note_v3_without_disease/83_checkpoint_Captopril_0.1985-448 ------\n",
      "INFO 02-04 23:06:53 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/83_checkpoint_Captopril_0.1985-448', tokenizer='../Models/each_drug_note_v3_without_disease/83_checkpoint_Captopril_0.1985-448', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 23:07:24 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 23:07:24 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 23:07:24 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 23:07:29 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 84-Salmeterol, model_path: ../Models/each_drug_note_v3_without_disease/84_checkpoint_Salmeterol_0.6184-320 ------\n",
      "INFO 02-04 23:09:19 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/84_checkpoint_Salmeterol_0.6184-320', tokenizer='../Models/each_drug_note_v3_without_disease/84_checkpoint_Salmeterol_0.6184-320', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 23:09:51 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 23:09:51 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 23:09:51 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 23:09:56 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 85-Neostigmine, model_path: ../Models/each_drug_note_v3_without_disease/85_checkpoint_Neostigmine_0.8795-1056 ------\n",
      "INFO 02-04 23:11:45 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/85_checkpoint_Neostigmine_0.8795-1056', tokenizer='../Models/each_drug_note_v3_without_disease/85_checkpoint_Neostigmine_0.8795-1056', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 23:12:16 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 23:12:17 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 23:12:17 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 23:12:22 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 86-Olanzapine, model_path: ../Models/each_drug_note_v3_without_disease/86_checkpoint_Olanzapine_0.2577-544 ------\n",
      "INFO 02-04 23:14:12 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/86_checkpoint_Olanzapine_0.2577-544', tokenizer='../Models/each_drug_note_v3_without_disease/86_checkpoint_Olanzapine_0.2577-544', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 23:14:43 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 23:14:43 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 23:14:43 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 23:14:48 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 87-Magnesium hydroxide, model_path: ../Models/each_drug_note_v3_without_disease/87_checkpoint_Magnesium hydroxide_0.1443-1216 ------\n",
      "INFO 02-04 23:16:38 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/87_checkpoint_Magnesium hydroxide_0.1443-1216', tokenizer='../Models/each_drug_note_v3_without_disease/87_checkpoint_Magnesium hydroxide_0.1443-1216', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 23:17:09 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 23:17:09 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 23:17:09 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 23:17:15 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:47<00:00, 22.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 88-Labetalol, model_path: ../Models/each_drug_note_v3_without_disease/88_checkpoint_Labetalol_0.2597-704 ------\n",
      "INFO 02-04 23:19:05 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/88_checkpoint_Labetalol_0.2597-704', tokenizer='../Models/each_drug_note_v3_without_disease/88_checkpoint_Labetalol_0.2597-704', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 23:19:37 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 23:19:37 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 23:19:37 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 23:19:42 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 89-Ibuprofen, model_path: ../Models/each_drug_note_v3_without_disease/89_checkpoint_Ibuprofen_0.1268-1216 ------\n",
      "INFO 02-04 23:21:32 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/89_checkpoint_Ibuprofen_0.1268-1216', tokenizer='../Models/each_drug_note_v3_without_disease/89_checkpoint_Ibuprofen_0.1268-1216', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 23:22:03 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 23:22:03 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 23:22:03 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 23:22:08 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 90-Digoxin, model_path: ../Models/each_drug_note_v3_without_disease/90_checkpoint_Digoxin_0.5572-256 ------\n",
      "INFO 02-04 23:23:58 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/90_checkpoint_Digoxin_0.5572-256', tokenizer='../Models/each_drug_note_v3_without_disease/90_checkpoint_Digoxin_0.5572-256', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 23:24:28 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 23:24:28 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 23:24:28 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 23:24:33 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 91-Ampicillin, model_path: ../Models/each_drug_note_v3_without_disease/91_checkpoint_Ampicillin_0.1781-992 ------\n",
      "INFO 02-04 23:26:23 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/91_checkpoint_Ampicillin_0.1781-992', tokenizer='../Models/each_drug_note_v3_without_disease/91_checkpoint_Ampicillin_0.1781-992', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 23:26:53 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 23:26:53 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 23:26:53 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 23:26:59 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 92-Citalopram, model_path: ../Models/each_drug_note_v3_without_disease/92_checkpoint_Citalopram_0.7547-864 ------\n",
      "INFO 02-04 23:28:49 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/92_checkpoint_Citalopram_0.7547-864', tokenizer='../Models/each_drug_note_v3_without_disease/92_checkpoint_Citalopram_0.7547-864', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 23:29:19 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 23:29:19 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 23:29:19 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 23:29:24 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 93-Pyridoxine, model_path: ../Models/each_drug_note_v3_without_disease/93_checkpoint_Pyridoxine_0.5500-352 ------\n",
      "INFO 02-04 23:31:14 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/93_checkpoint_Pyridoxine_0.5500-352', tokenizer='../Models/each_drug_note_v3_without_disease/93_checkpoint_Pyridoxine_0.5500-352', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 23:31:44 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 23:31:44 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 23:31:44 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 23:31:49 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 94-Ascorbic acid, model_path: ../Models/each_drug_note_v3_without_disease/94_checkpoint_Ascorbic acid_0.5155-416 ------\n",
      "INFO 02-04 23:33:39 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/94_checkpoint_Ascorbic acid_0.5155-416', tokenizer='../Models/each_drug_note_v3_without_disease/94_checkpoint_Ascorbic acid_0.5155-416', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 23:34:09 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 23:34:09 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 23:34:09 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 23:34:14 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 95-Niacin, model_path: ../Models/each_drug_note_v3_without_disease/95_checkpoint_Niacin_0.5227-192 ------\n",
      "INFO 02-04 23:36:04 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/95_checkpoint_Niacin_0.5227-192', tokenizer='../Models/each_drug_note_v3_without_disease/95_checkpoint_Niacin_0.5227-192', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 23:36:34 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 23:36:34 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 23:36:34 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 23:36:39 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 96-Dolasetron, model_path: ../Models/each_drug_note_v3_without_disease/96_checkpoint_Dolasetron_0.1111-768 ------\n",
      "INFO 02-04 23:38:29 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/96_checkpoint_Dolasetron_0.1111-768', tokenizer='../Models/each_drug_note_v3_without_disease/96_checkpoint_Dolasetron_0.1111-768', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 23:38:59 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 23:38:59 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 23:38:59 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 23:39:04 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 97-Tetracosactide, model_path: ../Models/each_drug_note_v3_without_disease/97_checkpoint_Tetracosactide_0.1329-416 ------\n",
      "INFO 02-04 23:40:54 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/97_checkpoint_Tetracosactide_0.1329-416', tokenizer='../Models/each_drug_note_v3_without_disease/97_checkpoint_Tetracosactide_0.1329-416', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 23:41:24 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 23:41:24 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 23:41:24 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 23:41:30 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:47<00:00, 22.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 98-Atropine, model_path: ../Models/each_drug_note_v3_without_disease/98_checkpoint_Atropine_0.4493-352 ------\n",
      "INFO 02-04 23:43:20 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/98_checkpoint_Atropine_0.4493-352', tokenizer='../Models/each_drug_note_v3_without_disease/98_checkpoint_Atropine_0.4493-352', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 23:43:49 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 23:43:50 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 23:43:50 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 23:43:55 model_runner.py:547] Graph capturing finished in 6 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 99-Sevelamer, model_path: ../Models/each_drug_note_v3_without_disease/99_checkpoint_Sevelamer_0.5568-1120 ------\n",
      "INFO 02-04 23:45:45 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/99_checkpoint_Sevelamer_0.5568-1120', tokenizer='../Models/each_drug_note_v3_without_disease/99_checkpoint_Sevelamer_0.5568-1120', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 23:46:14 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 23:46:14 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 23:46:14 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 23:46:19 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 100-Riboflavin, model_path: ../Models/each_drug_note_v3_without_disease/100_checkpoint_Riboflavin_0.5667-416 ------\n",
      "INFO 02-04 23:48:09 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/100_checkpoint_Riboflavin_0.5667-416', tokenizer='../Models/each_drug_note_v3_without_disease/100_checkpoint_Riboflavin_0.5667-416', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 23:48:39 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 23:48:39 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 23:48:39 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 23:48:44 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 101-Biotin, model_path: ../Models/each_drug_note_v3_without_disease/101_checkpoint_Biotin_0.5625-192 ------\n",
      "INFO 02-04 23:50:34 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/101_checkpoint_Biotin_0.5625-192', tokenizer='../Models/each_drug_note_v3_without_disease/101_checkpoint_Biotin_0.5625-192', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 23:51:03 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 23:51:04 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 23:51:04 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 23:51:09 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 102-Pantothenic acid, model_path: ../Models/each_drug_note_v3_without_disease/102_checkpoint_Pantothenic acid_0.5720-416 ------\n",
      "INFO 02-04 23:52:58 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/102_checkpoint_Pantothenic acid_0.5720-416', tokenizer='../Models/each_drug_note_v3_without_disease/102_checkpoint_Pantothenic acid_0.5720-416', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 23:53:28 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 23:53:28 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 23:53:28 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 23:53:33 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:47<00:00, 22.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 103-Sodium chloride, model_path: ../Models/each_drug_note_v3_without_disease/103_checkpoint_Sodium chloride_0.4181-640 ------\n",
      "INFO 02-04 23:55:23 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/103_checkpoint_Sodium chloride_0.4181-640', tokenizer='../Models/each_drug_note_v3_without_disease/103_checkpoint_Sodium chloride_0.4181-640', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 23:55:54 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 23:55:54 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 23:55:54 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 23:55:59 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:47<00:00, 22.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 104-Hydrocortisone succinate, model_path: ../Models/each_drug_note_v3_without_disease/104_checkpoint_Hydrocortisone succinate_0.2912-672 ------\n",
      "INFO 02-04 23:57:49 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/104_checkpoint_Hydrocortisone succinate_0.2912-672', tokenizer='../Models/each_drug_note_v3_without_disease/104_checkpoint_Hydrocortisone succinate_0.2912-672', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-04 23:58:18 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-04 23:58:18 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-04 23:58:18 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-04 23:58:24 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:47<00:00, 21.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 105-Dexamethasone, model_path: ../Models/each_drug_note_v3_without_disease/105_checkpoint_Dexamethasone_0.4313-608 ------\n",
      "INFO 02-05 00:00:15 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/105_checkpoint_Dexamethasone_0.4313-608', tokenizer='../Models/each_drug_note_v3_without_disease/105_checkpoint_Dexamethasone_0.4313-608', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-05 00:00:45 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-05 00:00:45 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-05 00:00:45 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-05 00:00:50 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:47<00:00, 22.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 106-Tramadol, model_path: ../Models/each_drug_note_v3_without_disease/106_checkpoint_Tramadol_0.2312-768 ------\n",
      "INFO 02-05 00:02:40 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/106_checkpoint_Tramadol_0.2312-768', tokenizer='../Models/each_drug_note_v3_without_disease/106_checkpoint_Tramadol_0.2312-768', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-05 00:03:10 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-05 00:03:10 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-05 00:03:10 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-05 00:03:15 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 107-Ketorolac, model_path: ../Models/each_drug_note_v3_without_disease/107_checkpoint_Ketorolac_0.3092-672 ------\n",
      "INFO 02-05 00:05:05 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/107_checkpoint_Ketorolac_0.3092-672', tokenizer='../Models/each_drug_note_v3_without_disease/107_checkpoint_Ketorolac_0.3092-672', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-05 00:05:35 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-05 00:05:35 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-05 00:05:35 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-05 00:05:40 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:47<00:00, 22.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 108-Levetiracetam, model_path: ../Models/each_drug_note_v3_without_disease/108_checkpoint_Levetiracetam_0.5441-640 ------\n",
      "INFO 02-05 00:07:31 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/108_checkpoint_Levetiracetam_0.5441-640', tokenizer='../Models/each_drug_note_v3_without_disease/108_checkpoint_Levetiracetam_0.5441-640', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-05 00:08:00 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-05 00:08:01 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-05 00:08:01 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-05 00:08:06 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:47<00:00, 22.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 109-Gentamicin, model_path: ../Models/each_drug_note_v3_without_disease/109_checkpoint_Gentamicin_0.1443-512 ------\n",
      "INFO 02-05 00:09:56 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/109_checkpoint_Gentamicin_0.1443-512', tokenizer='../Models/each_drug_note_v3_without_disease/109_checkpoint_Gentamicin_0.1443-512', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-05 00:10:27 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-05 00:10:27 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-05 00:10:27 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-05 00:10:32 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 110-Azithromycin, model_path: ../Models/each_drug_note_v3_without_disease/110_checkpoint_Azithromycin_0.2105-1056 ------\n",
      "INFO 02-05 00:12:22 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/110_checkpoint_Azithromycin_0.2105-1056', tokenizer='../Models/each_drug_note_v3_without_disease/110_checkpoint_Azithromycin_0.2105-1056', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-05 00:13:19 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-05 00:13:19 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-05 00:13:19 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-05 00:13:24 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 111-Sodium phosphate, monobasic, model_path: ../Models/each_drug_note_v3_without_disease/111_checkpoint_Sodium phosphate, monobasic_0.1505-768 ------\n",
      "INFO 02-05 00:15:14 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/111_checkpoint_Sodium phosphate, monobasic_0.1505-768', tokenizer='../Models/each_drug_note_v3_without_disease/111_checkpoint_Sodium phosphate, monobasic_0.1505-768', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-05 00:15:44 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-05 00:15:44 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-05 00:15:44 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-05 00:15:49 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:48<00:00, 21.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 112-Isosorbide, model_path: ../Models/each_drug_note_v3_without_disease/112_checkpoint_Isosorbide_0.3376-224 ------\n",
      "INFO 02-05 00:17:40 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/112_checkpoint_Isosorbide_0.3376-224', tokenizer='../Models/each_drug_note_v3_without_disease/112_checkpoint_Isosorbide_0.3376-224', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-05 00:18:10 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-05 00:18:10 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-05 00:18:10 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-05 00:18:16 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 113-Diazepam, model_path: ../Models/each_drug_note_v3_without_disease/113_checkpoint_Diazepam_0.4279-448 ------\n",
      "INFO 02-05 00:20:05 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/113_checkpoint_Diazepam_0.4279-448', tokenizer='../Models/each_drug_note_v3_without_disease/113_checkpoint_Diazepam_0.4279-448', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-05 00:20:35 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-05 00:20:35 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-05 00:20:35 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-05 00:20:40 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 114-Quetiapine, model_path: ../Models/each_drug_note_v3_without_disease/114_checkpoint_Quetiapine_0.4542-800 ------\n",
      "INFO 02-05 00:22:31 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/114_checkpoint_Quetiapine_0.4542-800', tokenizer='../Models/each_drug_note_v3_without_disease/114_checkpoint_Quetiapine_0.4542-800', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-05 00:23:17 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-05 00:23:17 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-05 00:23:17 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-05 00:23:22 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 115-Atenolol, model_path: ../Models/each_drug_note_v3_without_disease/115_checkpoint_Atenolol_0.5545-256 ------\n",
      "INFO 02-05 00:25:12 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/115_checkpoint_Atenolol_0.5545-256', tokenizer='../Models/each_drug_note_v3_without_disease/115_checkpoint_Atenolol_0.5545-256', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-05 00:25:42 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-05 00:25:42 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-05 00:25:42 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-05 00:25:47 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 116-Allopurinol, model_path: ../Models/each_drug_note_v3_without_disease/116_checkpoint_Allopurinol_0.6667-864 ------\n",
      "INFO 02-05 00:27:36 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/116_checkpoint_Allopurinol_0.6667-864', tokenizer='../Models/each_drug_note_v3_without_disease/116_checkpoint_Allopurinol_0.6667-864', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-05 00:28:06 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-05 00:28:06 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-05 00:28:06 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-05 00:28:11 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:47<00:00, 22.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 117-Linezolid, model_path: ../Models/each_drug_note_v3_without_disease/117_checkpoint_Linezolid_0.2674-864 ------\n",
      "INFO 02-05 00:30:02 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/117_checkpoint_Linezolid_0.2674-864', tokenizer='../Models/each_drug_note_v3_without_disease/117_checkpoint_Linezolid_0.2674-864', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-05 00:31:30 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-05 00:31:30 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-05 00:31:30 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-05 00:31:36 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 118-Phenytoin, model_path: ../Models/each_drug_note_v3_without_disease/118_checkpoint_Phenytoin_0.5266-448 ------\n",
      "INFO 02-05 00:33:25 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/118_checkpoint_Phenytoin_0.5266-448', tokenizer='../Models/each_drug_note_v3_without_disease/118_checkpoint_Phenytoin_0.5266-448', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-05 00:33:55 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-05 00:33:55 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-05 00:33:55 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-05 00:34:00 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 119-Carvedilol, model_path: ../Models/each_drug_note_v3_without_disease/119_checkpoint_Carvedilol_0.6171-448 ------\n",
      "INFO 02-05 00:35:50 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/119_checkpoint_Carvedilol_0.6171-448', tokenizer='../Models/each_drug_note_v3_without_disease/119_checkpoint_Carvedilol_0.6171-448', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-05 00:36:20 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-05 00:36:20 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-05 00:36:20 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-05 00:36:25 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 120-Isosorbide mononitrate, model_path: ../Models/each_drug_note_v3_without_disease/120_checkpoint_Isosorbide mononitrate_0.5078-1184 ------\n",
      "INFO 02-05 00:38:14 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/120_checkpoint_Isosorbide mononitrate_0.5078-1184', tokenizer='../Models/each_drug_note_v3_without_disease/120_checkpoint_Isosorbide mononitrate_0.5078-1184', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-05 00:38:45 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-05 00:38:45 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-05 00:38:45 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-05 00:38:50 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:47<00:00, 22.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 121-Calcium acetate, model_path: ../Models/each_drug_note_v3_without_disease/121_checkpoint_Calcium acetate_0.3036-256 ------\n",
      "INFO 02-05 00:40:40 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/121_checkpoint_Calcium acetate_0.3036-256', tokenizer='../Models/each_drug_note_v3_without_disease/121_checkpoint_Calcium acetate_0.3036-256', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-05 00:41:11 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-05 00:41:11 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-05 00:41:11 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-05 00:41:16 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:47<00:00, 22.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 122-Nitroprusside, model_path: ../Models/each_drug_note_v3_without_disease/122_checkpoint_Nitroprusside_0.0625-256 ------\n",
      "INFO 02-05 00:43:06 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/122_checkpoint_Nitroprusside_0.0625-256', tokenizer='../Models/each_drug_note_v3_without_disease/122_checkpoint_Nitroprusside_0.0625-256', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-05 00:43:36 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-05 00:43:36 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-05 00:43:36 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-05 00:43:42 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:47<00:00, 22.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 123-Acetazolamide, model_path: ../Models/each_drug_note_v3_without_disease/123_checkpoint_Acetazolamide_0.1687-960 ------\n",
      "INFO 02-05 00:45:32 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/123_checkpoint_Acetazolamide_0.1687-960', tokenizer='../Models/each_drug_note_v3_without_disease/123_checkpoint_Acetazolamide_0.1687-960', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-05 00:46:02 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-05 00:46:02 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-05 00:46:02 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-05 00:46:06 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 124-Hydrochlorothiazide, model_path: ../Models/each_drug_note_v3_without_disease/124_checkpoint_Hydrochlorothiazide_0.4821-416 ------\n",
      "INFO 02-05 00:47:56 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/124_checkpoint_Hydrochlorothiazide_0.4821-416', tokenizer='../Models/each_drug_note_v3_without_disease/124_checkpoint_Hydrochlorothiazide_0.4821-416', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-05 00:48:26 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-05 00:48:26 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-05 00:48:26 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-05 00:48:31 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:47<00:00, 22.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 125-Clonazepam, model_path: ../Models/each_drug_note_v3_without_disease/125_checkpoint_Clonazepam_0.5692-448 ------\n",
      "INFO 02-05 00:50:21 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/125_checkpoint_Clonazepam_0.5692-448', tokenizer='../Models/each_drug_note_v3_without_disease/125_checkpoint_Clonazepam_0.5692-448', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-05 00:50:52 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-05 00:50:52 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-05 00:50:52 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-05 00:50:57 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 126-Aluminum hydroxide, model_path: ../Models/each_drug_note_v3_without_disease/126_checkpoint_Aluminum hydroxide_0.0588-1920 ------\n",
      "INFO 02-05 00:52:46 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/126_checkpoint_Aluminum hydroxide_0.0588-1920', tokenizer='../Models/each_drug_note_v3_without_disease/126_checkpoint_Aluminum hydroxide_0.0588-1920', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-05 00:53:17 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-05 00:53:17 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-05 00:53:17 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-05 00:53:22 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:47<00:00, 22.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 127-Sertraline, model_path: ../Models/each_drug_note_v3_without_disease/127_checkpoint_Sertraline_0.7356-608 ------\n",
      "INFO 02-05 00:55:12 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/127_checkpoint_Sertraline_0.7356-608', tokenizer='../Models/each_drug_note_v3_without_disease/127_checkpoint_Sertraline_0.7356-608', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-05 00:55:42 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-05 00:55:42 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-05 00:55:42 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-05 00:55:47 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 128-Promethazine, model_path: ../Models/each_drug_note_v3_without_disease/128_checkpoint_Promethazine_0.0448-1280 ------\n",
      "INFO 02-05 00:57:36 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/128_checkpoint_Promethazine_0.0448-1280', tokenizer='../Models/each_drug_note_v3_without_disease/128_checkpoint_Promethazine_0.0448-1280', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-05 00:58:07 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-05 00:58:07 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-05 00:58:07 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-05 00:58:12 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 129-Clonidine, model_path: ../Models/each_drug_note_v3_without_disease/129_checkpoint_Clonidine_0.4314-832 ------\n",
      "INFO 02-05 01:00:01 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/129_checkpoint_Clonidine_0.4314-832', tokenizer='../Models/each_drug_note_v3_without_disease/129_checkpoint_Clonidine_0.4314-832', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-05 01:00:32 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-05 01:00:32 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-05 01:00:32 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-05 01:00:37 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 130-Tamsulosin, model_path: ../Models/each_drug_note_v3_without_disease/130_checkpoint_Tamsulosin_0.5481-672 ------\n",
      "INFO 02-05 01:02:27 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/130_checkpoint_Tamsulosin_0.5481-672', tokenizer='../Models/each_drug_note_v3_without_disease/130_checkpoint_Tamsulosin_0.5481-672', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-05 01:02:57 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-05 01:02:57 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-05 01:02:57 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-05 01:03:03 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 131-Epinephrine, model_path: ../Models/each_drug_note_v3_without_disease/131_checkpoint_Epinephrine_0.2447-224 ------\n",
      "INFO 02-05 01:04:52 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/131_checkpoint_Epinephrine_0.2447-224', tokenizer='../Models/each_drug_note_v3_without_disease/131_checkpoint_Epinephrine_0.2447-224', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-05 01:05:23 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-05 01:05:23 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-05 01:05:23 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-05 01:05:28 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 132-Mycophenolate mofetil, model_path: ../Models/each_drug_note_v3_without_disease/132_checkpoint_Mycophenolate mofetil_0.8034-704 ------\n",
      "INFO 02-05 01:07:18 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/132_checkpoint_Mycophenolate mofetil_0.8034-704', tokenizer='../Models/each_drug_note_v3_without_disease/132_checkpoint_Mycophenolate mofetil_0.8034-704', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-05 01:07:48 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-05 01:07:48 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-05 01:07:48 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-05 01:07:53 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:47<00:00, 22.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 133-Sulbactam, model_path: ../Models/each_drug_note_v3_without_disease/133_checkpoint_Sulbactam_0.1920-576 ------\n",
      "INFO 02-05 01:09:44 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/133_checkpoint_Sulbactam_0.1920-576', tokenizer='../Models/each_drug_note_v3_without_disease/133_checkpoint_Sulbactam_0.1920-576', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-05 01:10:14 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-05 01:10:14 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-05 01:10:14 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-05 01:10:19 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 134-Sodium sulfate, model_path: ../Models/each_drug_note_v3_without_disease/134_checkpoint_Sodium sulfate_0.5948-480 ------\n",
      "INFO 02-05 01:12:09 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/134_checkpoint_Sodium sulfate_0.5948-480', tokenizer='../Models/each_drug_note_v3_without_disease/134_checkpoint_Sodium sulfate_0.5948-480', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-05 01:12:39 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-05 01:12:39 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-05 01:12:39 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-05 01:12:45 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 135-Nicotine, model_path: ../Models/each_drug_note_v3_without_disease/135_checkpoint_Nicotine_0.2978-768 ------\n",
      "INFO 02-05 01:14:35 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/135_checkpoint_Nicotine_0.2978-768', tokenizer='../Models/each_drug_note_v3_without_disease/135_checkpoint_Nicotine_0.2978-768', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-05 01:15:05 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-05 01:15:05 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-05 01:15:05 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-05 01:15:10 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 136-Ceftazidime, model_path: ../Models/each_drug_note_v3_without_disease/136_checkpoint_Ceftazidime_0.0625-960 ------\n",
      "INFO 02-05 01:16:59 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/136_checkpoint_Ceftazidime_0.0625-960', tokenizer='../Models/each_drug_note_v3_without_disease/136_checkpoint_Ceftazidime_0.0625-960', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-05 01:17:30 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-05 01:17:30 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-05 01:17:30 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-05 01:17:35 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 137-Acyclovir, model_path: ../Models/each_drug_note_v3_without_disease/137_checkpoint_Acyclovir_0.4551-832 ------\n",
      "INFO 02-05 01:19:25 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/137_checkpoint_Acyclovir_0.4551-832', tokenizer='../Models/each_drug_note_v3_without_disease/137_checkpoint_Acyclovir_0.4551-832', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-05 01:19:55 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-05 01:19:55 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-05 01:19:55 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-05 01:20:00 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 138-Loperamide, model_path: ../Models/each_drug_note_v3_without_disease/138_checkpoint_Loperamide_0.1835-800 ------\n",
      "INFO 02-05 01:21:50 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/138_checkpoint_Loperamide_0.1835-800', tokenizer='../Models/each_drug_note_v3_without_disease/138_checkpoint_Loperamide_0.1835-800', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-05 01:22:20 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-05 01:22:20 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-05 01:22:20 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-05 01:22:25 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 139-Octreotide, model_path: ../Models/each_drug_note_v3_without_disease/139_checkpoint_Octreotide_0.5923-800 ------\n",
      "INFO 02-05 01:24:15 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/139_checkpoint_Octreotide_0.5923-800', tokenizer='../Models/each_drug_note_v3_without_disease/139_checkpoint_Octreotide_0.5923-800', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-05 01:24:44 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-05 01:24:44 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-05 01:24:44 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-05 01:24:49 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 140-Tiotropium, model_path: ../Models/each_drug_note_v3_without_disease/140_checkpoint_Tiotropium_0.5912-416 ------\n",
      "INFO 02-05 01:26:38 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/140_checkpoint_Tiotropium_0.5912-416', tokenizer='../Models/each_drug_note_v3_without_disease/140_checkpoint_Tiotropium_0.5912-416', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-05 01:27:08 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-05 01:27:08 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-05 01:27:08 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-05 01:27:13 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 141-Mirtazapine, model_path: ../Models/each_drug_note_v3_without_disease/141_checkpoint_Mirtazapine_0.5620-544 ------\n",
      "INFO 02-05 01:29:02 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/141_checkpoint_Mirtazapine_0.5620-544', tokenizer='../Models/each_drug_note_v3_without_disease/141_checkpoint_Mirtazapine_0.5620-544', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-05 01:29:31 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-05 01:29:32 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-05 01:29:32 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-05 01:29:36 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 142-Tacrolimus, model_path: ../Models/each_drug_note_v3_without_disease/142_checkpoint_Tacrolimus_0.8803-416 ------\n",
      "INFO 02-05 01:31:26 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/142_checkpoint_Tacrolimus_0.8803-416', tokenizer='../Models/each_drug_note_v3_without_disease/142_checkpoint_Tacrolimus_0.8803-416', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-05 01:31:57 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-05 01:31:57 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-05 01:31:57 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-05 01:32:02 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 143-Clindamycin, model_path: ../Models/each_drug_note_v3_without_disease/143_checkpoint_Clindamycin_0.1207-1312 ------\n",
      "INFO 02-05 01:33:52 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/143_checkpoint_Clindamycin_0.1207-1312', tokenizer='../Models/each_drug_note_v3_without_disease/143_checkpoint_Clindamycin_0.1207-1312', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-05 01:34:22 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-05 01:34:22 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-05 01:34:22 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-05 01:34:26 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 144-Erythromycin, model_path: ../Models/each_drug_note_v3_without_disease/144_checkpoint_Erythromycin_0.1000-1824 ------\n",
      "INFO 02-05 01:36:16 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/144_checkpoint_Erythromycin_0.1000-1824', tokenizer='../Models/each_drug_note_v3_without_disease/144_checkpoint_Erythromycin_0.1000-1824', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-05 01:36:46 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-05 01:36:46 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-05 01:36:46 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-05 01:36:50 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 145-Metolazone, model_path: ../Models/each_drug_note_v3_without_disease/145_checkpoint_Metolazone_0.2336-1120 ------\n",
      "INFO 02-05 01:38:40 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/145_checkpoint_Metolazone_0.2336-1120', tokenizer='../Models/each_drug_note_v3_without_disease/145_checkpoint_Metolazone_0.2336-1120', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-05 01:39:11 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-05 01:39:11 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-05 01:39:11 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-05 01:39:16 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 146-Hydrocodone, model_path: ../Models/each_drug_note_v3_without_disease/146_checkpoint_Hydrocodone_0.2202-1216 ------\n",
      "INFO 02-05 01:41:05 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/146_checkpoint_Hydrocodone_0.2202-1216', tokenizer='../Models/each_drug_note_v3_without_disease/146_checkpoint_Hydrocodone_0.2202-1216', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-05 01:41:35 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-05 01:41:35 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-05 01:41:35 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-05 01:41:40 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 147-Valsartan, model_path: ../Models/each_drug_note_v3_without_disease/147_checkpoint_Valsartan_0.4362-416 ------\n",
      "INFO 02-05 01:43:30 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/147_checkpoint_Valsartan_0.4362-416', tokenizer='../Models/each_drug_note_v3_without_disease/147_checkpoint_Valsartan_0.4362-416', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-05 01:44:00 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-05 01:44:00 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-05 01:44:00 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-05 01:44:05 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [01:46<00:00, 22.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 148-Codeine, model_path: ../Models/each_drug_note_v3_without_disease/148_checkpoint_Codeine_0.0407-1760 ------\n",
      "INFO 02-05 01:45:54 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/each_drug_note_v3_without_disease/148_checkpoint_Codeine_0.0407-1760', tokenizer='../Models/each_drug_note_v3_without_disease/148_checkpoint_Codeine_0.0407-1760', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-05 01:46:24 llm_engine.py:275] # GPU blocks: 7304, # CPU blocks: 512\n",
      "INFO 02-05 01:46:24 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-05 01:46:24 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 02-05 01:46:29 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  64%|██████▍   | 1527/2368 [01:18<00:38, 21.73it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '3'\n",
    "\n",
    "from _3_each_lora_evaluation_vllm import each_lora_evaluation_vllm\n",
    "\n",
    "gpu_memory_utilization = 0.9\n",
    "base_model = \"llama-2-7b\"\n",
    "run_name = \"each_drug_note_v3_without_disease\"\n",
    "\n",
    "each_lora_evaluation_vllm(run_name, gpu_memory_utilization, vllm=True, mode='test', \n",
    "                          use_history=True, \n",
    "                          use_note=True,\n",
    "                          use_disease=False,\n",
    "                          use_procedure=True,\n",
    "                          file_name='data4LLM_CONCISE_NOTE.csv',\n",
    "                          base_model=base_model,\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4_each_lora_evluation_vllm\n",
    "每个lora负责一部分药物，分组进行评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../data/')\n",
    "import utils\n",
    "from generate_finetune_data import generate_finetune_data\n",
    "\n",
    "lora_num = 10\n",
    "\n",
    "_, med_names = utils.load_data(mode='val', file_name='data4LLM_CONCISE_NOTE.csv')\n",
    "\n",
    "split_point_list = [0]\n",
    "split_list = []\n",
    "\n",
    "\n",
    "\n",
    "for i in range(lora_num):\n",
    "    split_point = int(len(med_names) / lora_num * (i + 1))\n",
    "    split_point_list.append(split_point)\n",
    "\n",
    "for i in range(lora_num):\n",
    "    split_list.append(med_names[split_point_list[i]:split_point_list[i + 1]])\n",
    "\n",
    "for i in range(len(split_list)):\n",
    "    # 将split_list[i]的内容转换成str，并写入med_list.txt文件\n",
    "    with open(f'../output/10lora_2/checkpoints/checkpoint_{i+1}_of_{lora_num}/med_list.txt', 'w') as f:\n",
    "        f.write(str(split_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data size: 2368\n",
      "\n",
      "{'Nicotine': 'checkpoint_10_of_10', 'Ceftazidime': 'checkpoint_10_of_10', 'Acyclovir': 'checkpoint_10_of_10', 'Loperamide': 'checkpoint_10_of_10', 'Octreotide': 'checkpoint_10_of_10', 'Tiotropium': 'checkpoint_10_of_10', 'Mirtazapine': 'checkpoint_10_of_10', 'Tacrolimus': 'checkpoint_10_of_10', 'Clindamycin': 'checkpoint_10_of_10', 'Erythromycin': 'checkpoint_10_of_10', 'Metolazone': 'checkpoint_10_of_10', 'Hydrocodone': 'checkpoint_10_of_10', 'Valsartan': 'checkpoint_10_of_10', 'Codeine': 'checkpoint_10_of_10', 'Fluoxetine': 'checkpoint_10_of_10', 'Nifedipine': 'checkpoint_10_of_10', 'Fluticasone propionate': 'checkpoint_4_of_10', 'Levothyroxine': 'checkpoint_4_of_10', 'Norepinephrine': 'checkpoint_4_of_10', 'Simvastatin': 'checkpoint_4_of_10', 'Ranitidine': 'checkpoint_4_of_10', 'Monopotassium phosphate': 'checkpoint_4_of_10', 'Thiamine': 'checkpoint_4_of_10', 'Piperacillin': 'checkpoint_4_of_10', 'Amiodarone': 'checkpoint_4_of_10', 'Haloperidol': 'checkpoint_4_of_10', 'Ceftriaxone': 'checkpoint_4_of_10', 'Trazodone': 'checkpoint_4_of_10', 'Phylloquinone': 'checkpoint_4_of_10', 'Cefazolin': 'checkpoint_4_of_10', 'Miconazole': 'checkpoint_4_of_10', 'Digoxin': 'checkpoint_7_of_10', 'Ampicillin': 'checkpoint_7_of_10', 'Citalopram': 'checkpoint_7_of_10', 'Pyridoxine': 'checkpoint_7_of_10', 'Ascorbic acid': 'checkpoint_7_of_10', 'Niacin': 'checkpoint_7_of_10', 'Dolasetron': 'checkpoint_7_of_10', 'Tetracosactide': 'checkpoint_7_of_10', 'Atropine': 'checkpoint_7_of_10', 'Sevelamer': 'checkpoint_7_of_10', 'Riboflavin': 'checkpoint_7_of_10', 'Biotin': 'checkpoint_7_of_10', 'Pantothenic acid': 'checkpoint_7_of_10', 'Sodium chloride': 'checkpoint_7_of_10', 'Hydrocortisone succinate': 'checkpoint_7_of_10', 'Isosorbide mononitrate': 'checkpoint_9_of_10', 'Calcium acetate': 'checkpoint_9_of_10', 'Nitroprusside': 'checkpoint_9_of_10', 'Acetazolamide': 'checkpoint_9_of_10', 'Hydrochlorothiazide': 'checkpoint_9_of_10', 'Clonazepam': 'checkpoint_9_of_10', 'Aluminum hydroxide': 'checkpoint_9_of_10', 'Sertraline': 'checkpoint_9_of_10', 'Promethazine': 'checkpoint_9_of_10', 'Clonidine': 'checkpoint_9_of_10', 'Tamsulosin': 'checkpoint_9_of_10', 'Epinephrine': 'checkpoint_9_of_10', 'Mycophenolate mofetil': 'checkpoint_9_of_10', 'Sulbactam': 'checkpoint_9_of_10', 'Sodium sulfate': 'checkpoint_9_of_10', 'Acetaminophen': 'checkpoint_1_of_10', 'Potassium chloride': 'checkpoint_1_of_10', 'Pantoprazole': 'checkpoint_1_of_10', 'Metoprolol': 'checkpoint_1_of_10', 'Magnesium sulfate': 'checkpoint_1_of_10', 'Furosemide': 'checkpoint_1_of_10', 'Vancomycin': 'checkpoint_1_of_10', 'Salbutamol': 'checkpoint_1_of_10', 'Oxycodone': 'checkpoint_1_of_10', 'Bisacodyl': 'checkpoint_1_of_10', 'Lorazepam': 'checkpoint_1_of_10', 'Magnesium': 'checkpoint_1_of_10', 'Ipratropium': 'checkpoint_1_of_10', 'Acetylsalicylic acid': 'checkpoint_1_of_10', 'Morphine': 'checkpoint_1_of_10', 'Glycopyrronium': 'checkpoint_6_of_10', 'Meropenem': 'checkpoint_6_of_10', 'Ergocalciferol': 'checkpoint_6_of_10', 'Sucralfate': 'checkpoint_6_of_10', 'Hydrocortisone': 'checkpoint_6_of_10', 'Guaifenesin': 'checkpoint_6_of_10', 'Meperidine': 'checkpoint_6_of_10', 'Methylprednisolone': 'checkpoint_6_of_10', 'Captopril': 'checkpoint_6_of_10', 'Salmeterol': 'checkpoint_6_of_10', 'Neostigmine': 'checkpoint_6_of_10', 'Olanzapine': 'checkpoint_6_of_10', 'Magnesium hydroxide': 'checkpoint_6_of_10', 'Labetalol': 'checkpoint_6_of_10', 'Ibuprofen': 'checkpoint_6_of_10', 'Chlorhexidine': 'checkpoint_3_of_10', 'Hydralazine': 'checkpoint_3_of_10', 'Phenylephrine': 'checkpoint_3_of_10', 'Metoclopramide': 'checkpoint_3_of_10', 'Ciprofloxacin': 'checkpoint_3_of_10', 'Spironolactone': 'checkpoint_3_of_10', 'Nitroglycerin': 'checkpoint_3_of_10', 'Prednisone': 'checkpoint_3_of_10', 'Sodium bicarbonate': 'checkpoint_3_of_10', 'Tazobactam': 'checkpoint_3_of_10', 'Diphenhydramine': 'checkpoint_3_of_10', 'Lactulose': 'checkpoint_3_of_10', 'Omeprazole': 'checkpoint_3_of_10', 'Clopidogrel': 'checkpoint_3_of_10', 'Zolpidem': 'checkpoint_3_of_10', 'Acetylcysteine': 'checkpoint_5_of_10', 'Cefepime': 'checkpoint_5_of_10', 'Gabapentin': 'checkpoint_5_of_10', 'Enalaprilat': 'checkpoint_5_of_10', 'Amlodipine': 'checkpoint_5_of_10', 'Folic acid': 'checkpoint_5_of_10', 'Nystatin': 'checkpoint_5_of_10', 'Cyanocobalamin': 'checkpoint_5_of_10', 'Prochlorperazine': 'checkpoint_5_of_10', 'Diltiazem': 'checkpoint_5_of_10', 'Sulfamethoxazole': 'checkpoint_5_of_10', 'Methylprednisolone hemisuccinate': 'checkpoint_5_of_10', 'Trimethoprim': 'checkpoint_5_of_10', 'Dopamine': 'checkpoint_5_of_10', 'Fluconazole': 'checkpoint_5_of_10', 'Dexamethasone': 'checkpoint_8_of_10', 'Tramadol': 'checkpoint_8_of_10', 'Ketorolac': 'checkpoint_8_of_10', 'Levetiracetam': 'checkpoint_8_of_10', 'Gentamicin': 'checkpoint_8_of_10', 'Azithromycin': 'checkpoint_8_of_10', 'Sodium phosphate, monobasic': 'checkpoint_8_of_10', 'Isosorbide': 'checkpoint_8_of_10', 'Diazepam': 'checkpoint_8_of_10', 'Quetiapine': 'checkpoint_8_of_10', 'Atenolol': 'checkpoint_8_of_10', 'Allopurinol': 'checkpoint_8_of_10', 'Linezolid': 'checkpoint_8_of_10', 'Phenytoin': 'checkpoint_8_of_10', 'Carvedilol': 'checkpoint_8_of_10', 'Fentanyl': 'checkpoint_2_of_10', 'Ondansetron': 'checkpoint_2_of_10', 'Calcium gluconate': 'checkpoint_2_of_10', 'Propofol': 'checkpoint_2_of_10', 'Lansoprazole': 'checkpoint_2_of_10', 'D-glucose': 'checkpoint_2_of_10', 'Levofloxacin': 'checkpoint_2_of_10', 'Midazolam': 'checkpoint_2_of_10', 'Metronidazole': 'checkpoint_2_of_10', 'Hydromorphone': 'checkpoint_2_of_10', 'Atorvastatin': 'checkpoint_2_of_10', 'Lisinopril': 'checkpoint_2_of_10', 'Warfarin': 'checkpoint_2_of_10', 'Lidocaine': 'checkpoint_2_of_10', 'Famotidine': 'checkpoint_2_of_10'}\n",
      "------ Merging lora weights from ../output/10lora_2/checkpoints/checkpoint_1_of_10 and saving hf model ------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8174e28284e348459de5cfe92184b87c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sds/cxfan/anaconda3/envs/qwen/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Merging lora weights from ../output/10lora_2/checkpoints/checkpoint_2_of_10 and saving hf model ------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3175253853ab4e59a45ccfdde7746089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Merging lora weights from ../output/10lora_2/checkpoints/checkpoint_3_of_10 and saving hf model ------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e70e678620854508b966f283de16ad54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Merging lora weights from ../output/10lora_2/checkpoints/checkpoint_4_of_10 and saving hf model ------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecd186e039ac46efaf83a5f1e8b4b8e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Merging lora weights from ../output/10lora_2/checkpoints/checkpoint_5_of_10 and saving hf model ------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca0cec4df10a47168ec0599d78b1f3eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Merging lora weights from ../output/10lora_2/checkpoints/checkpoint_6_of_10 and saving hf model ------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a985d561eb243ce973b396d0edfd483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Merging lora weights from ../output/10lora_2/checkpoints/checkpoint_7_of_10 and saving hf model ------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f89efdc3ec14ad4933ecad848681fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Merging lora weights from ../output/10lora_2/checkpoints/checkpoint_8_of_10 and saving hf model ------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d198ed3ea979443c8b3748aedf39dcd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Merging lora weights from ../output/10lora_2/checkpoints/checkpoint_9_of_10 and saving hf model ------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5996151dea604ceaa6180441fdfdf76d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Merging lora weights from ../output/10lora_2/checkpoints/checkpoint_10_of_10 and saving hf model ------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ba42e9528bf4c2983ab1a5386d1d608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 0-Acetaminophen, model_path: ../Models/10lora_2/checkpoint_1_of_10 ------\n",
      "INFO 03-03 22:52:47 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_1_of_10', tokenizer='../Models/10lora_2/checkpoint_1_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-03 22:53:30 llm_engine.py:275] # GPU blocks: 7296, # CPU blocks: 512\n",
      "INFO 03-03 22:53:31 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-03 22:53:31 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-03 22:53:36 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.31it/s]\n",
      "2024-03-03 22:56:50,386 - INFO - jaccard: 0.0392   f1: 0.0738   recall: 0.0392   precision: 0.8045   ddi: 0.0   drug_num: 0.9818   pred_num: 2325   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 1-Potassium chloride, model_path: ../Models/10lora_2/checkpoint_1_of_10 ------\n",
      "INFO 03-03 22:56:50 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_1_of_10', tokenizer='../Models/10lora_2/checkpoint_1_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-03 22:57:16 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-03 22:57:16 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-03 22:57:16 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-03 22:57:20 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.28it/s]\n",
      "2024-03-03 23:00:38,856 - INFO - jaccard: 0.069   f1: 0.1264   recall: 0.0701   precision: 0.7969   ddi: 0.0   drug_num: 1.8454   pred_num: 2045   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 2-Pantoprazole, model_path: ../Models/10lora_2/checkpoint_1_of_10 ------\n",
      "INFO 03-03 23:00:38 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_1_of_10', tokenizer='../Models/10lora_2/checkpoint_1_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-03 23:01:05 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-03 23:01:05 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-03 23:01:05 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-03 23:01:09 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.30it/s]\n",
      "2024-03-03 23:04:30,515 - INFO - jaccard: 0.0911   f1: 0.1631   recall: 0.0947   precision: 0.7409   ddi: 0.0   drug_num: 2.6854   pred_num: 1989   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 3-Metoprolol, model_path: ../Models/10lora_2/checkpoint_1_of_10 ------\n",
      "INFO 03-03 23:04:30 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_1_of_10', tokenizer='../Models/10lora_2/checkpoint_1_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-03 23:04:56 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-03 23:04:56 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-03 23:04:56 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-03 23:05:00 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.31it/s]\n",
      "2024-03-03 23:08:24,044 - INFO - jaccard: 0.1122   f1: 0.1971   recall: 0.1177   precision: 0.7433   ddi: 0.0   drug_num: 3.3438   pred_num: 1559   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 4-Magnesium sulfate, model_path: ../Models/10lora_2/checkpoint_1_of_10 ------\n",
      "INFO 03-03 23:08:24 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_1_of_10', tokenizer='../Models/10lora_2/checkpoint_1_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-03 23:08:51 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-03 23:08:51 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-03 23:08:51 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-03 23:08:56 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.26it/s]\n",
      "2024-03-03 23:12:22,276 - INFO - jaccard: 0.1362   f1: 0.2342   recall: 0.1445   precision: 0.7382   ddi: 0.0644   drug_num: 4.1816   pred_num: 1984   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 5-Furosemide, model_path: ../Models/10lora_2/checkpoint_1_of_10 ------\n",
      "INFO 03-03 23:12:22 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_1_of_10', tokenizer='../Models/10lora_2/checkpoint_1_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-03 23:12:48 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-03 23:12:48 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-03 23:12:48 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-03 23:12:52 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.25it/s]\n",
      "2024-03-03 23:16:21,258 - INFO - jaccard: 0.1539   f1: 0.2609   recall: 0.1645   precision: 0.7339   ddi: 0.0941   drug_num: 4.856   pred_num: 1597   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 6-Vancomycin, model_path: ../Models/10lora_2/checkpoint_1_of_10 ------\n",
      "INFO 03-03 23:16:21 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_1_of_10', tokenizer='../Models/10lora_2/checkpoint_1_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-03 23:16:49 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-03 23:16:49 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-03 23:16:49 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-03 23:16:53 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.27it/s]\n",
      "2024-03-03 23:20:24,196 - INFO - jaccard: 0.1701   f1: 0.2849   recall: 0.1827   precision: 0.7387   ddi: 0.0755   drug_num: 5.4079   pred_num: 1307   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 7-Salbutamol, model_path: ../Models/10lora_2/checkpoint_1_of_10 ------\n",
      "INFO 03-03 23:20:24 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_1_of_10', tokenizer='../Models/10lora_2/checkpoint_1_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-03 23:20:53 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-03 23:20:53 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-03 23:20:53 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-03 23:20:57 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.24it/s]\n",
      "2024-03-03 23:24:30,318 - INFO - jaccard: 0.1839   f1: 0.3048   recall: 0.1986   precision: 0.739   ddi: 0.082   drug_num: 5.9274   pred_num: 1230   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 8-Oxycodone, model_path: ../Models/10lora_2/checkpoint_1_of_10 ------\n",
      "INFO 03-03 23:24:30 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_1_of_10', tokenizer='../Models/10lora_2/checkpoint_1_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-03 23:24:59 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-03 23:24:59 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-03 23:24:59 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-03 23:25:03 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.29it/s]\n",
      "2024-03-03 23:28:37,641 - INFO - jaccard: 0.1961   f1: 0.3217   recall: 0.2131   precision: 0.7349   ddi: 0.084   drug_num: 6.397   pred_num: 1112   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 9-Bisacodyl, model_path: ../Models/10lora_2/checkpoint_1_of_10 ------\n",
      "INFO 03-03 23:28:37 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_1_of_10', tokenizer='../Models/10lora_2/checkpoint_1_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-03 23:29:06 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-03 23:29:06 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-03 23:29:06 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-03 23:29:10 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.25it/s]\n",
      "2024-03-03 23:32:47,625 - INFO - jaccard: 0.2059   f1: 0.3351   recall: 0.2258   precision: 0.7244   ddi: 0.1303   drug_num: 6.9151   pred_num: 1227   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 10-Lorazepam, model_path: ../Models/10lora_2/checkpoint_1_of_10 ------\n",
      "INFO 03-03 23:32:47 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_1_of_10', tokenizer='../Models/10lora_2/checkpoint_1_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-03 23:33:15 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-03 23:33:15 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-03 23:33:15 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-03 23:33:19 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.30it/s]\n",
      "2024-03-03 23:36:56,734 - INFO - jaccard: 0.2144   f1: 0.3467   recall: 0.2366   precision: 0.7192   ddi: 0.1238   drug_num: 7.3243   pred_num: 969   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 11-Magnesium, model_path: ../Models/10lora_2/checkpoint_1_of_10 ------\n",
      "INFO 03-03 23:36:56 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_1_of_10', tokenizer='../Models/10lora_2/checkpoint_1_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-03 23:37:22 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-03 23:37:22 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-03 23:37:22 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-03 23:37:26 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.28it/s]\n",
      "2024-03-03 23:41:06,852 - INFO - jaccard: 0.2305   f1: 0.3669   recall: 0.2555   precision: 0.7195   ddi: 0.1225   drug_num: 7.9189   pred_num: 1408   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 12-Ipratropium, model_path: ../Models/10lora_2/checkpoint_1_of_10 ------\n",
      "INFO 03-03 23:41:06 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_1_of_10', tokenizer='../Models/10lora_2/checkpoint_1_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-03 23:41:34 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-03 23:41:34 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-03 23:41:34 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-03 23:41:38 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.24it/s]\n",
      "2024-03-03 23:45:20,929 - INFO - jaccard: 0.2392   f1: 0.3783   recall: 0.2665   precision: 0.7188   ddi: 0.1259   drug_num: 8.2977   pred_num: 897   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 13-Acetylsalicylic acid, model_path: ../Models/10lora_2/checkpoint_1_of_10 ------\n",
      "INFO 03-03 23:45:20 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_1_of_10', tokenizer='../Models/10lora_2/checkpoint_1_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-03 23:45:46 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-03 23:45:46 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-03 23:45:46 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-03 23:45:50 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.24it/s]\n",
      "2024-03-03 23:49:33,715 - INFO - jaccard: 0.2473   f1: 0.3885   recall: 0.2769   precision: 0.7159   ddi: 0.1264   drug_num: 8.6554   pred_num: 847   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 14-Morphine, model_path: ../Models/10lora_2/checkpoint_1_of_10 ------\n",
      "INFO 03-03 23:49:33 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_1_of_10', tokenizer='../Models/10lora_2/checkpoint_1_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-03 23:50:04 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-03 23:50:04 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-03 23:50:04 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-03 23:50:08 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:24<00:00, 16.33it/s]\n",
      "2024-03-03 23:53:51,699 - INFO - jaccard: 0.2546   f1: 0.3974   recall: 0.287   precision: 0.7098   ddi: 0.15   drug_num: 9.0938   pred_num: 1038   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 15-Fentanyl, model_path: ../Models/10lora_2/checkpoint_2_of_10 ------\n",
      "INFO 03-03 23:53:51 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_2_of_10', tokenizer='../Models/10lora_2/checkpoint_2_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-03 23:54:31 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-03 23:54:31 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-03 23:54:31 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-03 23:54:36 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.33it/s]\n",
      "2024-03-03 23:58:20,200 - INFO - jaccard: 0.2604   f1: 0.4048   recall: 0.2946   precision: 0.7077   ddi: 0.1625   drug_num: 9.3957   pred_num: 715   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 16-Ondansetron, model_path: ../Models/10lora_2/checkpoint_2_of_10 ------\n",
      "INFO 03-03 23:58:20 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_2_of_10', tokenizer='../Models/10lora_2/checkpoint_2_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-03 23:58:51 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-03 23:58:51 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-03 23:58:51 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-03 23:58:55 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.31it/s]\n",
      "2024-03-04 00:02:42,065 - INFO - jaccard: 0.2664   f1: 0.4121   recall: 0.3022   precision: 0.7083   ddi: 0.1656   drug_num: 9.6174   pred_num: 525   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 17-Calcium gluconate, model_path: ../Models/10lora_2/checkpoint_2_of_10 ------\n",
      "INFO 03-04 00:02:42 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_2_of_10', tokenizer='../Models/10lora_2/checkpoint_2_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 00:03:13 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 00:03:13 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 00:03:13 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 00:03:18 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.26it/s]\n",
      "2024-03-04 00:07:04,569 - INFO - jaccard: 0.2671   f1: 0.413   recall: 0.3052   precision: 0.6973   ddi: 0.1597   drug_num: 9.913   pred_num: 700   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 18-Propofol, model_path: ../Models/10lora_2/checkpoint_2_of_10 ------\n",
      "INFO 03-04 00:07:04 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_2_of_10', tokenizer='../Models/10lora_2/checkpoint_2_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 00:07:30 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 00:07:31 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 00:07:31 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 00:07:35 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:24<00:00, 16.35it/s]\n",
      "2024-03-04 00:11:22,795 - INFO - jaccard: 0.2742   f1: 0.4216   recall: 0.3143   precision: 0.698   ddi: 0.1621   drug_num: 10.2542   pred_num: 808   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 19-Lansoprazole, model_path: ../Models/10lora_2/checkpoint_2_of_10 ------\n",
      "INFO 03-04 00:11:22 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_2_of_10', tokenizer='../Models/10lora_2/checkpoint_2_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 00:11:48 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 00:11:49 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 00:11:49 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 00:11:53 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.31it/s]\n",
      "2024-03-04 00:15:42,398 - INFO - jaccard: 0.2765   f1: 0.4243   recall: 0.318   precision: 0.6948   ddi: 0.1584   drug_num: 10.4455   pred_num: 453   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 20-D-glucose, model_path: ../Models/10lora_2/checkpoint_2_of_10 ------\n",
      "INFO 03-04 00:15:42 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_2_of_10', tokenizer='../Models/10lora_2/checkpoint_2_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 00:16:08 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 00:16:08 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 00:16:08 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 00:16:12 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.32it/s]\n",
      "2024-03-04 00:20:01,346 - INFO - jaccard: 0.2821   f1: 0.4309   recall: 0.3245   precision: 0.6978   ddi: 0.1532   drug_num: 10.6284   pred_num: 433   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 21-Levofloxacin, model_path: ../Models/10lora_2/checkpoint_2_of_10 ------\n",
      "INFO 03-04 00:20:01 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_2_of_10', tokenizer='../Models/10lora_2/checkpoint_2_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 00:20:26 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 00:20:26 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 00:20:26 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 00:20:30 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.29it/s]\n",
      "2024-03-04 00:24:21,109 - INFO - jaccard: 0.2837   f1: 0.4329   recall: 0.3282   precision: 0.6926   ddi: 0.1517   drug_num: 10.8429   pred_num: 508   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 22-Midazolam, model_path: ../Models/10lora_2/checkpoint_2_of_10 ------\n",
      "INFO 03-04 00:24:21 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_2_of_10', tokenizer='../Models/10lora_2/checkpoint_2_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 00:24:46 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 00:24:46 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 00:24:46 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 00:24:50 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.32it/s]\n",
      "2024-03-04 00:28:42,145 - INFO - jaccard: 0.2883   f1: 0.4382   recall: 0.3345   precision: 0.6915   ddi: 0.1497   drug_num: 11.1043   pred_num: 619   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 23-Metronidazole, model_path: ../Models/10lora_2/checkpoint_2_of_10 ------\n",
      "INFO 03-04 00:28:42 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_2_of_10', tokenizer='../Models/10lora_2/checkpoint_2_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 00:29:07 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 00:29:07 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 00:29:07 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 00:29:11 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.27it/s]\n",
      "2024-03-04 00:33:03,896 - INFO - jaccard: 0.292   f1: 0.4426   recall: 0.3399   precision: 0.6901   ddi: 0.1518   drug_num: 11.3374   pred_num: 552   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 24-Hydromorphone, model_path: ../Models/10lora_2/checkpoint_2_of_10 ------\n",
      "INFO 03-04 00:33:03 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_2_of_10', tokenizer='../Models/10lora_2/checkpoint_2_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 00:33:28 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 00:33:28 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 00:33:28 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 00:33:32 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.30it/s]\n",
      "2024-03-04 00:37:25,128 - INFO - jaccard: 0.2974   f1: 0.4487   recall: 0.3472   precision: 0.6903   ddi: 0.1587   drug_num: 11.5714   pred_num: 554   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 25-Atorvastatin, model_path: ../Models/10lora_2/checkpoint_2_of_10 ------\n",
      "INFO 03-04 00:37:25 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_2_of_10', tokenizer='../Models/10lora_2/checkpoint_2_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 00:37:52 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 00:37:52 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 00:37:52 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 00:37:56 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.31it/s]\n",
      "2024-03-04 00:41:50,056 - INFO - jaccard: 0.3027   f1: 0.4548   recall: 0.354   precision: 0.6915   ddi: 0.1539   drug_num: 11.7673   pred_num: 464   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 26-Lisinopril, model_path: ../Models/10lora_2/checkpoint_2_of_10 ------\n",
      "INFO 03-04 00:41:50 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_2_of_10', tokenizer='../Models/10lora_2/checkpoint_2_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 00:42:14 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 00:42:14 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 00:42:14 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 00:42:19 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.32it/s]\n",
      "2024-03-04 00:46:13,310 - INFO - jaccard: 0.3085   f1: 0.4615   recall: 0.3621   precision: 0.6917   ddi: 0.1585   drug_num: 12.0038   pred_num: 560   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 27-Warfarin, model_path: ../Models/10lora_2/checkpoint_2_of_10 ------\n",
      "INFO 03-04 00:46:13 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_2_of_10', tokenizer='../Models/10lora_2/checkpoint_2_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 00:46:41 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 00:46:41 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 00:46:41 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 00:46:46 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:24<00:00, 16.35it/s]\n",
      "2024-03-04 00:50:40,623 - INFO - jaccard: 0.3148   f1: 0.4687   recall: 0.3702   precision: 0.6927   ddi: 0.157   drug_num: 12.2449   pred_num: 571   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 28-Lidocaine, model_path: ../Models/10lora_2/checkpoint_2_of_10 ------\n",
      "INFO 03-04 00:50:40 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_2_of_10', tokenizer='../Models/10lora_2/checkpoint_2_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 00:51:09 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 00:51:09 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 00:51:09 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 00:51:13 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.30it/s]\n",
      "2024-03-04 00:55:09,663 - INFO - jaccard: 0.3161   f1: 0.4702   recall: 0.372   precision: 0.6926   ddi: 0.1572   drug_num: 12.3112   pred_num: 157   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 29-Famotidine, model_path: ../Models/10lora_2/checkpoint_2_of_10 ------\n",
      "INFO 03-04 00:55:09 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_2_of_10', tokenizer='../Models/10lora_2/checkpoint_2_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 00:55:38 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 00:55:39 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 00:55:39 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 00:55:43 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:24<00:00, 16.34it/s]\n",
      "2024-03-04 00:59:37,969 - INFO - jaccard: 0.3176   f1: 0.472   recall: 0.3741   precision: 0.6931   ddi: 0.1576   drug_num: 12.3708   pred_num: 141   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 30-Chlorhexidine, model_path: ../Models/10lora_2/checkpoint_3_of_10 ------\n",
      "INFO 03-04 00:59:37 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_3_of_10', tokenizer='../Models/10lora_2/checkpoint_3_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 01:00:14 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 01:00:14 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 01:00:14 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 01:00:19 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.28it/s]\n",
      "2024-03-04 01:04:15,407 - INFO - jaccard: 0.3235   f1: 0.4784   recall: 0.3818   precision: 0.6945   ddi: 0.1523   drug_num: 12.628   pred_num: 609   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 31-Hydralazine, model_path: ../Models/10lora_2/checkpoint_3_of_10 ------\n",
      "INFO 03-04 01:04:15 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_3_of_10', tokenizer='../Models/10lora_2/checkpoint_3_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 01:04:42 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 01:04:42 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 01:04:42 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 01:04:46 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:24<00:00, 16.33it/s]\n",
      "2024-03-04 01:08:43,189 - INFO - jaccard: 0.3256   f1: 0.4807   recall: 0.3849   precision: 0.6943   ddi: 0.1502   drug_num: 12.7327   pred_num: 248   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 32-Phenylephrine, model_path: ../Models/10lora_2/checkpoint_3_of_10 ------\n",
      "INFO 03-04 01:08:43 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_3_of_10', tokenizer='../Models/10lora_2/checkpoint_3_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 01:09:12 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 01:09:12 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 01:09:12 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 01:09:16 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.28it/s]\n",
      "2024-03-04 01:13:13,951 - INFO - jaccard: 0.3283   f1: 0.4835   recall: 0.3885   precision: 0.694   ddi: 0.1499   drug_num: 12.897   pred_num: 389   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 33-Metoclopramide, model_path: ../Models/10lora_2/checkpoint_3_of_10 ------\n",
      "INFO 03-04 01:13:13 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_3_of_10', tokenizer='../Models/10lora_2/checkpoint_3_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 01:13:45 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 01:13:45 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 01:13:45 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 01:13:50 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.25it/s]\n",
      "2024-03-04 01:17:48,508 - INFO - jaccard: 0.3309   f1: 0.4861   recall: 0.3917   precision: 0.6943   ddi: 0.1504   drug_num: 13.0059   pred_num: 258   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 34-Ciprofloxacin, model_path: ../Models/10lora_2/checkpoint_3_of_10 ------\n",
      "INFO 03-04 01:17:48 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_3_of_10', tokenizer='../Models/10lora_2/checkpoint_3_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 01:18:20 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 01:18:20 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 01:18:20 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 01:18:25 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:26<00:00, 16.21it/s]\n",
      "2024-03-04 01:22:24,814 - INFO - jaccard: 0.3335   f1: 0.4889   recall: 0.3957   precision: 0.6927   ddi: 0.1496   drug_num: 13.1769   pred_num: 405   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 35-Spironolactone, model_path: ../Models/10lora_2/checkpoint_3_of_10 ------\n",
      "INFO 03-04 01:22:24 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_3_of_10', tokenizer='../Models/10lora_2/checkpoint_3_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 01:22:58 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 01:22:58 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 01:22:58 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 01:23:02 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.27it/s]\n",
      "2024-03-04 01:27:02,891 - INFO - jaccard: 0.3372   f1: 0.4931   recall: 0.4012   precision: 0.6915   ddi: 0.1541   drug_num: 13.394   pred_num: 514   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 36-Nitroglycerin, model_path: ../Models/10lora_2/checkpoint_3_of_10 ------\n",
      "INFO 03-04 01:27:03 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_3_of_10', tokenizer='../Models/10lora_2/checkpoint_3_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 01:27:29 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 01:27:29 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 01:27:29 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 01:27:33 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.29it/s]\n",
      "2024-03-04 01:31:33,245 - INFO - jaccard: 0.3393   f1: 0.495   recall: 0.4036   precision: 0.6915   ddi: 0.153   drug_num: 13.4823   pred_num: 209   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 37-Prednisone, model_path: ../Models/10lora_2/checkpoint_3_of_10 ------\n",
      "INFO 03-04 01:31:33 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_3_of_10', tokenizer='../Models/10lora_2/checkpoint_3_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 01:31:57 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 01:31:58 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 01:31:58 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 01:32:02 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:24<00:00, 16.34it/s]\n",
      "2024-03-04 01:36:01,431 - INFO - jaccard: 0.3432   f1: 0.4996   recall: 0.4086   precision: 0.6929   ddi: 0.1502   drug_num: 13.6309   pred_num: 352   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 38-Sodium bicarbonate, model_path: ../Models/10lora_2/checkpoint_3_of_10 ------\n",
      "INFO 03-04 01:36:01 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_3_of_10', tokenizer='../Models/10lora_2/checkpoint_3_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 01:36:25 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 01:36:25 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 01:36:25 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 01:36:30 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.26it/s]\n",
      "2024-03-04 01:40:31,839 - INFO - jaccard: 0.3448   f1: 0.5013   recall: 0.411   precision: 0.6925   ddi: 0.1487   drug_num: 13.7255   pred_num: 224   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 39-Tazobactam, model_path: ../Models/10lora_2/checkpoint_3_of_10 ------\n",
      "INFO 03-04 01:40:31 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_3_of_10', tokenizer='../Models/10lora_2/checkpoint_3_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 01:40:56 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 01:40:56 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 01:40:56 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 01:41:01 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.32it/s]\n",
      "2024-03-04 01:45:02,332 - INFO - jaccard: 0.3469   f1: 0.5036   recall: 0.4145   precision: 0.6916   ddi: 0.1465   drug_num: 13.8877   pred_num: 384   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 40-Diphenhydramine, model_path: ../Models/10lora_2/checkpoint_3_of_10 ------\n",
      "INFO 03-04 01:45:02 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_3_of_10', tokenizer='../Models/10lora_2/checkpoint_3_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 01:45:27 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 01:45:27 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 01:45:27 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 01:45:31 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.30it/s]\n",
      "2024-03-04 01:49:32,168 - INFO - jaccard: 0.348   f1: 0.5048   recall: 0.4159   precision: 0.6917   ddi: 0.1476   drug_num: 13.9371   pred_num: 117   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 41-Lactulose, model_path: ../Models/10lora_2/checkpoint_3_of_10 ------\n",
      "INFO 03-04 01:49:32 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_3_of_10', tokenizer='../Models/10lora_2/checkpoint_3_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 01:49:57 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 01:49:57 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 01:49:57 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 01:50:01 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.33it/s]\n",
      "2024-03-04 01:54:02,718 - INFO - jaccard: 0.3504   f1: 0.5076   recall: 0.4193   precision: 0.6921   ddi: 0.1531   drug_num: 14.0401   pred_num: 244   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 42-Omeprazole, model_path: ../Models/10lora_2/checkpoint_3_of_10 ------\n",
      "INFO 03-04 01:54:02 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_3_of_10', tokenizer='../Models/10lora_2/checkpoint_3_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 01:54:26 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 01:54:26 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 01:54:26 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 01:54:31 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.31it/s]\n",
      "2024-03-04 01:58:33,607 - INFO - jaccard: 0.3551   f1: 0.5129   recall: 0.426   precision: 0.6923   ddi: 0.1496   drug_num: 14.2462   pred_num: 488   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 43-Clopidogrel, model_path: ../Models/10lora_2/checkpoint_3_of_10 ------\n",
      "INFO 03-04 01:58:33 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_3_of_10', tokenizer='../Models/10lora_2/checkpoint_3_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 01:58:57 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 01:58:57 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 01:58:57 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 01:59:02 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.31it/s]\n",
      "2024-03-04 02:03:04,377 - INFO - jaccard: 0.3585   f1: 0.5167   recall: 0.431   precision: 0.6927   ddi: 0.1482   drug_num: 14.3847   pred_num: 328   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 44-Zolpidem, model_path: ../Models/10lora_2/checkpoint_3_of_10 ------\n",
      "INFO 03-04 02:03:04 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_3_of_10', tokenizer='../Models/10lora_2/checkpoint_3_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 02:03:29 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 02:03:29 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 02:03:29 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 02:03:34 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.31it/s]\n",
      "2024-03-04 02:07:37,163 - INFO - jaccard: 0.3593   f1: 0.5176   recall: 0.4321   precision: 0.6926   ddi: 0.1473   drug_num: 14.4236   pred_num: 92   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 45-Fluticasone propionate, model_path: ../Models/10lora_2/checkpoint_4_of_10 ------\n",
      "INFO 03-04 02:07:37 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_4_of_10', tokenizer='../Models/10lora_2/checkpoint_4_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 02:08:10 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 02:08:10 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 02:08:10 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 02:08:15 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.28it/s]\n",
      "2024-03-04 02:12:18,278 - INFO - jaccard: 0.3635   f1: 0.5221   recall: 0.4379   precision: 0.6927   ddi: 0.1474   drug_num: 14.6174   pred_num: 459   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 46-Levothyroxine, model_path: ../Models/10lora_2/checkpoint_4_of_10 ------\n",
      "INFO 03-04 02:12:18 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_4_of_10', tokenizer='../Models/10lora_2/checkpoint_4_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 02:12:47 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 02:12:47 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 02:12:47 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 02:12:51 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.31it/s]\n",
      "2024-03-04 02:16:55,496 - INFO - jaccard: 0.3684   f1: 0.5275   recall: 0.4442   precision: 0.6949   ddi: 0.1457   drug_num: 14.7648   pred_num: 349   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 47-Norepinephrine, model_path: ../Models/10lora_2/checkpoint_4_of_10 ------\n",
      "INFO 03-04 02:16:55 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_4_of_10', tokenizer='../Models/10lora_2/checkpoint_4_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 02:17:26 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 02:17:26 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 02:17:26 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 02:17:31 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.31it/s]\n",
      "2024-03-04 02:21:34,904 - INFO - jaccard: 0.3694   f1: 0.5285   recall: 0.4467   precision: 0.6925   ddi: 0.1459   drug_num: 14.9278   pred_num: 386   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 48-Simvastatin, model_path: ../Models/10lora_2/checkpoint_4_of_10 ------\n",
      "INFO 03-04 02:21:34 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_4_of_10', tokenizer='../Models/10lora_2/checkpoint_4_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 02:22:05 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 02:22:05 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 02:22:05 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 02:22:09 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.33it/s]\n",
      "2024-03-04 02:26:14,427 - INFO - jaccard: 0.376   f1: 0.5354   recall: 0.4555   precision: 0.6945   ddi: 0.142   drug_num: 15.1448   pred_num: 514   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 49-Ranitidine, model_path: ../Models/10lora_2/checkpoint_4_of_10 ------\n",
      "INFO 03-04 02:26:14 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_4_of_10', tokenizer='../Models/10lora_2/checkpoint_4_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 02:26:49 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 02:26:49 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 02:26:49 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 02:26:54 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.32it/s]\n",
      "2024-03-04 02:30:59,385 - INFO - jaccard: 0.3787   f1: 0.538   recall: 0.4589   precision: 0.6948   ddi: 0.1418   drug_num: 15.2627   pred_num: 279   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 50-Monopotassium phosphate, model_path: ../Models/10lora_2/checkpoint_4_of_10 ------\n",
      "INFO 03-04 02:30:59 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_4_of_10', tokenizer='../Models/10lora_2/checkpoint_4_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 02:31:31 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 02:31:31 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 02:31:31 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 02:31:35 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.24it/s]\n",
      "2024-03-04 02:35:42,341 - INFO - jaccard: 0.3788   f1: 0.5381   recall: 0.4592   precision: 0.6948   ddi: 0.1419   drug_num: 15.2736   pred_num: 26   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 51-Thiamine, model_path: ../Models/10lora_2/checkpoint_4_of_10 ------\n",
      "INFO 03-04 02:35:42 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_4_of_10', tokenizer='../Models/10lora_2/checkpoint_4_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 02:36:11 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 02:36:11 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 02:36:11 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 02:36:15 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:24<00:00, 16.35it/s]\n",
      "2024-03-04 02:40:21,575 - INFO - jaccard: 0.3822   f1: 0.5422   recall: 0.4638   precision: 0.6951   ddi: 0.139   drug_num: 15.4185   pred_num: 343   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 52-Piperacillin, model_path: ../Models/10lora_2/checkpoint_4_of_10 ------\n",
      "INFO 03-04 02:40:21 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_4_of_10', tokenizer='../Models/10lora_2/checkpoint_4_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 02:40:54 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 02:40:54 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 02:40:54 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 02:40:58 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.31it/s]\n",
      "2024-03-04 02:45:05,463 - INFO - jaccard: 0.3835   f1: 0.5435   recall: 0.4663   precision: 0.6942   ddi: 0.1376   drug_num: 15.5359   pred_num: 278   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 53-Amiodarone, model_path: ../Models/10lora_2/checkpoint_4_of_10 ------\n",
      "INFO 03-04 02:45:05 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_4_of_10', tokenizer='../Models/10lora_2/checkpoint_4_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 02:45:31 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 02:45:31 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 02:45:31 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 02:45:36 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.32it/s]\n",
      "2024-03-04 02:49:42,656 - INFO - jaccard: 0.3859   f1: 0.5457   recall: 0.4695   precision: 0.6945   ddi: 0.1392   drug_num: 15.6377   pred_num: 241   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 54-Haloperidol, model_path: ../Models/10lora_2/checkpoint_4_of_10 ------\n",
      "INFO 03-04 02:49:42 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_4_of_10', tokenizer='../Models/10lora_2/checkpoint_4_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 02:50:07 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 02:50:07 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 02:50:07 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 02:50:12 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.32it/s]\n",
      "2024-03-04 02:54:19,800 - INFO - jaccard: 0.3864   f1: 0.5463   recall: 0.4713   precision: 0.6924   ddi: 0.1389   drug_num: 15.7656   pred_num: 303   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 55-Ceftriaxone, model_path: ../Models/10lora_2/checkpoint_4_of_10 ------\n",
      "INFO 03-04 02:54:19 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_4_of_10', tokenizer='../Models/10lora_2/checkpoint_4_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 02:54:44 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 02:54:44 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 02:54:44 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 02:54:49 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.32it/s]\n",
      "2024-03-04 02:58:56,777 - INFO - jaccard: 0.3867   f1: 0.5466   recall: 0.472   precision: 0.692   ddi: 0.1384   drug_num: 15.7994   pred_num: 80   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 56-Trazodone, model_path: ../Models/10lora_2/checkpoint_4_of_10 ------\n",
      "INFO 03-04 02:58:56 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_4_of_10', tokenizer='../Models/10lora_2/checkpoint_4_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 02:59:21 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 02:59:22 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 02:59:22 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 02:59:26 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.32it/s]\n",
      "2024-03-04 03:03:34,353 - INFO - jaccard: 0.3883   f1: 0.5482   recall: 0.4743   precision: 0.6919   ddi: 0.1399   drug_num: 15.8716   pred_num: 171   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 57-Phylloquinone, model_path: ../Models/10lora_2/checkpoint_4_of_10 ------\n",
      "INFO 03-04 03:03:34 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_4_of_10', tokenizer='../Models/10lora_2/checkpoint_4_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 03:03:59 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 03:03:59 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 03:03:59 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 03:04:04 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.31it/s]\n",
      "2024-03-04 03:08:12,450 - INFO - jaccard: 0.3891   f1: 0.5491   recall: 0.4761   precision: 0.6907   ddi: 0.1387   drug_num: 15.9793   pred_num: 255   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 58-Cefazolin, model_path: ../Models/10lora_2/checkpoint_4_of_10 ------\n",
      "INFO 03-04 03:08:12 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_4_of_10', tokenizer='../Models/10lora_2/checkpoint_4_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 03:08:36 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 03:08:36 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 03:08:36 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 03:08:40 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.32it/s]\n",
      "2024-03-04 03:12:49,451 - INFO - jaccard: 0.3908   f1: 0.5507   recall: 0.4788   precision: 0.6905   ddi: 0.1366   drug_num: 16.0726   pred_num: 221   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 59-Miconazole, model_path: ../Models/10lora_2/checkpoint_4_of_10 ------\n",
      "INFO 03-04 03:12:49 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_4_of_10', tokenizer='../Models/10lora_2/checkpoint_4_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 03:13:13 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 03:13:13 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 03:13:13 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 03:13:17 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.33it/s]\n",
      "2024-03-04 03:17:27,416 - INFO - jaccard: 0.3915   f1: 0.5515   recall: 0.4804   precision: 0.6894   ddi: 0.1352   drug_num: 16.1824   pred_num: 260   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 60-Acetylcysteine, model_path: ../Models/10lora_2/checkpoint_5_of_10 ------\n",
      "INFO 03-04 03:17:27 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_5_of_10', tokenizer='../Models/10lora_2/checkpoint_5_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 03:18:01 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 03:18:01 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 03:18:01 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 03:18:06 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.29it/s]\n",
      "2024-03-04 03:22:16,255 - INFO - jaccard: 0.3916   f1: 0.5515   recall: 0.4812   precision: 0.6882   ddi: 0.1343   drug_num: 16.2563   pred_num: 175   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 61-Cefepime, model_path: ../Models/10lora_2/checkpoint_5_of_10 ------\n",
      "INFO 03-04 03:22:16 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_5_of_10', tokenizer='../Models/10lora_2/checkpoint_5_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 03:22:41 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 03:22:41 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 03:22:41 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 03:22:45 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.33it/s]\n",
      "2024-03-04 03:26:55,095 - INFO - jaccard: 0.3928   f1: 0.5527   recall: 0.483   precision: 0.6881   ddi: 0.1334   drug_num: 16.3285   pred_num: 171   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 62-Gabapentin, model_path: ../Models/10lora_2/checkpoint_5_of_10 ------\n",
      "INFO 03-04 03:26:55 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_5_of_10', tokenizer='../Models/10lora_2/checkpoint_5_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 03:27:20 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 03:27:20 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 03:27:20 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 03:27:25 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.29it/s]\n",
      "2024-03-04 03:31:35,018 - INFO - jaccard: 0.3956   f1: 0.5556   recall: 0.4869   precision: 0.6884   ddi: 0.1339   drug_num: 16.4426   pred_num: 270   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 63-Enalaprilat, model_path: ../Models/10lora_2/checkpoint_5_of_10 ------\n",
      "INFO 03-04 03:31:35 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_5_of_10', tokenizer='../Models/10lora_2/checkpoint_5_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 03:32:05 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 03:32:05 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 03:32:05 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 03:32:09 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.31it/s]\n",
      "2024-03-04 03:36:19,863 - INFO - jaccard: 0.3954   f1: 0.5554   recall: 0.4869   precision: 0.6877   ddi: 0.1339   drug_num: 16.4649   pred_num: 53   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 64-Amlodipine, model_path: ../Models/10lora_2/checkpoint_5_of_10 ------\n",
      "INFO 03-04 03:36:19 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_5_of_10', tokenizer='../Models/10lora_2/checkpoint_5_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 03:36:49 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 03:36:49 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 03:36:49 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 03:36:53 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.25it/s]\n",
      "2024-03-04 03:41:04,557 - INFO - jaccard: 0.3978   f1: 0.5579   recall: 0.4903   precision: 0.688   ddi: 0.1351   drug_num: 16.5625   pred_num: 231   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 65-Folic acid, model_path: ../Models/10lora_2/checkpoint_5_of_10 ------\n",
      "INFO 03-04 03:41:04 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_5_of_10', tokenizer='../Models/10lora_2/checkpoint_5_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 03:41:35 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 03:41:36 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 03:41:36 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 03:41:40 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.31it/s]\n",
      "2024-03-04 03:45:51,703 - INFO - jaccard: 0.3989   f1: 0.5593   recall: 0.492   precision: 0.688   ddi: 0.1341   drug_num: 16.6318   pred_num: 164   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 66-Nystatin, model_path: ../Models/10lora_2/checkpoint_5_of_10 ------\n",
      "INFO 03-04 03:45:51 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_5_of_10', tokenizer='../Models/10lora_2/checkpoint_5_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 03:46:30 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 03:46:30 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 03:46:30 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 03:46:35 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.31it/s]\n",
      "2024-03-04 03:50:46,468 - INFO - jaccard: 0.3998   f1: 0.5603   recall: 0.4937   precision: 0.6873   ddi: 0.1329   drug_num: 16.7289   pred_num: 230   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 67-Cyanocobalamin, model_path: ../Models/10lora_2/checkpoint_5_of_10 ------\n",
      "INFO 03-04 03:50:46 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_5_of_10', tokenizer='../Models/10lora_2/checkpoint_5_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 03:51:25 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 03:51:25 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 03:51:25 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 03:51:29 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:26<00:00, 16.22it/s]\n",
      "2024-03-04 03:55:42,166 - INFO - jaccard: 0.4015   f1: 0.5621   recall: 0.4965   precision: 0.6866   ddi: 0.1314   drug_num: 16.8514   pred_num: 290   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 68-Prochlorperazine, model_path: ../Models/10lora_2/checkpoint_5_of_10 ------\n",
      "INFO 03-04 03:55:42 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_5_of_10', tokenizer='../Models/10lora_2/checkpoint_5_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 03:56:17 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 03:56:17 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 03:56:17 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 03:56:22 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.32it/s]\n",
      "2024-03-04 04:00:33,630 - INFO - jaccard: 0.4021   f1: 0.5628   recall: 0.4975   precision: 0.6863   ddi: 0.1318   drug_num: 16.8915   pred_num: 95   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 69-Diltiazem, model_path: ../Models/10lora_2/checkpoint_5_of_10 ------\n",
      "INFO 03-04 04:00:33 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_5_of_10', tokenizer='../Models/10lora_2/checkpoint_5_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 04:01:06 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 04:01:06 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 04:01:06 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 04:01:10 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:24<00:00, 16.34it/s]\n",
      "2024-03-04 04:05:21,305 - INFO - jaccard: 0.4034   f1: 0.564   recall: 0.4993   precision: 0.6865   ddi: 0.1328   drug_num: 16.9476   pred_num: 133   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 70-Sulfamethoxazole, model_path: ../Models/10lora_2/checkpoint_5_of_10 ------\n",
      "INFO 03-04 04:05:21 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_5_of_10', tokenizer='../Models/10lora_2/checkpoint_5_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 04:05:46 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 04:05:46 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 04:05:46 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 04:05:51 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.28it/s]\n",
      "2024-03-04 04:10:04,631 - INFO - jaccard: 0.4044   f1: 0.5652   recall: 0.5012   precision: 0.6859   ddi: 0.1333   drug_num: 17.0338   pred_num: 204   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 71-Methylprednisolone hemisuccinate, model_path: ../Models/10lora_2/checkpoint_5_of_10 ------\n",
      "INFO 03-04 04:10:04 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_5_of_10', tokenizer='../Models/10lora_2/checkpoint_5_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 04:10:30 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 04:10:30 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 04:10:30 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 04:10:34 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:26<00:00, 16.22it/s]\n",
      "2024-03-04 04:14:47,722 - INFO - jaccard: 0.4056   f1: 0.5664   recall: 0.5031   precision: 0.6855   ddi: 0.1324   drug_num: 17.1204   pred_num: 205   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 72-Trimethoprim, model_path: ../Models/10lora_2/checkpoint_5_of_10 ------\n",
      "INFO 03-04 04:14:47 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_5_of_10', tokenizer='../Models/10lora_2/checkpoint_5_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 04:15:13 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 04:15:13 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 04:15:13 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 04:15:17 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.29it/s]\n",
      "2024-03-04 04:19:29,891 - INFO - jaccard: 0.4067   f1: 0.5675   recall: 0.5049   precision: 0.6852   ddi: 0.1322   drug_num: 17.1951   pred_num: 177   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 73-Dopamine, model_path: ../Models/10lora_2/checkpoint_5_of_10 ------\n",
      "INFO 03-04 04:19:29 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_5_of_10', tokenizer='../Models/10lora_2/checkpoint_5_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 04:19:56 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 04:19:56 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 04:19:56 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 04:20:00 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:24<00:00, 16.34it/s]\n",
      "2024-03-04 04:24:13,542 - INFO - jaccard: 0.4066   f1: 0.5674   recall: 0.505   precision: 0.6848   ddi: 0.1322   drug_num: 17.2116   pred_num: 39   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 74-Fluconazole, model_path: ../Models/10lora_2/checkpoint_5_of_10 ------\n",
      "INFO 03-04 04:24:13 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_5_of_10', tokenizer='../Models/10lora_2/checkpoint_5_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 04:24:37 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 04:24:37 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 04:24:37 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 04:24:42 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.33it/s]\n",
      "2024-03-04 04:28:55,079 - INFO - jaccard: 0.4076   f1: 0.5684   recall: 0.5066   precision: 0.6846   ddi: 0.1326   drug_num: 17.2876   pred_num: 180   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 75-Glycopyrronium, model_path: ../Models/10lora_2/checkpoint_6_of_10 ------\n",
      "INFO 03-04 04:28:55 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_6_of_10', tokenizer='../Models/10lora_2/checkpoint_6_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 04:29:29 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 04:29:29 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 04:29:29 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 04:29:33 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.32it/s]\n",
      "2024-03-04 04:33:46,470 - INFO - jaccard: 0.4093   f1: 0.5697   recall: 0.5087   precision: 0.6848   ddi: 0.1332   drug_num: 17.3623   pred_num: 177   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 76-Meropenem, model_path: ../Models/10lora_2/checkpoint_6_of_10 ------\n",
      "INFO 03-04 04:33:46 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_6_of_10', tokenizer='../Models/10lora_2/checkpoint_6_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 04:34:11 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 04:34:11 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 04:34:11 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 04:34:15 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.31it/s]\n",
      "2024-03-04 04:38:29,140 - INFO - jaccard: 0.4103   f1: 0.5707   recall: 0.5102   precision: 0.6849   ddi: 0.1326   drug_num: 17.421   pred_num: 139   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 77-Ergocalciferol, model_path: ../Models/10lora_2/checkpoint_6_of_10 ------\n",
      "INFO 03-04 04:38:29 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_6_of_10', tokenizer='../Models/10lora_2/checkpoint_6_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 04:38:54 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 04:38:54 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 04:38:54 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 04:38:58 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.31it/s]\n",
      "2024-03-04 04:43:12,746 - INFO - jaccard: 0.412   f1: 0.5724   recall: 0.5138   precision: 0.6832   ddi: 0.131   drug_num: 17.5621   pred_num: 334   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 78-Sucralfate, model_path: ../Models/10lora_2/checkpoint_6_of_10 ------\n",
      "INFO 03-04 04:43:12 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_6_of_10', tokenizer='../Models/10lora_2/checkpoint_6_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 04:43:37 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 04:43:37 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 04:43:37 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 04:43:41 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.32it/s]\n",
      "2024-03-04 04:47:56,146 - INFO - jaccard: 0.4117   f1: 0.5723   recall: 0.5147   precision: 0.6812   ddi: 0.1299   drug_num: 17.6491   pred_num: 206   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 79-Hydrocortisone, model_path: ../Models/10lora_2/checkpoint_6_of_10 ------\n",
      "INFO 03-04 04:47:56 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_6_of_10', tokenizer='../Models/10lora_2/checkpoint_6_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 04:48:20 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 04:48:20 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 04:48:20 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 04:48:25 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.31it/s]\n",
      "2024-03-04 04:52:40,352 - INFO - jaccard: 0.4122   f1: 0.5728   recall: 0.5158   precision: 0.6806   ddi: 0.1294   drug_num: 17.7116   pred_num: 148   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 80-Guaifenesin, model_path: ../Models/10lora_2/checkpoint_6_of_10 ------\n",
      "INFO 03-04 04:52:40 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_6_of_10', tokenizer='../Models/10lora_2/checkpoint_6_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 04:53:06 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 04:53:06 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 04:53:06 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 04:53:10 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.33it/s]\n",
      "2024-03-04 04:57:24,738 - INFO - jaccard: 0.4126   f1: 0.5731   recall: 0.5164   precision: 0.6805   ddi: 0.1291   drug_num: 17.7327   pred_num: 50   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 81-Meperidine, model_path: ../Models/10lora_2/checkpoint_6_of_10 ------\n",
      "INFO 03-04 04:57:24 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_6_of_10', tokenizer='../Models/10lora_2/checkpoint_6_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 04:57:50 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 04:57:50 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 04:57:50 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 04:57:54 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:24<00:00, 16.34it/s]\n",
      "2024-03-04 05:02:09,689 - INFO - jaccard: 0.4127   f1: 0.5732   recall: 0.517   precision: 0.6797   ddi: 0.1297   drug_num: 17.7859   pred_num: 126   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 82-Methylprednisolone, model_path: ../Models/10lora_2/checkpoint_6_of_10 ------\n",
      "INFO 03-04 05:02:09 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_6_of_10', tokenizer='../Models/10lora_2/checkpoint_6_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 05:02:43 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 05:02:43 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 05:02:43 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 05:02:47 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.29it/s]\n",
      "2024-03-04 05:07:02,016 - INFO - jaccard: 0.4137   f1: 0.5742   recall: 0.5187   precision: 0.6798   ddi: 0.1297   drug_num: 17.8459   pred_num: 142   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 83-Captopril, model_path: ../Models/10lora_2/checkpoint_6_of_10 ------\n",
      "INFO 03-04 05:07:02 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_6_of_10', tokenizer='../Models/10lora_2/checkpoint_6_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 05:07:34 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 05:07:35 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 05:07:35 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 05:07:39 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:24<00:00, 16.36it/s]\n",
      "2024-03-04 05:11:53,275 - INFO - jaccard: 0.414   f1: 0.5745   recall: 0.5191   precision: 0.6798   ddi: 0.1299   drug_num: 17.8573   pred_num: 27   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 84-Salmeterol, model_path: ../Models/10lora_2/checkpoint_6_of_10 ------\n",
      "INFO 03-04 05:11:53 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_6_of_10', tokenizer='../Models/10lora_2/checkpoint_6_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 05:12:27 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 05:12:27 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 05:12:27 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 05:12:32 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:24<00:00, 16.37it/s]\n",
      "2024-03-04 05:16:46,267 - INFO - jaccard: 0.4163   f1: 0.5766   recall: 0.5223   precision: 0.6801   ddi: 0.1308   drug_num: 17.9561   pred_num: 234   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 85-Neostigmine, model_path: ../Models/10lora_2/checkpoint_6_of_10 ------\n",
      "INFO 03-04 05:16:46 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_6_of_10', tokenizer='../Models/10lora_2/checkpoint_6_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 05:17:25 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 05:17:25 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 05:17:25 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 05:17:29 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:24<00:00, 16.33it/s]\n",
      "2024-03-04 05:21:44,280 - INFO - jaccard: 0.4179   f1: 0.5779   recall: 0.5243   precision: 0.6804   ddi: 0.1303   drug_num: 18.0274   pred_num: 169   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 86-Olanzapine, model_path: ../Models/10lora_2/checkpoint_6_of_10 ------\n",
      "INFO 03-04 05:21:44 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_6_of_10', tokenizer='../Models/10lora_2/checkpoint_6_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 05:22:17 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 05:22:17 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 05:22:17 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 05:22:21 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.33it/s]\n",
      "2024-03-04 05:26:37,274 - INFO - jaccard: 0.4185   f1: 0.5785   recall: 0.5252   precision: 0.6806   ddi: 0.1311   drug_num: 18.0541   pred_num: 63   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 87-Magnesium hydroxide, model_path: ../Models/10lora_2/checkpoint_6_of_10 ------\n",
      "INFO 03-04 05:26:37 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_6_of_10', tokenizer='../Models/10lora_2/checkpoint_6_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 05:27:06 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 05:27:06 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 05:27:06 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 05:27:10 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.31it/s]\n",
      "2024-03-04 05:31:26,642 - INFO - jaccard: 0.4185   f1: 0.5786   recall: 0.5252   precision: 0.6806   ddi: 0.1311   drug_num: 18.0562   pred_num: 5   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 88-Labetalol, model_path: ../Models/10lora_2/checkpoint_6_of_10 ------\n",
      "INFO 03-04 05:31:26 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_6_of_10', tokenizer='../Models/10lora_2/checkpoint_6_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 05:31:53 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 05:31:53 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 05:31:53 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 05:31:57 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:24<00:00, 16.37it/s]\n",
      "2024-03-04 05:36:12,911 - INFO - jaccard: 0.4196   f1: 0.5797   recall: 0.5268   precision: 0.6805   ddi: 0.1307   drug_num: 18.1111   pred_num: 130   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 89-Ibuprofen, model_path: ../Models/10lora_2/checkpoint_6_of_10 ------\n",
      "INFO 03-04 05:36:12 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_6_of_10', tokenizer='../Models/10lora_2/checkpoint_6_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 05:36:38 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 05:36:38 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 05:36:38 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 05:36:42 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:24<00:00, 16.35it/s]\n",
      "2024-03-04 05:40:57,673 - INFO - jaccard: 0.4197   f1: 0.5798   recall: 0.5272   precision: 0.6803   ddi: 0.1308   drug_num: 18.1267   pred_num: 37   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 90-Digoxin, model_path: ../Models/10lora_2/checkpoint_7_of_10 ------\n",
      "INFO 03-04 05:40:57 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_7_of_10', tokenizer='../Models/10lora_2/checkpoint_7_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 05:41:32 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 05:41:32 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 05:41:32 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 05:41:36 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:24<00:00, 16.35it/s]\n",
      "2024-03-04 05:45:52,255 - INFO - jaccard: 0.421   f1: 0.5811   recall: 0.529   precision: 0.6806   ddi: 0.1313   drug_num: 18.1727   pred_num: 109   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 91-Ampicillin, model_path: ../Models/10lora_2/checkpoint_7_of_10 ------\n",
      "INFO 03-04 05:45:52 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_7_of_10', tokenizer='../Models/10lora_2/checkpoint_7_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 05:46:17 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 05:46:17 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 05:46:17 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 05:46:22 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.33it/s]\n",
      "2024-03-04 05:50:38,367 - INFO - jaccard: 0.4211   f1: 0.5811   recall: 0.5291   precision: 0.6806   ddi: 0.1312   drug_num: 18.1765   pred_num: 9   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 92-Citalopram, model_path: ../Models/10lora_2/checkpoint_7_of_10 ------\n",
      "INFO 03-04 05:50:38 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_7_of_10', tokenizer='../Models/10lora_2/checkpoint_7_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 05:51:02 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 05:51:03 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 05:51:03 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 05:51:07 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:24<00:00, 16.34it/s]\n",
      "2024-03-04 05:55:24,440 - INFO - jaccard: 0.4235   f1: 0.5834   recall: 0.5322   precision: 0.6814   ddi: 0.1332   drug_num: 18.2555   pred_num: 187   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 93-Pyridoxine, model_path: ../Models/10lora_2/checkpoint_7_of_10 ------\n",
      "INFO 03-04 05:55:24 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_7_of_10', tokenizer='../Models/10lora_2/checkpoint_7_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 05:55:49 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 05:55:49 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 05:55:49 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 05:55:53 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.32it/s]\n",
      "2024-03-04 06:00:09,521 - INFO - jaccard: 0.4246   f1: 0.5846   recall: 0.5337   precision: 0.6817   ddi: 0.1327   drug_num: 18.3074   pred_num: 123   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 94-Ascorbic acid, model_path: ../Models/10lora_2/checkpoint_7_of_10 ------\n",
      "INFO 03-04 06:00:09 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_7_of_10', tokenizer='../Models/10lora_2/checkpoint_7_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 06:00:34 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 06:00:34 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 06:00:34 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 06:00:39 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.32it/s]\n",
      "2024-03-04 06:04:56,212 - INFO - jaccard: 0.4257   f1: 0.5857   recall: 0.5351   precision: 0.6818   ddi: 0.1323   drug_num: 18.3598   pred_num: 124   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 95-Niacin, model_path: ../Models/10lora_2/checkpoint_7_of_10 ------\n",
      "INFO 03-04 06:04:56 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_7_of_10', tokenizer='../Models/10lora_2/checkpoint_7_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 06:05:21 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 06:05:21 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 06:05:21 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 06:05:25 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:24<00:00, 16.36it/s]\n",
      "2024-03-04 06:09:42,490 - INFO - jaccard: 0.427   f1: 0.5869   recall: 0.5368   precision: 0.682   ddi: 0.132   drug_num: 18.4172   pred_num: 136   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 96-Dolasetron, model_path: ../Models/10lora_2/checkpoint_7_of_10 ------\n",
      "INFO 03-04 06:09:42 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_7_of_10', tokenizer='../Models/10lora_2/checkpoint_7_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 06:10:08 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 06:10:09 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 06:10:09 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 06:10:13 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:24<00:00, 16.35it/s]\n",
      "2024-03-04 06:14:30,644 - INFO - jaccard: 0.427   f1: 0.5869   recall: 0.5368   precision: 0.682   ddi: 0.132   drug_num: 18.4172   pred_num: 0   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 97-Tetracosactide, model_path: ../Models/10lora_2/checkpoint_7_of_10 ------\n",
      "INFO 03-04 06:14:30 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_7_of_10', tokenizer='../Models/10lora_2/checkpoint_7_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 06:14:55 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 06:14:55 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 06:14:55 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 06:15:00 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.31it/s]\n",
      "2024-03-04 06:19:17,074 - INFO - jaccard: 0.427   f1: 0.5869   recall: 0.5368   precision: 0.682   ddi: 0.132   drug_num: 18.4206   pred_num: 8   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 98-Atropine, model_path: ../Models/10lora_2/checkpoint_7_of_10 ------\n",
      "INFO 03-04 06:19:17 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_7_of_10', tokenizer='../Models/10lora_2/checkpoint_7_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 06:19:47 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 06:19:47 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 06:19:47 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 06:19:52 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:24<00:00, 16.36it/s]\n",
      "2024-03-04 06:24:08,680 - INFO - jaccard: 0.4272   f1: 0.5871   recall: 0.5375   precision: 0.6814   ddi: 0.133   drug_num: 18.4548   pred_num: 81   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 99-Sevelamer, model_path: ../Models/10lora_2/checkpoint_7_of_10 ------\n",
      "INFO 03-04 06:24:08 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_7_of_10', tokenizer='../Models/10lora_2/checkpoint_7_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 06:24:39 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 06:24:39 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 06:24:39 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 06:24:44 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:24<00:00, 16.36it/s]\n",
      "2024-03-04 06:29:01,679 - INFO - jaccard: 0.4279   f1: 0.5877   recall: 0.5386   precision: 0.6811   ddi: 0.1328   drug_num: 18.5038   pred_num: 116   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 100-Riboflavin, model_path: ../Models/10lora_2/checkpoint_7_of_10 ------\n",
      "INFO 03-04 06:29:01 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_7_of_10', tokenizer='../Models/10lora_2/checkpoint_7_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 06:29:35 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 06:29:35 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 06:29:35 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 06:29:40 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:24<00:00, 16.33it/s]\n",
      "2024-03-04 06:33:57,162 - INFO - jaccard: 0.429   f1: 0.5887   recall: 0.54   precision: 0.6813   ddi: 0.1325   drug_num: 18.5515   pred_num: 113   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 101-Biotin, model_path: ../Models/10lora_2/checkpoint_7_of_10 ------\n",
      "INFO 03-04 06:33:57 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_7_of_10', tokenizer='../Models/10lora_2/checkpoint_7_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 06:34:27 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 06:34:27 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 06:34:27 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 06:34:32 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:24<00:00, 16.36it/s]\n",
      "2024-03-04 06:38:50,485 - INFO - jaccard: 0.4301   f1: 0.5896   recall: 0.5414   precision: 0.6814   ddi: 0.1323   drug_num: 18.6039   pred_num: 124   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 102-Pantothenic acid, model_path: ../Models/10lora_2/checkpoint_7_of_10 ------\n",
      "INFO 03-04 06:38:50 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_7_of_10', tokenizer='../Models/10lora_2/checkpoint_7_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 06:39:18 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 06:39:18 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 06:39:18 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 06:39:23 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.31it/s]\n",
      "2024-03-04 06:43:41,499 - INFO - jaccard: 0.4313   f1: 0.5906   recall: 0.5429   precision: 0.6815   ddi: 0.1321   drug_num: 18.6601   pred_num: 133   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 103-Sodium chloride, model_path: ../Models/10lora_2/checkpoint_7_of_10 ------\n",
      "INFO 03-04 06:43:41 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_7_of_10', tokenizer='../Models/10lora_2/checkpoint_7_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 06:44:11 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 06:44:11 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 06:44:11 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 06:44:15 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.30it/s]\n",
      "2024-03-04 06:48:34,143 - INFO - jaccard: 0.4325   f1: 0.5917   recall: 0.5446   precision: 0.6819   ddi: 0.1316   drug_num: 18.6943   pred_num: 81   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 104-Hydrocortisone succinate, model_path: ../Models/10lora_2/checkpoint_7_of_10 ------\n",
      "INFO 03-04 06:48:34 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_7_of_10', tokenizer='../Models/10lora_2/checkpoint_7_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 06:49:04 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 06:49:04 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 06:49:04 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 06:49:08 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.27it/s]\n",
      "2024-03-04 06:53:27,755 - INFO - jaccard: 0.4328   f1: 0.592   recall: 0.5452   precision: 0.6818   ddi: 0.1315   drug_num: 18.7272   pred_num: 78   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 105-Dexamethasone, model_path: ../Models/10lora_2/checkpoint_8_of_10 ------\n",
      "INFO 03-04 06:53:27 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_8_of_10', tokenizer='../Models/10lora_2/checkpoint_8_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 06:54:05 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 06:54:05 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 06:54:05 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 06:54:10 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.29it/s]\n",
      "2024-03-04 06:58:29,351 - INFO - jaccard: 0.4339   f1: 0.5932   recall: 0.5469   precision: 0.6817   ddi: 0.1308   drug_num: 18.7867   pred_num: 141   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 106-Tramadol, model_path: ../Models/10lora_2/checkpoint_8_of_10 ------\n",
      "INFO 03-04 06:58:29 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_8_of_10', tokenizer='../Models/10lora_2/checkpoint_8_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 06:58:54 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 06:58:54 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 06:58:54 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 06:58:59 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:24<00:00, 16.33it/s]\n",
      "2024-03-04 07:03:16,969 - INFO - jaccard: 0.4345   f1: 0.5937   recall: 0.548   precision: 0.6814   ddi: 0.1311   drug_num: 18.8281   pred_num: 98   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 107-Ketorolac, model_path: ../Models/10lora_2/checkpoint_8_of_10 ------\n",
      "INFO 03-04 07:03:16 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_8_of_10', tokenizer='../Models/10lora_2/checkpoint_8_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 07:03:42 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 07:03:42 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 07:03:42 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 07:03:46 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.32it/s]\n",
      "2024-03-04 07:08:05,770 - INFO - jaccard: 0.4343   f1: 0.5936   recall: 0.5488   precision: 0.68   ddi: 0.1315   drug_num: 18.902   pred_num: 175   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 108-Levetiracetam, model_path: ../Models/10lora_2/checkpoint_8_of_10 ------\n",
      "INFO 03-04 07:08:05 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_8_of_10', tokenizer='../Models/10lora_2/checkpoint_8_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 07:08:31 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 07:08:31 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 07:08:31 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 07:08:35 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.30it/s]\n",
      "2024-03-04 07:12:53,985 - INFO - jaccard: 0.4366   f1: 0.5959   recall: 0.5521   precision: 0.6806   ddi: 0.1325   drug_num: 18.9878   pred_num: 203   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 109-Gentamicin, model_path: ../Models/10lora_2/checkpoint_8_of_10 ------\n",
      "INFO 03-04 07:12:53 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_8_of_10', tokenizer='../Models/10lora_2/checkpoint_8_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 07:13:18 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 07:13:18 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 07:13:18 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 07:13:22 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:24<00:00, 16.33it/s]\n",
      "2024-03-04 07:17:41,807 - INFO - jaccard: 0.4366   f1: 0.596   recall: 0.5523   precision: 0.6805   ddi: 0.1324   drug_num: 18.9979   pred_num: 24   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 110-Azithromycin, model_path: ../Models/10lora_2/checkpoint_8_of_10 ------\n",
      "INFO 03-04 07:17:41 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_8_of_10', tokenizer='../Models/10lora_2/checkpoint_8_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 07:18:07 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 07:18:07 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 07:18:07 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 07:18:11 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.32it/s]\n",
      "2024-03-04 07:22:30,420 - INFO - jaccard: 0.4368   f1: 0.5962   recall: 0.5529   precision: 0.6801   ddi: 0.1327   drug_num: 19.0321   pred_num: 81   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 111-Sodium phosphate, monobasic, model_path: ../Models/10lora_2/checkpoint_8_of_10 ------\n",
      "INFO 03-04 07:22:30 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_8_of_10', tokenizer='../Models/10lora_2/checkpoint_8_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 07:22:57 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 07:22:57 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 07:22:57 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 07:23:01 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:26<00:00, 16.20it/s]\n",
      "2024-03-04 07:27:22,495 - INFO - jaccard: 0.4368   f1: 0.5962   recall: 0.5529   precision: 0.6801   ddi: 0.1327   drug_num: 19.0334   pred_num: 3   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 112-Isosorbide, model_path: ../Models/10lora_2/checkpoint_8_of_10 ------\n",
      "INFO 03-04 07:27:22 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_8_of_10', tokenizer='../Models/10lora_2/checkpoint_8_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 07:27:49 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 07:27:49 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 07:27:49 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 07:27:53 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.32it/s]\n",
      "2024-03-04 07:32:13,629 - INFO - jaccard: 0.4375   f1: 0.5968   recall: 0.5541   precision: 0.68   ddi: 0.1326   drug_num: 19.0807   pred_num: 112   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 113-Diazepam, model_path: ../Models/10lora_2/checkpoint_8_of_10 ------\n",
      "INFO 03-04 07:32:13 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_8_of_10', tokenizer='../Models/10lora_2/checkpoint_8_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 07:32:43 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 07:32:43 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 07:32:43 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 07:32:47 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:24<00:00, 16.34it/s]\n",
      "2024-03-04 07:37:06,743 - INFO - jaccard: 0.4386   f1: 0.598   recall: 0.5557   precision: 0.6803   ddi: 0.1331   drug_num: 19.1166   pred_num: 85   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 114-Quetiapine, model_path: ../Models/10lora_2/checkpoint_8_of_10 ------\n",
      "INFO 03-04 07:37:06 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_8_of_10', tokenizer='../Models/10lora_2/checkpoint_8_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 07:37:38 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 07:37:38 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 07:37:38 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 07:37:42 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:24<00:00, 16.34it/s]\n",
      "2024-03-04 07:42:02,856 - INFO - jaccard: 0.4393   f1: 0.5987   recall: 0.5566   precision: 0.6805   ddi: 0.1335   drug_num: 19.1385   pred_num: 52   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 115-Atenolol, model_path: ../Models/10lora_2/checkpoint_8_of_10 ------\n",
      "INFO 03-04 07:42:02 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_8_of_10', tokenizer='../Models/10lora_2/checkpoint_8_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 07:42:33 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 07:42:34 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 07:42:34 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 07:42:38 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.33it/s]\n",
      "2024-03-04 07:46:58,697 - INFO - jaccard: 0.4403   f1: 0.5997   recall: 0.5585   precision: 0.6801   ddi: 0.1343   drug_num: 19.2052   pred_num: 158   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 116-Allopurinol, model_path: ../Models/10lora_2/checkpoint_8_of_10 ------\n",
      "INFO 03-04 07:46:58 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_8_of_10', tokenizer='../Models/10lora_2/checkpoint_8_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 07:47:29 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 07:47:30 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 07:47:30 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 07:47:34 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.32it/s]\n",
      "2024-03-04 07:51:54,972 - INFO - jaccard: 0.4416   f1: 0.6009   recall: 0.5604   precision: 0.6802   ddi: 0.134   drug_num: 19.2698   pred_num: 153   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 117-Linezolid, model_path: ../Models/10lora_2/checkpoint_8_of_10 ------\n",
      "INFO 03-04 07:51:54 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_8_of_10', tokenizer='../Models/10lora_2/checkpoint_8_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 07:52:21 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 07:52:21 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 07:52:21 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 07:52:25 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:24<00:00, 16.36it/s]\n",
      "2024-03-04 07:56:45,076 - INFO - jaccard: 0.4418   f1: 0.6012   recall: 0.5609   precision: 0.6802   ddi: 0.1342   drug_num: 19.2859   pred_num: 38   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 118-Phenytoin, model_path: ../Models/10lora_2/checkpoint_8_of_10 ------\n",
      "INFO 03-04 07:56:45 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_8_of_10', tokenizer='../Models/10lora_2/checkpoint_8_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 07:57:11 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 07:57:12 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 07:57:12 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 07:57:16 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:24<00:00, 16.34it/s]\n",
      "2024-03-04 08:01:36,345 - INFO - jaccard: 0.4432   f1: 0.6027   recall: 0.5633   precision: 0.6799   ddi: 0.1334   drug_num: 19.3674   pred_num: 193   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 119-Carvedilol, model_path: ../Models/10lora_2/checkpoint_8_of_10 ------\n",
      "INFO 03-04 08:01:36 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_8_of_10', tokenizer='../Models/10lora_2/checkpoint_8_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 08:02:02 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 08:02:02 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 08:02:02 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 08:02:06 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.32it/s]\n",
      "2024-03-04 08:06:27,451 - INFO - jaccard: 0.4444   f1: 0.6038   recall: 0.5649   precision: 0.6802   ddi: 0.1337   drug_num: 19.413   pred_num: 108   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 120-Isosorbide mononitrate, model_path: ../Models/10lora_2/checkpoint_9_of_10 ------\n",
      "INFO 03-04 08:06:27 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_9_of_10', tokenizer='../Models/10lora_2/checkpoint_9_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 08:07:05 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 08:07:05 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 08:07:05 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 08:07:09 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.26it/s]\n",
      "2024-03-04 08:11:30,708 - INFO - jaccard: 0.4448   f1: 0.6041   recall: 0.5657   precision: 0.68   ddi: 0.1337   drug_num: 19.4502   pred_num: 88   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 121-Calcium acetate, model_path: ../Models/10lora_2/checkpoint_9_of_10 ------\n",
      "INFO 03-04 08:11:30 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_9_of_10', tokenizer='../Models/10lora_2/checkpoint_9_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 08:12:02 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 08:12:02 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 08:12:02 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 08:12:06 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.31it/s]\n",
      "2024-03-04 08:16:27,360 - INFO - jaccard: 0.4452   f1: 0.6045   recall: 0.5663   precision: 0.6801   ddi: 0.1337   drug_num: 19.47   pred_num: 47   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 122-Nitroprusside, model_path: ../Models/10lora_2/checkpoint_9_of_10 ------\n",
      "INFO 03-04 08:16:27 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_9_of_10', tokenizer='../Models/10lora_2/checkpoint_9_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 08:16:57 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 08:16:57 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 08:16:57 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 08:17:02 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.30it/s]\n",
      "2024-03-04 08:21:23,299 - INFO - jaccard: 0.445   f1: 0.6043   recall: 0.5664   precision: 0.6794   ddi: 0.1335   drug_num: 19.4941   pred_num: 57   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 123-Acetazolamide, model_path: ../Models/10lora_2/checkpoint_9_of_10 ------\n",
      "INFO 03-04 08:21:23 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_9_of_10', tokenizer='../Models/10lora_2/checkpoint_9_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 08:21:52 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 08:21:53 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 08:21:53 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 08:21:57 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.31it/s]\n",
      "2024-03-04 08:26:18,224 - INFO - jaccard: 0.4451   f1: 0.6044   recall: 0.5667   precision: 0.6793   ddi: 0.1336   drug_num: 19.5144   pred_num: 48   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 124-Hydrochlorothiazide, model_path: ../Models/10lora_2/checkpoint_9_of_10 ------\n",
      "INFO 03-04 08:26:18 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_9_of_10', tokenizer='../Models/10lora_2/checkpoint_9_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 08:26:43 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 08:26:43 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 08:26:43 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 08:26:48 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.26it/s]\n",
      "2024-03-04 08:31:09,500 - INFO - jaccard: 0.4455   f1: 0.6048   recall: 0.5674   precision: 0.6792   ddi: 0.1338   drug_num: 19.5401   pred_num: 61   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 125-Clonazepam, model_path: ../Models/10lora_2/checkpoint_9_of_10 ------\n",
      "INFO 03-04 08:31:09 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_9_of_10', tokenizer='../Models/10lora_2/checkpoint_9_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 08:31:34 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 08:31:34 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 08:31:34 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 08:31:38 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.32it/s]\n",
      "2024-03-04 08:35:59,020 - INFO - jaccard: 0.4465   f1: 0.6058   recall: 0.5688   precision: 0.6795   ddi: 0.1342   drug_num: 19.5764   pred_num: 86   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 126-Aluminum hydroxide, model_path: ../Models/10lora_2/checkpoint_9_of_10 ------\n",
      "INFO 03-04 08:35:59 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_9_of_10', tokenizer='../Models/10lora_2/checkpoint_9_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 08:36:24 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 08:36:24 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 08:36:24 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 08:36:28 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.31it/s]\n",
      "2024-03-04 08:40:50,077 - INFO - jaccard: 0.4465   f1: 0.6058   recall: 0.5688   precision: 0.6795   ddi: 0.1342   drug_num: 19.5764   pred_num: 0   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 127-Sertraline, model_path: ../Models/10lora_2/checkpoint_9_of_10 ------\n",
      "INFO 03-04 08:40:50 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_9_of_10', tokenizer='../Models/10lora_2/checkpoint_9_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 08:41:15 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 08:41:15 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 08:41:15 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 08:41:20 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:24<00:00, 16.34it/s]\n",
      "2024-03-04 08:45:40,753 - INFO - jaccard: 0.448   f1: 0.6072   recall: 0.5708   precision: 0.6799   ddi: 0.1348   drug_num: 19.6263   pred_num: 118   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 128-Promethazine, model_path: ../Models/10lora_2/checkpoint_9_of_10 ------\n",
      "INFO 03-04 08:45:40 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_9_of_10', tokenizer='../Models/10lora_2/checkpoint_9_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 08:46:07 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 08:46:07 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 08:46:07 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 08:46:11 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:24<00:00, 16.36it/s]\n",
      "2024-03-04 08:50:33,008 - INFO - jaccard: 0.448   f1: 0.6072   recall: 0.5708   precision: 0.6798   ddi: 0.1348   drug_num: 19.6318   pred_num: 13   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 129-Clonidine, model_path: ../Models/10lora_2/checkpoint_9_of_10 ------\n",
      "INFO 03-04 08:50:33 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_9_of_10', tokenizer='../Models/10lora_2/checkpoint_9_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 08:51:03 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 08:51:03 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 08:51:03 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 08:51:08 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:24<00:00, 16.34it/s]\n",
      "2024-03-04 08:55:28,867 - INFO - jaccard: 0.4485   f1: 0.6077   recall: 0.5717   precision: 0.6798   ddi: 0.1345   drug_num: 19.6571   pred_num: 60   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 130-Tamsulosin, model_path: ../Models/10lora_2/checkpoint_9_of_10 ------\n",
      "INFO 03-04 08:55:28 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_9_of_10', tokenizer='../Models/10lora_2/checkpoint_9_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 08:55:58 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 08:55:59 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 08:55:59 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 08:56:03 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:24<00:00, 16.34it/s]\n",
      "2024-03-04 09:00:24,500 - INFO - jaccard: 0.4499   f1: 0.609   recall: 0.5736   precision: 0.6803   ddi: 0.1343   drug_num: 19.7069   pred_num: 118   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 131-Epinephrine, model_path: ../Models/10lora_2/checkpoint_9_of_10 ------\n",
      "INFO 03-04 09:00:24 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_9_of_10', tokenizer='../Models/10lora_2/checkpoint_9_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 09:00:55 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 09:00:55 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 09:00:55 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 09:00:59 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:24<00:00, 16.34it/s]\n",
      "2024-03-04 09:05:20,450 - INFO - jaccard: 0.4501   f1: 0.6092   recall: 0.5742   precision: 0.68   ddi: 0.1342   drug_num: 19.7437   pred_num: 87   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 132-Mycophenolate mofetil, model_path: ../Models/10lora_2/checkpoint_9_of_10 ------\n",
      "INFO 03-04 09:05:20 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_9_of_10', tokenizer='../Models/10lora_2/checkpoint_9_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 09:05:53 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 09:05:53 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 09:05:53 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 09:05:58 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.26it/s]\n",
      "2024-03-04 09:10:19,999 - INFO - jaccard: 0.4509   f1: 0.6099   recall: 0.5752   precision: 0.6804   ddi: 0.1341   drug_num: 19.7698   pred_num: 62   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 133-Sulbactam, model_path: ../Models/10lora_2/checkpoint_9_of_10 ------\n",
      "INFO 03-04 09:10:20 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_9_of_10', tokenizer='../Models/10lora_2/checkpoint_9_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 09:10:46 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 09:10:46 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 09:10:46 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 09:10:51 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:24<00:00, 16.35it/s]\n",
      "2024-03-04 09:15:12,460 - INFO - jaccard: 0.451   f1: 0.61   recall: 0.5753   precision: 0.6803   ddi: 0.134   drug_num: 19.7783   pred_num: 20   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 134-Sodium sulfate, model_path: ../Models/10lora_2/checkpoint_9_of_10 ------\n",
      "INFO 03-04 09:15:12 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_9_of_10', tokenizer='../Models/10lora_2/checkpoint_9_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 09:15:38 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 09:15:38 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 09:15:38 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 09:15:43 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.28it/s]\n",
      "2024-03-04 09:20:06,044 - INFO - jaccard: 0.4522   f1: 0.611   recall: 0.577   precision: 0.6807   ddi: 0.1343   drug_num: 19.8125   pred_num: 81   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 135-Nicotine, model_path: ../Models/10lora_2/checkpoint_10_of_10 ------\n",
      "INFO 03-04 09:20:06 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_10_of_10', tokenizer='../Models/10lora_2/checkpoint_10_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 09:20:41 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 09:20:41 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 09:20:41 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 09:20:45 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:24<00:00, 16.37it/s]\n",
      "2024-03-04 09:25:07,051 - INFO - jaccard: 0.4526   f1: 0.6115   recall: 0.5777   precision: 0.6807   ddi: 0.1344   drug_num: 19.8323   pred_num: 47   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 136-Ceftazidime, model_path: ../Models/10lora_2/checkpoint_10_of_10 ------\n",
      "INFO 03-04 09:25:07 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_10_of_10', tokenizer='../Models/10lora_2/checkpoint_10_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 09:25:33 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 09:25:33 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 09:25:33 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 09:25:37 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.31it/s]\n",
      "2024-03-04 09:30:00,723 - INFO - jaccard: 0.4526   f1: 0.6115   recall: 0.5777   precision: 0.6806   ddi: 0.1344   drug_num: 19.8353   pred_num: 7   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 137-Acyclovir, model_path: ../Models/10lora_2/checkpoint_10_of_10 ------\n",
      "INFO 03-04 09:30:00 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_10_of_10', tokenizer='../Models/10lora_2/checkpoint_10_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 09:30:25 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 09:30:25 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 09:30:25 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 09:30:30 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:24<00:00, 16.33it/s]\n",
      "2024-03-04 09:34:51,409 - INFO - jaccard: 0.453   f1: 0.6118   recall: 0.5784   precision: 0.6806   ddi: 0.1342   drug_num: 19.8606   pred_num: 60   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 138-Loperamide, model_path: ../Models/10lora_2/checkpoint_10_of_10 ------\n",
      "INFO 03-04 09:34:51 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_10_of_10', tokenizer='../Models/10lora_2/checkpoint_10_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 09:35:21 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 09:35:21 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 09:35:21 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 09:35:25 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:24<00:00, 16.34it/s]\n",
      "2024-03-04 09:39:47,904 - INFO - jaccard: 0.453   f1: 0.6119   recall: 0.5786   precision: 0.6803   ddi: 0.1343   drug_num: 19.8763   pred_num: 37   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 139-Octreotide, model_path: ../Models/10lora_2/checkpoint_10_of_10 ------\n",
      "INFO 03-04 09:39:47 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_10_of_10', tokenizer='../Models/10lora_2/checkpoint_10_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 09:40:23 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 09:40:23 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 09:40:23 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 09:40:28 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:24<00:00, 16.34it/s]\n",
      "2024-03-04 09:44:51,475 - INFO - jaccard: 0.4542   f1: 0.6131   recall: 0.5803   precision: 0.6807   ddi: 0.1343   drug_num: 19.9113   pred_num: 83   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 140-Tiotropium, model_path: ../Models/10lora_2/checkpoint_10_of_10 ------\n",
      "INFO 03-04 09:44:51 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_10_of_10', tokenizer='../Models/10lora_2/checkpoint_10_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 09:45:23 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 09:45:23 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 09:45:23 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 09:45:27 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.29it/s]\n",
      "2024-03-04 09:49:51,383 - INFO - jaccard: 0.4553   f1: 0.614   recall: 0.582   precision: 0.6807   ddi: 0.1345   drug_num: 19.9717   pred_num: 143   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 141-Mirtazapine, model_path: ../Models/10lora_2/checkpoint_10_of_10 ------\n",
      "INFO 03-04 09:49:51 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_10_of_10', tokenizer='../Models/10lora_2/checkpoint_10_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 09:50:20 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 09:50:20 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 09:50:20 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 09:50:25 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.29it/s]\n",
      "2024-03-04 09:54:49,182 - INFO - jaccard: 0.4561   f1: 0.6148   recall: 0.5832   precision: 0.6809   ddi: 0.1344   drug_num: 20.0017   pred_num: 71   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 142-Tacrolimus, model_path: ../Models/10lora_2/checkpoint_10_of_10 ------\n",
      "INFO 03-04 09:54:49 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_10_of_10', tokenizer='../Models/10lora_2/checkpoint_10_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 09:55:14 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 09:55:14 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 09:55:14 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 09:55:19 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.29it/s]\n",
      "2024-03-04 09:59:42,326 - INFO - jaccard: 0.4569   f1: 0.6155   recall: 0.5841   precision: 0.6812   ddi: 0.1345   drug_num: 20.0283   pred_num: 63   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 143-Clindamycin, model_path: ../Models/10lora_2/checkpoint_10_of_10 ------\n",
      "INFO 03-04 09:59:42 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_10_of_10', tokenizer='../Models/10lora_2/checkpoint_10_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 10:00:09 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 10:00:09 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 10:00:09 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 10:00:13 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.31it/s]\n",
      "2024-03-04 10:04:37,268 - INFO - jaccard: 0.4569   f1: 0.6155   recall: 0.5842   precision: 0.6811   ddi: 0.1345   drug_num: 20.0308   pred_num: 6   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 144-Erythromycin, model_path: ../Models/10lora_2/checkpoint_10_of_10 ------\n",
      "INFO 03-04 10:04:37 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_10_of_10', tokenizer='../Models/10lora_2/checkpoint_10_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 10:05:03 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 10:05:03 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 10:05:03 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 10:05:08 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.29it/s]\n",
      "2024-03-04 10:09:31,526 - INFO - jaccard: 0.457   f1: 0.6156   recall: 0.5843   precision: 0.6811   ddi: 0.1345   drug_num: 20.0351   pred_num: 10   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 145-Metolazone, model_path: ../Models/10lora_2/checkpoint_10_of_10 ------\n",
      "INFO 03-04 10:09:31 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_10_of_10', tokenizer='../Models/10lora_2/checkpoint_10_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 10:10:02 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 10:10:02 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 10:10:02 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 10:10:07 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.32it/s]\n",
      "2024-03-04 10:14:29,468 - INFO - jaccard: 0.4571   f1: 0.6157   recall: 0.5845   precision: 0.6811   ddi: 0.1345   drug_num: 20.0427   pred_num: 18   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 146-Hydrocodone, model_path: ../Models/10lora_2/checkpoint_10_of_10 ------\n",
      "INFO 03-04 10:14:29 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_10_of_10', tokenizer='../Models/10lora_2/checkpoint_10_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 10:15:00 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 10:15:00 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 10:15:00 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 10:15:05 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.29it/s]\n",
      "2024-03-04 10:19:28,465 - INFO - jaccard: 0.4572   f1: 0.6157   recall: 0.5848   precision: 0.6808   ddi: 0.1345   drug_num: 20.0595   pred_num: 40   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 147-Valsartan, model_path: ../Models/10lora_2/checkpoint_10_of_10 ------\n",
      "INFO 03-04 10:19:28 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_10_of_10', tokenizer='../Models/10lora_2/checkpoint_10_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 10:19:58 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 10:19:58 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 10:19:58 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 10:20:02 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.31it/s]\n",
      "2024-03-04 10:24:27,151 - INFO - jaccard: 0.458   f1: 0.6164   recall: 0.586   precision: 0.6809   ddi: 0.1344   drug_num: 20.095   pred_num: 84   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 148-Codeine, model_path: ../Models/10lora_2/checkpoint_10_of_10 ------\n",
      "INFO 03-04 10:24:27 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_10_of_10', tokenizer='../Models/10lora_2/checkpoint_10_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 10:24:52 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 10:24:53 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 10:24:53 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 10:24:57 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:24<00:00, 16.37it/s]\n",
      "2024-03-04 10:29:20,525 - INFO - jaccard: 0.458   f1: 0.6164   recall: 0.586   precision: 0.6809   ddi: 0.1344   drug_num: 20.0967   pred_num: 4   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 149-Fluoxetine, model_path: ../Models/10lora_2/checkpoint_10_of_10 ------\n",
      "INFO 03-04 10:29:20 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_10_of_10', tokenizer='../Models/10lora_2/checkpoint_10_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 10:29:45 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 10:29:45 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 10:29:45 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 10:29:50 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.31it/s]\n",
      "2024-03-04 10:34:13,135 - INFO - jaccard: 0.4587   f1: 0.6172   recall: 0.5871   precision: 0.6812   ddi: 0.1346   drug_num: 20.122   pred_num: 60   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Evaluating 150-Nifedipine, model_path: ../Models/10lora_2/checkpoint_10_of_10 ------\n",
      "INFO 03-04 10:34:13 llm_engine.py:70] Initializing an LLM engine with config: model='../Models/10lora_2/checkpoint_10_of_10', tokenizer='../Models/10lora_2/checkpoint_10_of_10', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 03-04 10:34:38 llm_engine.py:275] # GPU blocks: 7198, # CPU blocks: 512\n",
      "INFO 03-04 10:34:38 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 10:34:38 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 03-04 10:34:42 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2368/2368 [02:25<00:00, 16.32it/s]\n",
      "2024-03-04 10:39:05,212 - INFO - jaccard: 0.4591   f1: 0.6175   recall: 0.5877   precision: 0.6811   ddi: 0.135   drug_num: 20.1432   pred_num: 50   \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '7'\n",
    "\n",
    "from _4_group_lora_evaluation_vllm import group_lora_evaluation_vllm\n",
    "\n",
    "gpu_memory_utilization = 0.9\n",
    "base_model = \"llama-2-7b\"\n",
    "run_name = \"10lora_2\"\n",
    "\n",
    "group_lora_evaluation_vllm(run_name, gpu_memory_utilization, vllm=True, mode='test', \n",
    "                          use_history=True, \n",
    "                          use_note=True,\n",
    "                          file_name='data4LLM_CONCISE_NOTE.csv',\n",
    "                          base_model=base_model,\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5_generalization_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '7'\n",
    "\n",
    "from _2_each_drug_evaluation_vllm import each_drug_evaluation_vllm\n",
    "\n",
    "similar_med_list = [['Trazodone','Citalopram','Quetiapine','Sertraline','Ascorbic acid','Niacin','Riboflavin','Biotin','Salbutamol','Ipratropium','Fluticasone propionate']]\n",
    "\n",
    "def find_folder_with_string(path, target_string):\n",
    "    folders = [f for f in os.listdir(path) if os.path.isdir(os.path.join(path, f))]\n",
    "\n",
    "    matching_folders = [folder for folder in folders if target_string in folder]\n",
    "\n",
    "    return os.path.join(path, matching_folders[0])\n",
    "\n",
    "for idx in range(len(similar_med_list)):\n",
    "    med_list = similar_med_list[idx]\n",
    "    print(f'---------START TESTING MEDS: {med_list}---------\\n')\n",
    "    for test_med in med_list:\n",
    "        for lora_med in med_list:\n",
    "            model_name = find_folder_with_string('../output/each_drug_note_v3_drug_generalization/checkpoints/', lora_med)\n",
    "            f1 = each_drug_evaluation_vllm(model_name, [test_med], gpu_memory_utilization=0.9, \n",
    "                            data_len=-1, vllm=True, remove_model=False, base_model='llama-2-7b', use_note=True, \n",
    "                            file_name='data4LLM_CONCISE_NOTE.csv')\n",
    "            \n",
    "            print(f'test_med:{test_med},')\n",
    "            print(f'lora_med:{lora_med},')\n",
    "            print(f'f1:{f1:.4f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Acetaminophen 11881\n",
      "1 Potassium chloride 10442\n",
      "2 Pantoprazole 9869\n",
      "3 Metoprolol 9147\n",
      "4 Magnesium sulfate 8637\n",
      "5 Furosemide 8529\n",
      "6 Vancomycin 7783\n",
      "7 Salbutamol 6647\n",
      "8 Oxycodone 6401\n",
      "9 Bisacodyl 6257\n",
      "10 Lorazepam 5950\n",
      "11 Magnesium 5799\n",
      "12 Ipratropium 5407\n",
      "13 Acetylsalicylic acid 5287\n",
      "14 Morphine 4705\n",
      "15 Fentanyl 4611\n",
      "16 Ondansetron 4597\n",
      "17 Calcium gluconate 4471\n",
      "18 Propofol 4078\n",
      "19 Lansoprazole 4042\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import utils\n",
    "\n",
    "data, med_names, med_weights = utils.load_data(mode='train')\n",
    "for i in range(20):\n",
    "    print(i, med_names[i], med_weights[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
